id,问题标题,问题内容,回答数量,问题评论量,问题关注量,问题链接,提问者昵称,提问者主页,提问时间,回答时间,回答更新时间,回答内容,赞同量,评论量,喜欢量,回答作者,回答作者主页,行为类型
1586177478948,千禧年七大数学问题之——“NP=P”,,,,,https://api.zhihu.com/articles/53954424,,,,1586177478,,"<p>1900年，德国大数学家大卫·希尔伯特在巴黎提出了23个待解决的数学问题，这些问题直接影响、指导了一百多年的数学研究方向。为了呼应一个世纪前希尔伯特在世纪之初通过提问题而指导研究方向的传统，2000年5月24日美国克雷数学研究所( Clay Mathematics Institute , CMI ) 公布了七个有待证明或证伪的数学猜想，并为每个猜想给出了100万美金的奖金。</p><p>科学发展越来越深入，学科专业划分越来越细致，可能如今已经没有像当年的希尔伯特那样的集大成的数学家了，所以这七个问题是由证明了费马大定理的英国数学家安德鲁·怀尔斯、菲尔兹奖和阿贝尔奖双奖获得者英国数学家阿蒂亚（前些日子声称证明了黎曼猜想的那个老人）、美国数学家阿贝尔奖获得者约翰·泰特、法国数学家阿兰·孔涅（ Alain Connes ）、弦论创始人美国数学家物理学家威滕等科学家共同讨论确定的。</p><p>这七个问题可以说在当今数学及数学物理领域是赫赫有名，其中的第一个问题就是被称为“NP=P”的问题。这是一个看起来不太像传统数学猜想的问题，其实它属于计算复杂性理论的问题，IT行业的人士可能对这个问题更熟悉一些。今天我们就来聊聊这个问题。</p><p><b>一、“NP=P”到底是个什么问题？</b></p><p>要知道“NP=P”是个什么问题，先要知道什么是“P类问题”，什么是“NP类问题”，而这两个概念又和计算理论中的时间复杂度有关。不过不用担心，这几个概念都不是很复杂。</p><p>简单的说，解决一个问题的某种算法所需要的计算量（或计算步骤）随着这个问题的规模增长而增长的速度就被称为这个算法的时间复杂度。要注意的是，时间复杂度本质上指的是计算量增长的速度而不是这个算法运行的时间。例如：</p><blockquote>（1）我们要计算前n个自然数的和，如果使用最直接而笨拙的方法，需要计算n-1次加法，那么可以认为这个算法的时间复杂度就是“n-1”，记作O(n-1)或者O(n)。之所以可以记作O(n)，是因为随着n的增大，1这个常数就忽略不计了。<br>（2）如果我们要把n个不同的自然数排序，那么就需要对这些自然数的大小进行比较。我们也采用最笨拙的算法，也就是将每两个自然数都做一次比较，那么需要比较n(n-1)/2次，时间复杂度可以记为 <img src=""https://www.zhihu.com/equation?tex=O%28%5Cfrac%7Bn%5E2%7D%7B2%7D-%5Cfrac%7Bn%7D%7B2%7D%29"" alt=""O(\frac{n^2}{2}-\frac{n}{2})"" eeimg=""1""> ，或者记为<img src=""https://www.zhihu.com/equation?tex=O%28%5Cfrac%7Bn%5E2%7D%7B2%7D%29"" alt=""O(\frac{n^2}{2})"" eeimg=""1""> 。<br>（3）下面举一个更复杂些的例子。给出一个含有n个逻辑变量的逻辑表达式，请判断这个表达式是否是一个“重言”的逻辑表达式。“重言”的意思是说这个表达式无论所包含的n个逻辑变量怎么取值，其结果都是真，类似于我们语言中的“废话”（如“明天要么下雨要么不下雨”）。最简单的重言表达式如 <img src=""https://www.zhihu.com/equation?tex=A%5Cvee+%5Cbar%7BA%7D"" alt=""A\vee \bar{A}"" eeimg=""1""> ，其结果永远为真。我们如果也采用最笨拙的算法，穷举出n个逻辑变量的全部可能的组合，计算每种组合下逻辑表达式的值，如果都是真，就说明这个逻辑表达式是重言的。n个逻辑变量全部可能的组合有 <img src=""https://www.zhihu.com/equation?tex=2%5En"" alt=""2^n"" eeimg=""1""> 种，每种组合要进行大约P(n)次逻辑运算（P(n)是n的某个多项式），因此其时间复杂度为 <img src=""https://www.zhihu.com/equation?tex=O%28P%28n%29%5Ccdot+2%5En%29"" alt=""O(P(n)\cdot 2^n)"" eeimg=""1""> 。</blockquote><p>当然，对于同样的问题，采用不同的算法其时间复杂度不一定相同。<b>如果某个问题，我们能够找到的最优算法的时间复杂度是n的多项式函数，我们就说这个问题属于“P类问题”。</b>这里面的P就是多项式的英文（Polynomial）的首字母。前面例子中的（1）和（2）就属于“P类问题”。</p><p>还有一些问题，无论其是否能够在多项式时间复杂度内求解，至少我们知道<b>如果随便给出一个可能的解，我们可以在多项式时间复杂度内验证其是否为所求的解。</b>这类问题我们称之为“NP类问题”。比如前面的例子（3），我们随便猜测一组逻辑变量的组合，就可以通过P(n)次逻辑运算判定其结果是否为假，如果是，那么我们就确定这个逻辑表达式不是重言表达式。因此，（3）中的问题虽然我们还没有找到一个多项式时间复杂度的算法，不知道它是否属于“P类问题”，但是我们很确定它一定属于“NP类问题”。</p><p>之所以要研究一个问题是否有多项式时间复杂度的算法，是因为多项式时间复杂度的计算量增长速度相对来说算是“慢”的，随着n的增大，其计算量远远小于 <img src=""https://www.zhihu.com/equation?tex=O%282%5En%29"" alt=""O(2^n)"" eeimg=""1""> 、 <img src=""https://www.zhihu.com/equation?tex=O%28n%21%29"" alt=""O(n!)"" eeimg=""1""> 、 <img src=""https://www.zhihu.com/equation?tex=O%28n%5En%29"" alt=""O(n^n)"" eeimg=""1""> 等时间复杂度问题。比如很有名的大整数质因数分解问题，给出一个2048位的二进制整数，要找到它的某个质因数，一般情况下穷尽全世界的计算能力也不能在100年内完成这个求解计算过程；但是如果我给出一个质数，却可以用普通的计算机在几秒钟时间以内确定这个质数是否是这个2048位二进制整数的一个因数。这就是不同时间复杂度在实际计算过程中的差别！</p><p>知道了什么是“P类问题”，什么是“NP类问题”，我们就很容易知道，全部的“P类问题”都属于“NP类问题”，也就是“NP类问题集合” <img src=""https://www.zhihu.com/equation?tex=%5Csupseteq"" alt=""\supseteq"" eeimg=""1""> “P类问题集合”。这是显然的，一个问题可以在多项式时间复杂度内求解，当然可以在多项式时间复杂度内验证。但是反过来，一个可以在多项式时间复杂度内验证的问题是否一定能够通过多项式时间复杂度的算法求解呢？也就是说，<b>是否全部的“NP类问题”都属于“P类问题”呢？</b>这就是著名的“NP=P”问题。如果答案为“是”，那就意味着“NP类问题集合”=“P类问题集合”；如果答案为“否”，那就意味着“NP类问题集合” <img src=""https://www.zhihu.com/equation?tex=%5Csupset"" alt=""\supset"" eeimg=""1""> “P类问题集合”，但不相等。</p><p>如果“NP=P”，这个结果对我们这个世界的影响是很大的。这意味着任何一个原来找不到“P类算法”的NP类问题都可以找到相应的“P类算法”了。比如刚才说的大整数的质因数分解问题，就成为了P类问题。这意味着刚才例子中2048位的二进制大整数可以用一台普通电脑在几秒钟甚至更短的时间完成其质因数分解，那么被广泛应用的RSA加密算法就彻底失效了。我们大量的银行数字证书、网站SSL加密都不再安全，人类必须要寻找新的、更强的加密算法。</p><p>同时，这也意味着很多原来通过计算很难解决的大量问题都可以通过算法优化而轻松得到解决了。如果NP=P，那么我们就可以更好地预测天气，更容易通过氨基酸序列来预测蛋白质结构，更好的确定计算机芯片上最有效的晶体管布局，更优的完成物流交通调度，......。</p><p>如果“NP <img src=""https://www.zhihu.com/equation?tex=%5Cne"" alt=""\ne"" eeimg=""1""> P”，对我们这个世界的影响很小，或者说对实际生活几乎没有什么影响。可是，迄今为止还没有谁能给出这个证明。</p><p>这个问题的难度远远超过一般人的想象。目前，绝大多数的相关领域科学家（包括数学家、计算理论科学家、IT行业资深算法研究人员等）都认为“NP <img src=""https://www.zhihu.com/equation?tex=%5Cne"" alt=""\ne"" eeimg=""1""> P”，所以，我们可以暂时先松口气，不用太担心“NP=P”给我们日常生活带来的影响。</p><p><b>二、什么是NPC问题？以及第一个NPC问题</b></p><p>虽然我们还不知道NP是否等于P，但也不是在这方面的研究完全没有进展。早在1971年，多伦多大学计算复杂理论教授斯蒂芬·库克就在其著名论文《定理证明过程的复杂性》（《The Complexity of Theorem - Proving Procedures》）中明确提出了人们一直怀疑其是否存在的一类问题——NPC问题（NP完全问题），并给出了第一个NPC问题的证明。这对推动“ NP = P ”问题的解决是一个巨大的贡献。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-5a0f796061de47c7e2f7a54c8f12040f_b.jpg"" data-size=""normal"" data-rawwidth=""1067"" data-rawheight=""662"" class=""origin_image zh-lightbox-thumb"" width=""1067"" data-original=""https://pic1.zhimg.com/v2-5a0f796061de47c7e2f7a54c8f12040f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1067'%20height='662'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""1067"" data-rawheight=""662"" class=""origin_image zh-lightbox-thumb lazy"" width=""1067"" data-original=""https://pic1.zhimg.com/v2-5a0f796061de47c7e2f7a54c8f12040f_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-5a0f796061de47c7e2f7a54c8f12040f_b.jpg""><figcaption>库克教授论文的截图</figcaption></figure><p><b>（一）什么是NPC（NP-Complete）问题？</b></p><p>如果用最通俗的话来介绍，NPC问题就是NP问题中最难的那一类问题，或者说任何NP类问题的难度都小于等于NPC类问题。那么怎么定义这个“最难”呢？</p><blockquote>如果说问题1可以在多项式时间复杂度内转换为问题2，我们就说问题2的难度大于等于问题1 。<br>更严格一点的描述为：设有两个问题 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 和 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> ， <img src=""https://www.zhihu.com/equation?tex=S_1"" alt=""S_1"" eeimg=""1""> 和 <img src=""https://www.zhihu.com/equation?tex=S_2"" alt=""S_2"" eeimg=""1""> 分别为 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 和 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> 的所有待验证的解的集合（就是穷举法中全部需要判断的解的集合）， <img src=""https://www.zhihu.com/equation?tex=I_1"" alt=""I_1"" eeimg=""1""> 和 <img src=""https://www.zhihu.com/equation?tex=I_2"" alt=""I_2"" eeimg=""1""> 分别为 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 和 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> 的解的集合，如果存在一个多项式时间复杂度的算法能够完成的映射 <img src=""https://www.zhihu.com/equation?tex=f%3AS_1%5Crightarrow+S_2"" alt=""f:S_1\rightarrow S_2"" eeimg=""1""> ，使得当且仅当 <img src=""https://www.zhihu.com/equation?tex=i%5Cin+I_1"" alt=""i\in I_1"" eeimg=""1""> 时， <img src=""https://www.zhihu.com/equation?tex=f%28i%29%5Cin+I_2"" alt=""f(i)\in I_2"" eeimg=""1""> ，我们就说问题 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 可以在多项式时间复杂度内转换为问题 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> ，称为问题 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 可以归约为问题 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> ，记作 <img src=""https://www.zhihu.com/equation?tex=L_1%E2%88%9DL_2"" alt=""L_1∝L_2"" eeimg=""1""> 。<br>这种归约关系显然是可传递的，也就是说，如果 <img src=""https://www.zhihu.com/equation?tex=L_1%E2%88%9DL_2"" alt=""L_1∝L_2"" eeimg=""1""> 且 <img src=""https://www.zhihu.com/equation?tex=L_2%E2%88%9DL_3"" alt=""L_2∝L_3"" eeimg=""1""> ，那么 <img src=""https://www.zhihu.com/equation?tex=L_1%E2%88%9DL_3"" alt=""L_1∝L_3"" eeimg=""1""> 。</blockquote><p>这种情况下，问题2如果能够在多项式时间复杂度内得到解决，或者说如果问题2是一个“P类问题”，那么问题1显然也会是一个“P类问题”。这是比较显然的，简略证明如下：</p><blockquote>按照前面的定义， <img src=""https://www.zhihu.com/equation?tex=L_1%E2%88%9DL_2"" alt=""L_1∝L_2"" eeimg=""1""> ，并设问题 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 和 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> 的最大规模为n，映射f可以在 <img src=""https://www.zhihu.com/equation?tex=P_1%28n%29"" alt=""P_1(n)"" eeimg=""1""> 的步骤内完成。如果 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> 可以在 <img src=""https://www.zhihu.com/equation?tex=P_2%28n%29"" alt=""P_2(n)"" eeimg=""1""> 步骤内得到解决，即在 <img src=""https://www.zhihu.com/equation?tex=P_2%28n%29"" alt=""P_2(n)"" eeimg=""1""> 步骤内可以得到 <img src=""https://www.zhihu.com/equation?tex=I_2"" alt=""I_2"" eeimg=""1""> ，那么我们再用 <img src=""https://www.zhihu.com/equation?tex=n%5Ccdot+P_1%28n%29"" alt=""n\cdot P_1(n)"" eeimg=""1""> 的步骤得到 <img src=""https://www.zhihu.com/equation?tex=f%28S_1%29"" alt=""f(S_1)"" eeimg=""1""> ，对于使 <img src=""https://www.zhihu.com/equation?tex=f%28S_1%29"" alt=""f(S_1)"" eeimg=""1"">中元素 属于 <img src=""https://www.zhihu.com/equation?tex=I_2"" alt=""I_2"" eeimg=""1""> 的 <img src=""https://www.zhihu.com/equation?tex=S_1"" alt=""S_1"" eeimg=""1""> 的子集即构成 <img src=""https://www.zhihu.com/equation?tex=I_1"" alt=""I_1"" eeimg=""1""> 。因此，可以用 <img src=""https://www.zhihu.com/equation?tex=n%5Ccdot+P_1%28n%29%2BP_2%28n%29"" alt=""n\cdot P_1(n)+P_2(n)"" eeimg=""1""> 的时间复杂度得到 <img src=""https://www.zhihu.com/equation?tex=I_1"" alt=""I_1"" eeimg=""1""> ，也就是得到 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 的解集。<br>由于 <img src=""https://www.zhihu.com/equation?tex=P_1%28n%29"" alt=""P_1(n)"" eeimg=""1""> 和 <img src=""https://www.zhihu.com/equation?tex=P_2%28n%29"" alt=""P_2(n)"" eeimg=""1""> 都是多项式，因此<img src=""https://www.zhihu.com/equation?tex=n%5Ccdot+P_1%28n%29%2BP_2%28n%29"" alt=""n\cdot P_1(n)+P_2(n)"" eeimg=""1""> 也是一个多项式，即问题 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 也可以通过多项式时间复杂度求解，从而 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 也是一个“P类问题”。</blockquote><p>既然我们说NPC问题是所有NP类问题中最难的那种，那么按照上述定义，也就意味着<b>如果一个问题是NPC问题需要满足两个条件：一是它首先要是一个NP类问题；二是任何其它NP类问题都可以归约到这个问题</b>。</p><p>由于任何NP类问题都可以归约到一个NPC问题，那么如果求解这个NPC问题存在多项式时间复杂度的算法，也就是说如果这个NPC问题是“P类问题”，就意味着任何NP类问题都是“P类问题”，即“NP=P”成立了。这就是NPC问题在解决“NP=P”问题中的巨大价值。</p><p>另外，根据归约关系的可传递性，如果某一个问题 <img src=""https://www.zhihu.com/equation?tex=L_1"" alt=""L_1"" eeimg=""1""> 是NPC问题，并且这个问题还可以归约到另外一个NP类问题 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> ，也即 <img src=""https://www.zhihu.com/equation?tex=L_1%E2%88%9DL_2"" alt=""L_1∝L_2"" eeimg=""1""> ，那么 <img src=""https://www.zhihu.com/equation?tex=L_2"" alt=""L_2"" eeimg=""1""> 也必然是一个NPC问题。换句话说，如果存在多个NPC问题，那么这些NPC问题之间是可以互相归约的，也就是说任意两个NPC问题的难度都是一样的。</p><p><b>（二）第一个被发现的NPC问题及其证明思路</b></p><p>找到一个NPC问题是很不容易的，特别是找到第一个NPC问题更不容易。一度人们曾经怀疑是否真的存在NPC问题。</p><p>前面提到，1971年库克教授在论文中提出了第一个NPC问题并给出了证明。这使得世人知道了这类NPC问题是真的存在的。库克教授给出的这第一个NPC问题叫做“SAT问题”，又称作“可满足性问题”，英文为“The Satisfiability Problem”，SAT是Satisfiability单词的前三个字母。“SAT问题是一个NPC问题”这个结论被称作<b>库克定理</b>。其实SAT问题就是我们前面在介绍时间复杂度的时候提到的例子（3），只不过换了个说法而已，库克教授论文中用的就是例子（3）中的说法，判断是否是重言表达式。</p><p>如今的SAT问题被定义为“给出一个含有n个逻辑变量的逻辑表达式，判断这个表达式是否可能取值为真，也就是判断这个逻辑表达式是否是可被满足的”，所以它又叫做“可满足性问题”。</p><p>解决SAT问题并不像看起来这么简单，目前已知的各类解决SAT问题的算法的时间复杂度都是较高的，都大于多项式时间。同样的，证明SAT问题是NPC问题也不是一个简单的事，因为我们需要证明任何NP类问题都可以在多项式时间复杂度内归约为SAT问题。</p><p>库克教授找到了一个非常巧妙的方法给出了这个证明。这个方法是基于伟大的英国数学家、罗辑学家、计算机科学之父<b>艾伦·麦席森·图灵</b>（Alan Mathison Turing，1912年6月23日－1954年6月7日）设计的“图灵机”。提到图灵，为了以示尊敬，我得先漱漱口、洗洗手再来码字写这篇文章，因为他实在是太伟大了。</p><p>... ... ... ...</p><p>好，洗漱完毕后，我们来介绍库克教授的证明思路。之所以仅仅介绍证明思路，是因为全部证明过程涉及到比较复杂的逻辑表达式构造，算不上很优美。但是，其证明思路与框架确实非常巧妙，充分体现了逻辑学的优美。</p><p>由于库克定理的证明主要基于非确定图灵机，我们先要简单介绍一下图灵机与非确定图灵机。</p><p>1、神奇而伟大的图灵机</p><p>（以下关于图灵机的介绍主要参照了百度百科，<a href=""http://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%259B%25BE%25E7%2581%25B5%25E6%259C%25BA/2112989%3Ffr%3Daladdin"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">网址在此</a>。不过百度百科介绍得也不是足够清楚，我适当做了一些补充。）</p><p>图灵机，又称图灵计算、图灵计算机，是由伟大的图灵提出的一种抽象计算模型。它将人们使用纸笔进行数学运算的过程进行抽象，由一个虚拟的机器替代人们进行数学运算。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-f85652a750fcd0d75922dd9c5ee74f2e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""300"" data-rawheight=""129"" class=""content_image"" width=""300""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='300'%20height='129'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""300"" data-rawheight=""129"" class=""content_image lazy"" width=""300"" data-actualsrc=""https://pic3.zhimg.com/v2-f85652a750fcd0d75922dd9c5ee74f2e_b.jpg""></figure><p>图灵的基本思想是用机器来模拟人们用纸笔进行数学运算的过程，他把这样的过程看作下列两种简单的动作：</p><p>一是在纸上写上或擦除某个符号；二是把注意力从纸的一个位置移动到另一个位置。而在每个阶段，人要决定下一步的动作，依赖于 (a) 此人当前所关注的纸上某个位置的符号和（b) 此人当前思维的状态。</p><p>为了模拟人的这种运算过程，图灵构造出一台假想的机器，该机器由以下几个部分组成：</p><p>（1）一条无限长的纸带 <b>TAPE</b>。纸带被划分为一个接一个的小格子，每个格子上包含一个来自有限字母表的符号，字母表中有一个特殊的符号“B”表示空白。纸带上的格子从左到右依次被编号为 0,1,2,... ，纸带的右端可以无限伸展。</p><p>（2）一个读写头 <b>HEAD</b>。该读写头可以在纸带上左右移动，它能读出当前所指的格子上的符号，并能改变当前格子上的符号。</p><p>（3）一套控制规则 <b>TABLE</b>。根据当前机器所处的状态（相当于当前所执行的指令）以及当前读写头所指的格子上的符号来确定读写头下一步的动作，改变纸带当前格子里面的符号（如需要），并令机器进入一个新的状态（相当于确定下一步要执行的指令）。这个TABLE其实就是一套指令集，它在某种意义上就相当于我们今天计算机里面的程序。它至少包括以下内容：改写当前格子为某个符号、读写头向左或者向右移动一步、确定下一步执行的指令。</p><p>为了让大家更容易理解图灵机，举个例子吧。下图就是某个图灵机的一张控制规则TABLE，这个图灵机假设输入是只有“a”和“b”两种字符组成的字符串，它能够判断这个字符串是否是类似“aaabbb”形式的，也就是由同样数量连续个a和连续个b组成的字符串。在图灵机中，“B”表示“空”（Blank），如果后续状态是“Qaccept”或者“Qreject”，则分别表示运行结束且判断结果为“是”或“否”。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-628db0569066f6a5ed681f9998dab1c7_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1946"" data-rawheight=""508"" class=""origin_image zh-lightbox-thumb"" width=""1946"" data-original=""https://pic2.zhimg.com/v2-628db0569066f6a5ed681f9998dab1c7_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1946'%20height='508'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1946"" data-rawheight=""508"" class=""origin_image zh-lightbox-thumb lazy"" width=""1946"" data-original=""https://pic2.zhimg.com/v2-628db0569066f6a5ed681f9998dab1c7_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-628db0569066f6a5ed681f9998dab1c7_b.jpg""></figure><p>（4）一个<b>状态寄存器</b>。它用来保存图灵机当前所处的状态，也就是当前需要执行的指令（上图中的Q0～Q4）。</p><p>某一些指令运行完毕后，图灵机就进入了“停机状态”，这时纸带上的符号或者“Accept”、“Reject”状态就是本次运行的结果。</p><p>图灵设计的这样一台机器能模拟人类所能进行的任何计算过程。不考虑效率的前提下，今天人类的再高端的计算机的程序运算都可以通过图灵机实现。</p><p>2、非确定图灵机（NDTM，Non-Deterministic Turing Machine）</p><p>非确定图灵机与前面说的图灵机的区别在于，“控制规则TABLE”中的确定的下一步动作不是一个，而是多个。在实际运行过程中，NDTM会随机选择某一个动作继续运行下去，形成一个运行分支。因此，NDTM针对同一个输入，实际运行结果是不确定的。但是有一点需要明确，就是NDTM中不会产生矛盾，某一个输入如果某个分支给出的结果是“接受”，那么无论选择哪一条道路，最终的结果都不会是“拒绝”（这里我们没有排除某种分支下某个NDTM无限运行下去，永不停机的可能）。</p><p>3、库克定理证明思路</p><p>做了这么多铺垫，我们来开始说库克定理的证明思路。<b>请大家千万别跳过这一部分，这是NPC问题存在的鼻祖式证明</b>。</p><blockquote>（1）按照NPC问题的定义，先要证明SAT问题是一个NP问题。<br>这个证明太显然了。要验证某个SAT问题，只需要把任意给定的n个逻辑变量的取值带入那个逻辑表达式运算一下，看结果是否为真即可。因为逻辑表达式都是基于“与、或、非”这几种运算的，这个运算过程必然是在以n为变量的多项式步骤内可以完成的。<br><br>（2）难的是这第二步，要证明任意一个NP类问题都可以在多项式时间复杂度内规约为SAT问题。<br>这个证明最主要难在如何处理“任意”这个条件。NP类问题无穷多种，它们唯一的共同点就是给出一个待定解，可以在多项式时间复杂度内判断是否真的是这个问题的解。库克的论文中给出了一个非常巧妙的思路，充分利用这个唯一的共同点，基于非确定图灵机的模型及其运行过程，完成对一个逻辑表达式的构造。而这个构造出来的逻辑表达式对应的SAT问题，就是被规约到的问题。<br>（I）对于任意一个NP类问题，既然都可以在多项式时间复杂度内完成对某个待定解的判定，那么对应这个待定解，也必然存在着一个相应的图灵机来完成这个判定过程，且这个图灵机的运行时间复杂度是多项式级的。既然该问题的每个待定解都对应着一个图灵机的判定过程，那么也就意味着存在一个非确定图灵机（NDTM），对于每个待定解，这个NDTM都存在着某个运行分支可以完成相应的判定过程。特别地，对于每个真正的解，相应的NDTM运行分支一定会给出“接受”的结果。<br>（II）由于对任意一个NP类问题，相应的NDTM是确定的，显然其控制规则TABLE也是确定的。而且，对于NDTM的每个运行分支，都有一条确定的运行路径。<br>我们假设类似上图的TABLE表中有t行状态，分别为{Q0, Q1, Q2, ......, Qt}，我们设第i步的状态为 <img src=""https://www.zhihu.com/equation?tex=q_i%5Cin"" alt=""q_i\in"" eeimg=""1""> {Q0, Q1, Q2, ......, Qt}；又因为这个NDTM至多运行P(n)步（P(n)是n的某个多项式函数），因此至多纸带上有P(n)+1个格子被读写，于是我们设初始输入为 <img src=""https://www.zhihu.com/equation?tex=S_0"" alt=""S_0"" eeimg=""1""> ，第i步的时候纸带上的字符集合为 <img src=""https://www.zhihu.com/equation?tex=S_i"" alt=""S_i"" eeimg=""1""> ，这里每个 <img src=""https://www.zhihu.com/equation?tex=S_i"" alt=""S_i"" eeimg=""1""> 的长度都是P(n)+1 。<br>于是，NDTM的每个运行分支都会形成一组确定的（ <img src=""https://www.zhihu.com/equation?tex=q_i%2C+S_i"" alt=""q_i, S_i"" eeimg=""1""> ）序列。如果序列的最后一个 <img src=""https://www.zhihu.com/equation?tex=q_%7BP%28n%29%2B1%7D"" alt=""q_{P(n)+1}"" eeimg=""1""> 状态是“Accept”，就说明输入 <img src=""https://www.zhihu.com/equation?tex=S_0"" alt=""S_0"" eeimg=""1""> 是这个NP类问题的解。<br>（III）<b>核心步骤到了</b>：如果某个输入 <img src=""https://www.zhihu.com/equation?tex=S_0"" alt=""S_0"" eeimg=""1""> 是那个NP类问题的解，就意味着至少存在一个按照上述定义确定的、符合这个NDTM运行逻辑规则的（ <img src=""https://www.zhihu.com/equation?tex=q_i%2C+S_i"" alt=""q_i, S_i"" eeimg=""1""> ）序列，且其 <img src=""https://www.zhihu.com/equation?tex=q_%7BP%28n%29%2B1%7D"" alt=""q_{P(n)+1}"" eeimg=""1""> 状态是“Accept”；反过来，如果我能找到一个按照上述定义确定的、符合这个NDTM运行逻辑规则的（ <img src=""https://www.zhihu.com/equation?tex=q_i%2C+S_i"" alt=""q_i, S_i"" eeimg=""1""> ）序列，且其 <img src=""https://www.zhihu.com/equation?tex=q_%7BP%28n%29%2B1%7D"" alt=""q_{P(n)+1}"" eeimg=""1""> 状态是“Accept”，那么这组序列的初始输入 <img src=""https://www.zhihu.com/equation?tex=S_0"" alt=""S_0"" eeimg=""1""> 就必然是那个NP类问题的解。<br>剩下的事就是构造一个逻辑表达式，使得这个逻辑表达式得到满足的时候，就对应着一个符合这个NDTM运行规则的（ <img src=""https://www.zhihu.com/equation?tex=q_i%2C+S_i"" alt=""q_i, S_i"" eeimg=""1""> ）序列，且其 <img src=""https://www.zhihu.com/equation?tex=q_%7BP%28n%29%2B1%7D"" alt=""q_{P(n)+1}"" eeimg=""1""> 状态是“Accept”。由于图灵机运行规则的确定性，这个逻辑表达式显然存在，且其构造不能用“难”来形容，而应该用“复杂”来形容，需要细致、认真、准确的态度和基本的逻辑能力。本文就不赘述了，有兴趣的可以参看相应专业的参考书。<br>（IV）既然我们构造出了一个逻辑表达式，一旦这个逻辑表达式被满足了（这正好是一个SAT问题），就意味着那个NP类问题有解了，那么显然那个NP类问题已经被规约到了一个SAT问题。而且，构造过程其实就是NDTM这个分支的运行过程，其时间复杂度当然是多项式级别的。由此，库克证明了任意NP类问题都可以在多项式时间复杂度内规约为一个SAT问题。</blockquote><p><b>三、其它已经发现的NPC问题</b></p><p>“SAT问题”是在1971年由库克发现的第一个NPC问题。在之后的1972年，理查德·卡普将这个想法往前推进，发表了他著名的论文“Reducibility Among Combinatorial Problems”，在里面提出并证明了21个NPC问题。这些被证明为NPC的问题都是比较有名的组合数学与图论等方面的问题，包括“0-1整数规划”、“集合覆盖”、“哈密顿循环”、“3-SAT”问题、“背包问题”、“三位匹配问题”等等。</p><p>其中“哈密顿循环”演化而成的比较著名的一个问题叫做“推销员问题”，是说某个推销员要走访n个城市，每两个城市之间的距离都给出了，推销员从某给定的城市出发，要求每个城市走访一次后再回到出发城市，那么怎么安排走访顺序使得总行程最短？</p><p>这个问题在n的多项式时间复杂度内目前是无法求解的。如果用最笨的穷举法，总共可能的顺序有(n-1)!种情况，每种情况还需要计算n-1次加法，其时间复杂度为O((n-1)*(n-1)!) = O(n!)。当然，经过优化，肯定存在更理想的算法，比如通过动态规划精确算法，可以达到的时间复杂度为O( <img src=""https://www.zhihu.com/equation?tex=n%5E2%5Ccdot+2%5En"" alt=""n^2\cdot 2^n"" eeimg=""1""> )。</p><p><b>四、关于NP是否等于P</b></p><p>2010年8月6日，HP LAB的 Vinay Deolalikar 教授宣布证明了 <img src=""https://www.zhihu.com/equation?tex=NP%5Cneq+P"" alt=""NP\neq P"" eeimg=""1""> ，在他的主页上证明过程已经公布（PDF格式共103页），但在8月15日，专家学者对论文的看法基本达成共识，那就是证明不能成立。</p><p>时至今日，还没有哪位学者能够给出确定性的结论，但是计算复杂度理论的学者普遍认为<img src=""https://www.zhihu.com/equation?tex=NP%5Cneq+P"" alt=""NP\neq P"" eeimg=""1""> 。</p><p>当然，从非证明的角度来看这个问题的话，存在那么多个NPC问题，只要有一个能够找到多项式时间复杂度的算法，那么NP=P就成立了，可是50多年下来，一个都没有找到，这也从反面说明了<img src=""https://www.zhihu.com/equation?tex=NP%5Cneq+P"" alt=""NP\neq P"" eeimg=""1""> 的可能性很大。</p><p>我还看到过更具情感色彩的观点。麻省理工学院的科学家Scott Aronson在一篇博文中提出了10个<img src=""https://www.zhihu.com/equation?tex=NP%5Cneq+P"" alt=""NP\neq P"" eeimg=""1""> 的理由，我把第9个理由的大致内容记述在这里：<i>如果NP=P，那么“创造性的飞跃”将没有特殊价值，在解决问题与认可解决方案之间没有根本的隔阂，任何能欣赏交响乐的人都是莫扎特，每个能看懂一步一步的数学证明的人都是高斯，每个能认识到好的投资策略的人都是巴菲特</i>。</p><p>也许正是由于NP=P会使这个世界太过于无趣，所以上帝不允许这样的事情发生。</p>",367,12,,遥远地方剑星,https://api.zhihu.com/people/cc0f2fc8e6e43c25485449a2a83ddd26,赞同了文章
1583873374720,工业界中NLP(自然语言处理)算法工程师的核心竞争力是什么？,,,,,https://api.zhihu.com/questions/296661606,,,,1583873374,,,,0,,吴海波,https://api.zhihu.com/people/42b67e693fb4addfe21f59919872f74f,关注了问题
1583460380744,面试官如何判断面试者的机器学习水平？,,,,,https://api.zhihu.com/questions/62482926,,,,1583460380,,,,1,,简枫,https://api.zhihu.com/people/147fc19c3801b056e6437864322c8836,关注了问题
1583407913987,图像风格迁移(Neural Style)简史,,,,,https://api.zhihu.com/articles/26746283,,,,1583407913,,"<h2>图像风格迁移(Neural Style)简史</h2><p>面向读者：没有或有一定机器学习经验并对Prisma之类的app背后的原理感兴趣的读者。比较有经验的读者可以直接参照科技树阅读文章末罗列的引用论文。<br>阅读时间：10-20分钟<br>注：多图，请注意流量。<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-526f16430324d3fbd8c07ff3d1c05c0b_b.jpg"" data-caption="""" data-size=""normal"" class=""content_image""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='0'%20height='0'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" class=""content_image lazy"" data-actualsrc=""https://pic1.zhimg.com/v2-526f16430324d3fbd8c07ff3d1c05c0b_b.jpg""></figure><p><i>图像风格迁移科技树</i></p><h2>序：什么是图像风格迁移？</h2><p>先上一组图吧。以下每一张图都是一种不同的艺术风格。作为非艺术专业的人，我就不扯艺术风格是什么了，每个人都有每个人的见解，有些东西大概艺术界也没明确的定义。如何要把一个图像的风格变成另一种风格更是难以定义的问题。对于程序员，特别是对于机器学习方面的程序员来说，这种模糊的定义简直就是噩梦。到底怎么把一个说都说不清的东西变成一个可执行的程序，是困扰了很多图像风格迁移方面的研究者的问题。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-4113959ce95593435c213e58e651d992_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1001"" data-rawheight=""784"" class=""origin_image zh-lightbox-thumb"" width=""1001"" data-original=""https://pic3.zhimg.com/v2-4113959ce95593435c213e58e651d992_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1001'%20height='784'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1001"" data-rawheight=""784"" class=""origin_image zh-lightbox-thumb lazy"" width=""1001"" data-original=""https://pic3.zhimg.com/v2-4113959ce95593435c213e58e651d992_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-4113959ce95593435c213e58e651d992_b.jpg""></figure><p><br></p><p>在神经网络之前，图像风格迁移的程序有一个共同的思路：分析某一种风格的图像，给那一种风格建立一个数学或者统计模型，再改变要做迁移的图像让它能更好的符合建立的模型。这样做出来效果还是不错的，比如下面的三张图中所示，但一个很大的缺点：<b><i>一个程序基本只能做某一种风格或者某一个场景</i></b>。因此基于传统风格迁移研究的实际应用非常有限。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-59558f2733d67e635eae6fa6a62e0039_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""4299"" data-rawheight=""1223"" class=""origin_image zh-lightbox-thumb"" width=""4299"" data-original=""https://pic2.zhimg.com/v2-59558f2733d67e635eae6fa6a62e0039_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='4299'%20height='1223'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""4299"" data-rawheight=""1223"" class=""origin_image zh-lightbox-thumb lazy"" width=""4299"" data-original=""https://pic2.zhimg.com/v2-59558f2733d67e635eae6fa6a62e0039_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-59558f2733d67e635eae6fa6a62e0039_b.jpg""></figure><p><br></p><p><a href=""http://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2508419"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">景色照片时间迁移</a></p><p>改变了这种现状的是两篇Gatys的论文，在这之前让程序模仿任意一张图片画画是没法想象的。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-77e4dc591850616514eca142dcc79daa_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1055"" data-rawheight=""1201"" class=""origin_image zh-lightbox-thumb"" width=""1055"" data-original=""https://pic4.zhimg.com/v2-77e4dc591850616514eca142dcc79daa_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1055'%20height='1201'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1055"" data-rawheight=""1201"" class=""origin_image zh-lightbox-thumb lazy"" width=""1055"" data-original=""https://pic4.zhimg.com/v2-77e4dc591850616514eca142dcc79daa_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-77e4dc591850616514eca142dcc79daa_b.jpg""></figure><p><br></p><p><i><a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06576"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">第一个基于神经网络的图像风格迁移算法</a>，生成时间：5-20分钟</i> </p><p>这篇文章中你不会看到数学公式，如果想要更加详细了解其中的数学的话可以阅读原论文。我想试着从头开始讲起，从<a href=""http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Gatys et al., 2015a</a>和<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06576"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Gatys et al., 2015b</a>中用到的一些技术的历史开始讲起，用最简单的方法说清楚基于神经网络的图像风格迁移的思路是什么，以及Gatys为什么能够想到使用神经网络来实现图像风格迁移。</p><p>如果大家对这个感兴趣的话，我将来可以继续写一些关于Neural Style最新的一些研究的进展，或者其他相关的一些图像生成类的研究，对抗网络之类的。写的有错误的不到位的地方请随意指正。</p><h2>Neural Style元年前20年-前3年</h2><p>要理解对于计算机来说图片的风格是什么，只能追根溯源到2000年以及之前的图片纹理生成的研究上。明明是图像风格迁移的文章，为啥要说到图片纹理？在这儿我先卖个关子吧。</p><p>据我所知，在2015年前所有的关于图像纹理的论文都是手动建模的（比如 <a href=""http://link.zhihu.com/?target=http%3A//www.springerlink.com/index/r244h74572250895.pdf"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients</a>），其中用到的最重要的一个思想是：纹理可以用图像局部特征的统计模型来描述。没有这个前提一切模型无从谈起。什么是统计特征呢，简单的举个栗子<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-bc1e092d498218c1f880bf9559ca41eb_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""450"" data-rawheight=""308"" class=""origin_image zh-lightbox-thumb"" width=""450"" data-original=""https://pic1.zhimg.com/v2-bc1e092d498218c1f880bf9559ca41eb_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='450'%20height='308'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""450"" data-rawheight=""308"" class=""origin_image zh-lightbox-thumb lazy"" width=""450"" data-original=""https://pic1.zhimg.com/v2-bc1e092d498218c1f880bf9559ca41eb_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-bc1e092d498218c1f880bf9559ca41eb_b.jpg""></figure><p>这个图片可以被称作栗子的纹理，这纹理有个特征，就是所有的栗子都有个开口，用简单的数学模型表示开口的话，就是两条大概某个弧度的弧线相交嘛，统计学上来说就是这种纹理有两条这个弧度的弧线相交的概率比较大，这种可以被称为统计特征。有了这个前提或者思想之后，研究者成功的用复杂的数学模型和公式来归纳和生成了一些纹理，但毕竟手工建模耗时耗力，（通俗的说，想象一下手工算栗子开口的数学模型，算出来的模型大概除了能套用在开心果上就没啥用了。。。）当时计算机计算能力还没现在的手机强，这方面的研究进展缓慢，十几年就这么过去了。<br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-55554235048e0759eae2d0f43c5ebf8d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""810"" data-rawheight=""1107"" class=""origin_image zh-lightbox-thumb"" width=""810"" data-original=""https://pic2.zhimg.com/v2-55554235048e0759eae2d0f43c5ebf8d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='810'%20height='1107'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""810"" data-rawheight=""1107"" class=""origin_image zh-lightbox-thumb lazy"" width=""810"" data-original=""https://pic2.zhimg.com/v2-55554235048e0759eae2d0f43c5ebf8d_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-55554235048e0759eae2d0f43c5ebf8d_b.jpg""></figure><p><a href=""http://link.zhihu.com/?target=http%3A//www.springerlink.com/index/r244h74572250895.pdf"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">早期纹理生成结果</a></p><p>与此同时，隔壁的图像风格迁移也好不到哪里去，甚至比纹理生成还惨。因为纹理生成至少不管生成什么样子的纹理都叫纹理生成，然而图像风格迁移这个领域当时连个合适的名字都没有，因为每个风格的算法都是各管各的，互相之间并没有太多的共同之处。比如<a href=""http://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2811248"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">油画风格迁移</a>，里面用到了7种不同的步骤来描述和迁移油画的特征。又比如<a href=""http://link.zhihu.com/?target=https%3A//dspace.mit.edu/handle/1721.1/100018"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">头像风格迁移</a>里用到了三个步骤来把一种头像摄影风格迁移到另一种上。以上十个步骤里没一个重样的，可以看出图像风格处理的研究在2015年之前基本都是各自为战，捣鼓出来的算法也没引起什么注意。相比之下Photoshop虽然要手动修图，但比大部分算法好用多了。<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-fc8c6a5a840ea83f5e5d3241707d08bc_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""998"" data-rawheight=""445"" class=""origin_image zh-lightbox-thumb"" width=""998"" data-original=""https://pic2.zhimg.com/v2-fc8c6a5a840ea83f5e5d3241707d08bc_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='998'%20height='445'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""998"" data-rawheight=""445"" class=""origin_image zh-lightbox-thumb lazy"" width=""998"" data-original=""https://pic2.zhimg.com/v2-fc8c6a5a840ea83f5e5d3241707d08bc_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-fc8c6a5a840ea83f5e5d3241707d08bc_b.jpg""></figure><p><br></p><p><a href=""http://link.zhihu.com/?target=https%3A//dspace.mit.edu/handle/1721.1/100018"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">头像风格迁移</a></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-61b9db9ae9be6fdf5994af21124ce8eb_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1286"" data-rawheight=""412"" class=""origin_image zh-lightbox-thumb"" width=""1286"" data-original=""https://pic4.zhimg.com/v2-61b9db9ae9be6fdf5994af21124ce8eb_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1286'%20height='412'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1286"" data-rawheight=""412"" class=""origin_image zh-lightbox-thumb lazy"" width=""1286"" data-original=""https://pic4.zhimg.com/v2-61b9db9ae9be6fdf5994af21124ce8eb_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-61b9db9ae9be6fdf5994af21124ce8eb_b.jpg""></figure><p><br></p><p><a href=""http://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2811248"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">油画风格迁移</a></p><p>同一个时期，计算机领域进展最大的研究之一可以说是计算机图形学了。（这段有相关知识的可以跳过，不影响之后的阅读。）简单的来说计算机图形学就是现在几乎所有游戏的基础，不论是男友1(战地1)里穿越回一战的战斗场景，还是FGO之类的手游，背后都少不了一代又一代的图形学研究者的工作。在他们整日整夜忙着研究如何能让程序里的妹纸变成有血有肉的样子的时候，点科技树点出了一个重要的分支：显卡（GPU）。游戏机从刚诞生开始就伴随着显卡。显卡最大的功能当然是处理和显示图像。不同于CPU的是，CPU早期是单线程的，也就是一次只能处理一个任务，GPU可以一次同时处理很多任务，虽然单个任务的处理能力和速度比CPU差很多。比如一个128x128的超级马里奥游戏， 用CPU处理的话，每一帧都需要运行128x128=16384歩，而GPU因为可以同时计算所有像素点，时间上只需要1步，速度比CPU快很多。为了让游戏越来越逼近现实，显卡在过去20年内也变得越来越好。巧合的是，显卡计算能力的爆炸性增长直接导致了被放置play十几年的神经网络的复活和深度学习的崛起，因为神经网络和游戏图形计算的相似处是两者都需要对大量数据进行重复单一的计算。可以说如果没有游戏界就没有深度学习，也就没有Neural Style。所以想学机器学习先得去steam买买买支持显卡研究（误）。<br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-bae83305475ea7033e17b38c8bb2b42b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""617"" data-rawheight=""354"" class=""origin_image zh-lightbox-thumb"" width=""617"" data-original=""https://pic3.zhimg.com/v2-bae83305475ea7033e17b38c8bb2b42b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='617'%20height='354'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""617"" data-rawheight=""354"" class=""origin_image zh-lightbox-thumb lazy"" width=""617"" data-original=""https://pic3.zhimg.com/v2-bae83305475ea7033e17b38c8bb2b42b_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-bae83305475ea7033e17b38c8bb2b42b_b.jpg""></figure><p><i>ImageNet物体识别比赛中使用GPU的队伍数量逐年上升，错误率逐年下降</i></p><p>提到神经网络我想稍微讲一下神经网络（特别是卷积神经网络）和传统做法的区别，已经有了解的可以跳过本段。卷积神经网络分为很多层，每一层都是由很多单个的人工神经元组成的。可以把每个神经元看作一个识别器，用刚刚的栗子来说的话，每一个或者几个神经元的组合都可以被用来识别某个特征，比如栗子的开口。在训练前它们都是随机的，所以啥都不能做，训练的过程中它们会自动的被变成一个个不同的识别器并且相互组合起来，大量的识别器组合起来之后就可以识别物体了。整个过程除了一开始的神经网络的设计和参数的调整之外其他全是自动的。这里我们就不介绍神经网络(Neural Network)和卷积神经网络(Convolutional Neural Network)具体怎么工作的了，如果对于神经网络具体怎么工作不了解的话，相信网上已经有很多很多相关的介绍和教程，有兴趣的可以去了解一下，不了解也不影响本文的阅读。<br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-57f9671f9f19b71432c570aa1d8b7a0d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""560"" data-rawheight=""253"" class=""origin_image zh-lightbox-thumb"" width=""560"" data-original=""https://pic4.zhimg.com/v2-57f9671f9f19b71432c570aa1d8b7a0d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='560'%20height='253'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""560"" data-rawheight=""253"" class=""origin_image zh-lightbox-thumb lazy"" width=""560"" data-original=""https://pic4.zhimg.com/v2-57f9671f9f19b71432c570aa1d8b7a0d_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-57f9671f9f19b71432c570aa1d8b7a0d_b.jpg""></figure><p><i>卷积神经网络图例</i></p><h2>Neural Style元年前3年-前1年</h2><p>2012-2014年的时候深度学习刚开始火，火的一个主要原因是因为人们发现深度学习可以用来训练物体识别的模型。之前的物体识别模型有些是用几何形状和物体的不同部分比较来识别，有些按颜色，有些按3d建模，还有一些按照局部特征。传统物体识别算法中值得一提的是按照比较局部特征来识别物体，其原理如下：</p><p>比如我们的目标是在图片之中找到这个人：<br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-c03d1a4ad9d19b87c5455c16fe22606e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""147"" data-rawheight=""182"" class=""content_image"" width=""147""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='147'%20height='182'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""147"" data-rawheight=""182"" class=""content_image lazy"" width=""147"" data-actualsrc=""https://pic4.zhimg.com/v2-c03d1a4ad9d19b87c5455c16fe22606e_b.jpg""></figure><p><i>目标物体</i><br>对于程序而言这个人就是一堆像素嘛，让它直接找的话它只能一个个像素的去比较然后返回最接近的了（近邻算法）。但是现实中物体的形状颜色会发生变化，如果手头又只有这一张照片，直接去找的速度和正确率实在太低。<br>有研究者想到，可以把这个人的照片拆成许多小块，然后一块一块的比较(方法叫Bag of Features)。最后哪一块区域相似的块数最多就把那片区域标出来。这种做法的好处在于即使识别一个小块出了问题，还有其他的小块能作为识别的依据，发生错误的风险比之前大大降低了。<br><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-b5244bd30b6dce5ee65c660185678ea8_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""344"" data-rawheight=""446"" class=""content_image"" width=""344""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='344'%20height='446'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""344"" data-rawheight=""446"" class=""content_image lazy"" width=""344"" data-actualsrc=""https://pic4.zhimg.com/v2-b5244bd30b6dce5ee65c660185678ea8_b.jpg""></figure><p><i>Bag of Features</i></p><p>这种做法最大的缺点就是它还是把一个小块看成一坨像素然后按照像素的数值去比较，之前提到的改变光照改变形状导致物体无法被识别的问题根本上并没有得到解决。</p><p>用卷积神经网络做的物体识别器其实原理和bag of features差不了太多，<b><i>只是把有用的特征(feature)都装到了神经网络里了</i></b>。刚提到了神经网络经过训练会自动提取最有用的特征，所以特征也不再只是单纯的把原来的物体一小块一小块的切开产生的，而是由神经网络选择最优的方式提取。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-c29a52fa109192d2f6df1869bd78f8b0_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""560"" data-rawheight=""558"" class=""origin_image zh-lightbox-thumb"" width=""560"" data-original=""https://pic4.zhimg.com/v2-c29a52fa109192d2f6df1869bd78f8b0_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='560'%20height='558'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""560"" data-rawheight=""558"" class=""origin_image zh-lightbox-thumb lazy"" width=""560"" data-original=""https://pic4.zhimg.com/v2-c29a52fa109192d2f6df1869bd78f8b0_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-c29a52fa109192d2f6df1869bd78f8b0_b.jpg""></figure><p><i>卷积神经网络提取的特征示意图，每一格代表一个神经元最会被哪种图片激活。</i></p><p>卷积神经网络当时最出名的一个物体识别网络之一叫做VGG19，结构如下：<br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-e93222f0c3892ab00845dcd75e531386_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""638"" data-rawheight=""479"" class=""origin_image zh-lightbox-thumb"" width=""638"" data-original=""https://pic3.zhimg.com/v2-e93222f0c3892ab00845dcd75e531386_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='638'%20height='479'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""638"" data-rawheight=""479"" class=""origin_image zh-lightbox-thumb lazy"" width=""638"" data-original=""https://pic3.zhimg.com/v2-e93222f0c3892ab00845dcd75e531386_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-e93222f0c3892ab00845dcd75e531386_b.jpg""></figure><p><i>VGG19网络结构</i><br>每一层神经网络都会利用上一层的输出来进一步提取更加复杂的特征，直到复杂到能被用来识别物体为止，<i><b>所以每一层都可以被看做很多个局部特征的提取器</b></i>。VGG19在物体识别方面的精度甩了之前的算法一大截，之后的物体识别系统也基本都改用深度学习了。</p><p>因为VGG19的优秀表现，引起了很多兴趣和讨论，但是VGG19具体内部在做什么其实很难理解，因为每一个神经元内部参数只是一堆数字而已。每个神经元有几百个输入和几百个输出，一个一个去梳理清楚神经元和神经元之间的关系太难。于是有人想出来一种办法：虽然我们不知道神经元是怎么工作的，但是如果我们知道了它的激活条件，会不会能对理解神经网络更有帮助呢？于是他们编了一个程序，（用的方法叫back propagation，和训练神经网络的方法一样，只是倒过来生成图片。）把每个神经元所对应的能激活它的图片找了出来，之前的那幅特征提取示意图就是这么生成的。有人在这之上又进一步，觉得，诶既然我们能找到一个神经元的激活条件，那能不能把所有关于“狗’的神经元找出来，让他们全部被激活，然后看看对于神经网络来说”狗“长什么样子的？<br>长得其实是这样的：<br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-d793ef70697509e624bc043457b67997_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1131"" data-rawheight=""707"" class=""origin_image zh-lightbox-thumb"" width=""1131"" data-original=""https://pic3.zhimg.com/v2-d793ef70697509e624bc043457b67997_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1131'%20height='707'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1131"" data-rawheight=""707"" class=""origin_image zh-lightbox-thumb lazy"" width=""1131"" data-original=""https://pic3.zhimg.com/v2-d793ef70697509e624bc043457b67997_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-d793ef70697509e624bc043457b67997_b.jpg""></figure><p><i>神经网络想象中的狗</i></p><p>这是神经网络想象中最完美的狗的样子，非常迷幻，感觉都可以自成一派搞个艺术风格出来了。而能把任何图片稍作修改让神经网络产生那就是狗的幻觉的程序被称作deep dream。<br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-3877e2da44845fd8643d21bd4473c02e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1280"" data-rawheight=""720"" class=""origin_image zh-lightbox-thumb"" width=""1280"" data-original=""https://pic2.zhimg.com/v2-3877e2da44845fd8643d21bd4473c02e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1280'%20height='720'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1280"" data-rawheight=""720"" class=""origin_image zh-lightbox-thumb lazy"" width=""1280"" data-original=""https://pic2.zhimg.com/v2-3877e2da44845fd8643d21bd4473c02e_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-3877e2da44845fd8643d21bd4473c02e_b.jpg""></figure><p><i>Deep Dream</i></p><h2>Neural Style元年</h2><p>有了这么多铺垫，一切的要素已经凑齐，前置科技树也都已经被点亮了，终于可以进入正题了。基于神经网络的图像风格迁移在2015年由Gatys et al. 在两篇论文中提出：<a href=""http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Gatys et al., 2015a</a>和<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06576"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Gatys et al., 2015b</a>。我们先说第一篇。第一篇比起之前的纹理生成算法，创新点只有一个：它给了一种用深度学习来给纹理建模的方法。之前说到纹理生成的一个重要的假设是纹理能够通过局部统计模型来描述，而手动建模方法太麻烦。于是Gatys看了一眼隔壁的物体识别论文，发现VGG19说白了不就是一堆局部特征识别器嘛。他把事先训练好的网络拿过来一看，发现这些识别器还挺好用的。于是Gatys套了个Gramian matrix上去算了一下那些不同局部特征的相关性，把它变成了一个统计模型，于是就有了一个不用手工建模就能生成纹理的方法。<br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-99f6ed199359a99d9b8524d7f86ac328_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""888"" data-rawheight=""666"" class=""origin_image zh-lightbox-thumb"" width=""888"" data-original=""https://pic1.zhimg.com/v2-99f6ed199359a99d9b8524d7f86ac328_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='888'%20height='666'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""888"" data-rawheight=""666"" class=""origin_image zh-lightbox-thumb lazy"" width=""888"" data-original=""https://pic1.zhimg.com/v2-99f6ed199359a99d9b8524d7f86ac328_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-99f6ed199359a99d9b8524d7f86ac328_b.jpg""></figure><p><i>基于神经网络的纹理生成算法</i></p><p>从纹理到图片风格其实只差两步。第一步也是比较神奇的，是Gatys发现<b><i>纹理能够描述一个图像的风格</i></b>。严格来说文理只是图片风格的一部分，但是不仔细研究纹理和风格之间的区别的话，乍一看给人感觉还真差不多。第二步是<b><i>如何只提取图片内容而不包括图片风格</i></b>。这两点就是他的第二篇论文做的事情：Gatys又偷了个懒，把物体识别模型再拿出来用了一遍，这次不拿Gramian算统计模型了，直接把局部特征看做近似的图片内容，这样就得到了一个把图片内容和图片风格（说白了就是纹理）分开的系统，剩下的就是把一个图片的内容和另一个图片的风格合起来。合起来的方法用的正是之前提到的让神经网络“梦到”狗的方法，也就是研究员们玩出来的Deep Dream，找到能让合适的特征提取神经元被激活的图片即可。<br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-7fbd6cd3d0fe9bb7941aa4dcffe6d9ab_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""950"" data-rawheight=""765"" class=""origin_image zh-lightbox-thumb"" width=""950"" data-original=""https://pic4.zhimg.com/v2-7fbd6cd3d0fe9bb7941aa4dcffe6d9ab_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='950'%20height='765'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""950"" data-rawheight=""765"" class=""origin_image zh-lightbox-thumb lazy"" width=""950"" data-original=""https://pic4.zhimg.com/v2-7fbd6cd3d0fe9bb7941aa4dcffe6d9ab_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-7fbd6cd3d0fe9bb7941aa4dcffe6d9ab_b.jpg""></figure><p><i>基于神经网络的图像风格迁移</i></p><p>至此，我们就把关于基于神经网络的图像风格迁移(Neural Style)的重点解释清楚了。背后的每一步都是前人研究的结果，不用因为名字里带深度啊神经网络啊而感觉加了什么特技，特别的高级。Gatys所做的改进是把两个不同领域的研究成果有机的结合了起来，做出了令人惊艳的结果。其实最让我惊讶的是纹理竟然能够和人们心目中认识到的图片的风格在很大程度上相吻合。（和真正的艺术风格有很大区别，但是看上去挺好看的。。。）从那之后对neural style的改进也层出不穷，在这里就先放一些图，技术细节暂且不表。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-b483638c8b37dbd9e2c57eea652f3dc3_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""542"" data-rawheight=""494"" class=""origin_image zh-lightbox-thumb"" width=""542"" data-original=""https://pic4.zhimg.com/v2-b483638c8b37dbd9e2c57eea652f3dc3_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='542'%20height='494'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""542"" data-rawheight=""494"" class=""origin_image zh-lightbox-thumb lazy"" width=""542"" data-original=""https://pic4.zhimg.com/v2-b483638c8b37dbd9e2c57eea652f3dc3_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-b483638c8b37dbd9e2c57eea652f3dc3_b.jpg""></figure><p><i><a href=""http://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/html/Li_Combining_Markov_Random_CVPR_2016_paper.html"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">改进后的图像风格迁移算法</a>，左：输入图像，中：改进前，右：改进后。生成时间：5-20分钟</i> </p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-d0c4a9e4008e25962beea47e10d3c508_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1183"" data-rawheight=""845"" class=""origin_image zh-lightbox-thumb"" width=""1183"" data-original=""https://pic3.zhimg.com/v2-d0c4a9e4008e25962beea47e10d3c508_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1183'%20height='845'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1183"" data-rawheight=""845"" class=""origin_image zh-lightbox-thumb lazy"" width=""1183"" data-original=""https://pic3.zhimg.com/v2-d0c4a9e4008e25962beea47e10d3c508_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-d0c4a9e4008e25962beea47e10d3c508_b.jpg""></figure><p><i><a href=""http://link.zhihu.com/?target=https%3A//research.google.com/pubs/pub45832.html"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">多个预设风格的融合</a>，生成时间：少于1秒，训练时间：每个风格1-10小时</i></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-1588808c905d85d9b71572777515a947_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1683"" data-rawheight=""2178"" class=""origin_image zh-lightbox-thumb"" width=""1683"" data-original=""https://pic3.zhimg.com/v2-1588808c905d85d9b71572777515a947_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1683'%20height='2178'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1683"" data-rawheight=""2178"" class=""origin_image zh-lightbox-thumb lazy"" width=""1683"" data-original=""https://pic3.zhimg.com/v2-1588808c905d85d9b71572777515a947_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-1588808c905d85d9b71572777515a947_b.jpg""></figure><p><i><a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.04337"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">最新的实时任意风格迁移算法之一</a>，生成时间：少于10秒（<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06868"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">少于一秒的算法</a>也有，但个人认为看上去没这个好看），训练时间：10小时</i> </p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-ec79ae0b94c79aede2f4315a3afa19ef_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1012"" data-rawheight=""859"" class=""origin_image zh-lightbox-thumb"" width=""1012"" data-original=""https://pic2.zhimg.com/v2-ec79ae0b94c79aede2f4315a3afa19ef_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1012'%20height='859'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1012"" data-rawheight=""859"" class=""origin_image zh-lightbox-thumb lazy"" width=""1012"" data-original=""https://pic2.zhimg.com/v2-ec79ae0b94c79aede2f4315a3afa19ef_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-ec79ae0b94c79aede2f4315a3afa19ef_b.jpg""></figure><p><i><a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.01088"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">图片类比</a>，生成时间：5-20分钟</i> </p><p>最后安利一篇与本文无关的文章吧，<a href=""http://link.zhihu.com/?target=http%3A//distill.pub/2017/research-debt/"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Research Debt</a> (原文为英语，相关知乎问题在<a href=""https://www.zhihu.com/question/57639134"" class=""internal"">这里</a>）是我写本文的动机。希望各位喜欢本文，也希望有余力的人能多写一些科普文。文笔不好献丑了。</p><h2>引用：</h2><p>注：排序基本按照时间顺序，带星号越多越重要，这里只引用了文章中提到过的论文，若有需要以后再加。</p><p>前置科技： <br></p><a href=""http://link.zhihu.com/?target=http%3A//www.springerlink.com/index/r244h74572250895.pdf"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients</a><a href=""http://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2508419"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Data-driven hallucination of different times of day from a single outdoor photo</a><a href=""http://link.zhihu.com/?target=https%3A//dspace.mit.edu/handle/1721.1/100018"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Style Transfer for Headshot Portraits</a><a href=""http://link.zhihu.com/?target=http%3A//dl.acm.org/citation.cfm%3Fid%3D2811248"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Image stylization by oil paint ﬁltering using color palettes</a><p>基于神经网络的图像风格迁移： <br></p><a href=""http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">**Texture synthesis using convolutional neural networks</a><a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06576"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">***A neural algorithm of artistic style</a><a href=""http://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/html/Li_Combining_Markov_Random_CVPR_2016_paper.html"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">*Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis</a><a href=""http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">*Texture networks: Feed-forward synthesis of textures and stylized images</a><a href=""http://link.zhihu.com/?target=https%3A//research.google.com/pubs/pub45832.html"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">A Learned Representation For Artistic Style</a><a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.04337"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Fast Patch-based Style Transfer of Arbitrary Style</a><a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06868"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">*Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</a><a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.01088"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Visual Attribute Transfer through Deep Image Analogy</a><p></p>",488,47,,李嘉铭,https://api.zhihu.com/people/c58a2c9502b619ff92e6b5f36bc21b68,赞同了文章
1582443470081,arXiv每日学术速递,,,,,https://api.zhihu.com/columns/arxivdaily,,,,1582443470,,,,,,匿名用户,,关注了专栏
1581400542398,医疗知识图谱问答系统探究（一）,,,,,https://api.zhihu.com/articles/59401605,,,,1581400542,,"<p></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-651996b553bdeee9ea1eb350b00e67bd_b.jpg"" data-rawwidth=""467"" data-rawheight=""111"" data-size=""normal"" class=""origin_image zh-lightbox-thumb"" width=""467"" data-original=""https://pic2.zhimg.com/v2-651996b553bdeee9ea1eb350b00e67bd_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='467'%20height='111'&gt;&lt;/svg&gt;"" data-rawwidth=""467"" data-rawheight=""111"" data-size=""normal"" class=""origin_image zh-lightbox-thumb lazy"" width=""467"" data-original=""https://pic2.zhimg.com/v2-651996b553bdeee9ea1eb350b00e67bd_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-651996b553bdeee9ea1eb350b00e67bd_b.jpg""><figcaption>这是 阿拉灯神丁Vicky 的第 23 篇文章</figcaption></figure><h2><b>1、项目背景</b></h2><p>为通过项目实战增加对知识图谱的认识，几乎找了所有网上的开源项目及视频实战教程。</p><p>果然，功夫不负有心人，找到了<b>中科院软件所刘焕勇老师</b>在github上的开源项目，基于知识图谱的医药领域问答项目QABasedOnMedicaKnowledgeGraph。</p><p><b>项目地址：</b><a href=""http://link.zhihu.com/?target=https%3A//github.com/liuhuanyong/QASystemOnMedicalKG"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">github.com/liuhuanyong/</span><span class=""invisible"">QASystemOnMedicalKG</span><span class=""ellipsis""></span></a></p><p>用了两个晚上搭建了两套，Mac版与Windows版，哈哈，运行成功！！！</p><p>从无到有搭建一个以疾病为中心的一定规模医药领域知识图谱，以该知识图谱完成自动问答与分析服务。该项目立足医药领域，以垂直型医药网站为数据来源，以疾病为核心，构建起一个包含7类规模为4.4万的知识实体，11类规模约30万实体关系的知识图谱。 本项目将包括以下两部分的内容：</p><blockquote>1、基于垂直网站数据的医药知识图谱构建<br>2、基于医药知识图谱的自动问答</blockquote><h2><b>2、项目环境</b></h2><h2><b>2.1 windows系统</b></h2><p><b>搭建中间有很多坑，且行且注意。</b></p><p><b>配置要求：</b>要求配置neo4j数据库及相应的python依赖包。neo4j数据库用户名密码记住，并修改相应文件。</p><p>安装neo4j，neo4j 依赖java jdk 1.8版本以上：</p><blockquote>java jdk安装方法可参考：<a href=""http://link.zhihu.com/?target=https%3A//blog.csdn.net/yx1214442120/article/details/55098380"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">windows系统下安装JDK8</a>，下载地址：<a href=""http://link.zhihu.com/?target=https%3A//download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-windows-x64.exe"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">download.oracle.com/otn</span><span class=""invisible"">-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-windows-x64.exe</span><span class=""ellipsis""></span></a><br>安装neo4j可参考博文：<a href=""http://link.zhihu.com/?target=https%3A//blog.csdn.net/huanxuwu/article/details/80785986"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">windows安装neo4j</a>，下载地址：<a href=""http://link.zhihu.com/?target=https%3A//go.neo4j.com/download-thanks.html%3Fedition%3Dcommunity%26release%3D3.4.1%26flavour%3Dwinzip"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">go.neo4j.com/download-t</span><span class=""invisible"">hanks.html?edition=community&amp;release=3.4.1&amp;flavour=winzip</span><span class=""ellipsis""></span></a><br>安装python可参考：<a href=""http://link.zhihu.com/?target=https%3A//www.jianshu.com/p/edf7679d4aab"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Windows环境下安装python2.7</a></blockquote><p>根据neo4j 安装时的端口、账户、密码配置设置设置项目配置文件：<b>answer_search.py</b> &amp;  <b>build_medicalgraph.py</b> (github下载项目时根据个人需要也可使用git)</p><p><b>数据导入：</b>python build_medicalgraph.py，导入的数据较多，估计需要几个小时。</p><p>python build_medicalgraph.py导入数据之前，需要在该文件main函数中加入：</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-2d9a3e8ff2d3c62831488cf74d2002d6_b.jpg"" data-rawwidth=""548"" data-rawheight=""121"" data-size=""normal"" class=""origin_image zh-lightbox-thumb"" width=""548"" data-original=""https://pic2.zhimg.com/v2-2d9a3e8ff2d3c62831488cf74d2002d6_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='548'%20height='121'&gt;&lt;/svg&gt;"" data-rawwidth=""548"" data-rawheight=""121"" data-size=""normal"" class=""origin_image zh-lightbox-thumb lazy"" width=""548"" data-original=""https://pic2.zhimg.com/v2-2d9a3e8ff2d3c62831488cf74d2002d6_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-2d9a3e8ff2d3c62831488cf74d2002d6_b.jpg""><figcaption> build_medicalgraph.py</figcaption></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-ec983555835b7bc75e05c717ff9d4507_b.jpg"" data-rawwidth=""700"" data-rawheight=""169"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""700"" data-original=""https://pic1.zhimg.com/v2-ec983555835b7bc75e05c717ff9d4507_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='700'%20height='169'&gt;&lt;/svg&gt;"" data-rawwidth=""700"" data-rawheight=""169"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""700"" data-original=""https://pic1.zhimg.com/v2-ec983555835b7bc75e05c717ff9d4507_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-ec983555835b7bc75e05c717ff9d4507_b.jpg""></figure><p><b>启动问答：</b>python chat_graph.py</p><p><b>2.2 Mac系统</b></p><p>mac本身自带python、java jdk环境，可直接安装neo4j图数据库，项目运行步骤与windows基本一样。</p><p><b>问题解答：</b></p><blockquote><b>安装过程中如遇问题可联系Wechat: dandan-sbb。</b></blockquote><h2><b>2.3 Neo4j数据库展示</b></h2><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-952be99ec5699dfb478ea84a38913844_b.jpg"" data-rawwidth=""1240"" data-rawheight=""632"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-952be99ec5699dfb478ea84a38913844_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1240'%20height='632'&gt;&lt;/svg&gt;"" data-rawwidth=""1240"" data-rawheight=""632"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-952be99ec5699dfb478ea84a38913844_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-952be99ec5699dfb478ea84a38913844_b.jpg""></figure><h2><b>2.4 问答系统运行效果</b></h2><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-534b768afc4ca87a1260f010a26ff62a_b.jpg"" data-rawwidth=""815"" data-rawheight=""518"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""815"" data-original=""https://pic2.zhimg.com/v2-534b768afc4ca87a1260f010a26ff62a_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='815'%20height='518'&gt;&lt;/svg&gt;"" data-rawwidth=""815"" data-rawheight=""518"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""815"" data-original=""https://pic2.zhimg.com/v2-534b768afc4ca87a1260f010a26ff62a_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-534b768afc4ca87a1260f010a26ff62a_b.jpg""></figure><h2><b>3、项目介绍</b></h2><p>该项目的数据来自垂直类医疗网站寻医问药，使用爬虫脚本data_spider.py，以结构化数据为主，构建了以疾病为中心的医疗知识图谱，实体规模4.4万，实体关系规模30万。schema的设计根据所采集的结构化数据生成，对网页的结构化数据进行xpath解析。</p><p>项目的数据存储采用Neo4j图数据库，问答系统采用了规则匹配方式完成，数据操作采用neo4j声明的cypher。</p><p>项目的不足之处在于疾病的引发原因、预防等以大段文字返回，这块可引入事件抽取，可将原因结构化表示出来。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-3f5b609c81ac05aec36b0e194e02a8ba_b.jpg"" data-rawwidth=""1240"" data-rawheight=""842"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-3f5b609c81ac05aec36b0e194e02a8ba_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1240'%20height='842'&gt;&lt;/svg&gt;"" data-rawwidth=""1240"" data-rawheight=""842"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""1240"" data-original=""https://pic4.zhimg.com/v2-3f5b609c81ac05aec36b0e194e02a8ba_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-3f5b609c81ac05aec36b0e194e02a8ba_b.jpg""></figure><h2><b>3.1 项目目录</b></h2><div class=""highlight""><pre><code class=""language-python3""><span></span><span class=""o"">.</span>
<span class=""err"">├──</span> <span class=""n"">README</span><span class=""o"">.</span><span class=""n"">md</span>
<span class=""err"">├──</span> <span class=""n"">__pycache__</span>      \\<span class=""n"">编译结果保存目录</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">answer_search</span><span class=""o"">.</span><span class=""n"">cpython</span><span class=""o"">-</span><span class=""mf"">36.</span><span class=""n"">pyc</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">question_classifier</span><span class=""o"">.</span><span class=""n"">cpython</span><span class=""o"">-</span><span class=""mf"">36.</span><span class=""n"">pyc</span>
<span class=""err"">│</span>   <span class=""err"">└──</span> <span class=""n"">question_parser</span><span class=""o"">.</span><span class=""n"">cpython</span><span class=""o"">-</span><span class=""mf"">36.</span><span class=""n"">pyc</span>
<span class=""err"">├──</span> <span class=""n"">answer_search</span><span class=""o"">.</span><span class=""n"">py</span>
<span class=""err"">├──</span> <span class=""n"">answer_search</span><span class=""o"">.</span><span class=""n"">pyc</span>
<span class=""err"">├──</span> <span class=""n"">build_medicalgraph</span><span class=""o"">.</span><span class=""n"">py</span>    \\<span class=""n"">知识图谱数据入库脚本</span>
<span class=""err"">├──</span> <span class=""n"">chatbot_graph</span><span class=""o"">.</span><span class=""n"">py</span>    \\<span class=""n"">问答程序脚本</span>
<span class=""err"">├──</span> <span class=""n"">data</span>
<span class=""err"">│</span>   <span class=""err"">└──</span> <span class=""n"">medicaln</span><span class=""o"">.</span><span class=""n"">json</span> \\<span class=""n"">本项目的全部数据</span><span class=""err"">，</span><span class=""n"">通过build_medicalgraph</span><span class=""o"">.</span><span class=""n"">py导neo4j</span>
<span class=""err"">├──</span> <span class=""nb"">dict</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">check</span><span class=""o"">.</span><span class=""n"">txt</span>    \\<span class=""n"">诊断检查项目实体库</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">deny</span><span class=""o"">.</span><span class=""n"">txt</span>      \\<span class=""n"">否定词库</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">department</span><span class=""o"">.</span><span class=""n"">txt</span>  \\<span class=""n"">医疗科目实体库</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">disease</span><span class=""o"">.</span><span class=""n"">txt</span>    \\<span class=""n"">疾病实体库</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">drug</span><span class=""o"">.</span><span class=""n"">txt</span>      \\<span class=""n"">药品实体库</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">food</span><span class=""o"">.</span><span class=""n"">txt</span>      \\<span class=""n"">食物实体库</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">producer</span><span class=""o"">.</span><span class=""n"">txt</span>    \\<span class=""n"">在售药品库</span>
<span class=""err"">│</span>   <span class=""err"">└──</span> <span class=""n"">symptom</span><span class=""o"">.</span><span class=""n"">txt</span>    \\<span class=""n"">疾病症状实体库</span>
<span class=""err"">├──</span> <span class=""n"">document</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">chat1</span><span class=""o"">.</span><span class=""n"">png</span>    \\<span class=""n"">系统运行问答截图01</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">chat2</span><span class=""o"">.</span><span class=""n"">png</span>      \\<span class=""n"">系统运行问答截图01</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">kg_route</span><span class=""o"">.</span><span class=""n"">png</span>    \\<span class=""n"">知识图谱构建框架</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">qa_route</span><span class=""o"">.</span><span class=""n"">png</span>    \\<span class=""n"">问答系统框架图</span>
<span class=""err"">├──</span> <span class=""n"">img</span>    \\<span class=""n"">README</span><span class=""o"">.</span><span class=""n"">md中的所用图片</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">chat1</span><span class=""o"">.</span><span class=""n"">png</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">chat2</span><span class=""o"">.</span><span class=""n"">png</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">graph_summary</span><span class=""o"">.</span><span class=""n"">png</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">kg_route</span><span class=""o"">.</span><span class=""n"">png</span>
<span class=""err"">│</span>   <span class=""err"">└──</span> <span class=""n"">qa_route</span><span class=""o"">.</span><span class=""n"">png</span>
<span class=""err"">├──</span> <span class=""n"">prepare_data</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">build_data</span><span class=""o"">.</span><span class=""n"">py</span>    \\<span class=""n"">数据库操作脚本</span>
<span class=""err"">│</span>   <span class=""err"">├──</span> <span class=""n"">data_spider</span><span class=""o"">.</span><span class=""n"">py</span>    \\<span class=""n"">网络资讯采集脚本</span>
<span class=""err"">│</span>   <span class=""err"">└──</span> <span class=""n"">max_cut</span><span class=""o"">.</span><span class=""n"">py</span>      \\<span class=""n"">基于词典的最大向前</span><span class=""o"">/</span><span class=""n"">向后脚本</span>
<span class=""err"">├──</span> <span class=""n"">question_classifier</span><span class=""o"">.</span><span class=""n"">py</span>    \\<span class=""n"">问句类型分类脚本</span>
<span class=""err"">├──</span> <span class=""n"">question_classifier</span><span class=""o"">.</span><span class=""n"">pyc</span>    
<span class=""err"">├──</span> <span class=""n"">question_parser</span><span class=""o"">.</span><span class=""n"">py</span>    \\<span class=""n"">问句解析脚本</span>
<span class=""err"">├──</span> <span class=""n"">question_parser</span><span class=""o"">.</span><span class=""n"">pyc</span>
</code></pre></div><h2><b>3.2 知识图谱的实体类型</b></h2><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-917d09deeec4def889eb9ed92539fb0c_b.jpg"" data-rawwidth=""631"" data-rawheight=""316"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""631"" data-original=""https://pic4.zhimg.com/v2-917d09deeec4def889eb9ed92539fb0c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='631'%20height='316'&gt;&lt;/svg&gt;"" data-rawwidth=""631"" data-rawheight=""316"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""631"" data-original=""https://pic4.zhimg.com/v2-917d09deeec4def889eb9ed92539fb0c_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-917d09deeec4def889eb9ed92539fb0c_b.jpg""></figure><h2><b>3.3 知识图谱的实体关系类型</b></h2><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-ae76d70da233abf1ada91a677503d2f0_b.jpg"" data-rawwidth=""637"" data-rawheight=""566"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""637"" data-original=""https://pic4.zhimg.com/v2-ae76d70da233abf1ada91a677503d2f0_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='637'%20height='566'&gt;&lt;/svg&gt;"" data-rawwidth=""637"" data-rawheight=""566"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""637"" data-original=""https://pic4.zhimg.com/v2-ae76d70da233abf1ada91a677503d2f0_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-ae76d70da233abf1ada91a677503d2f0_b.jpg""></figure><h2><b>3.4 知识图谱的属性类型</b></h2><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-c465fabbff14f7fdacb6112e3d5e7229_b.jpg"" data-rawwidth=""548"" data-rawheight=""299"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""548"" data-original=""https://pic2.zhimg.com/v2-c465fabbff14f7fdacb6112e3d5e7229_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='548'%20height='299'&gt;&lt;/svg&gt;"" data-rawwidth=""548"" data-rawheight=""299"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""548"" data-original=""https://pic2.zhimg.com/v2-c465fabbff14f7fdacb6112e3d5e7229_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-c465fabbff14f7fdacb6112e3d5e7229_b.jpg""></figure><h2><b>3.5 问答项目实现原理</b></h2><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-c9238815f1da73f43c452ac32a2cf675_b.jpg"" data-rawwidth=""1240"" data-rawheight=""688"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic1.zhimg.com/v2-c9238815f1da73f43c452ac32a2cf675_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1240'%20height='688'&gt;&lt;/svg&gt;"" data-rawwidth=""1240"" data-rawheight=""688"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""1240"" data-original=""https://pic1.zhimg.com/v2-c9238815f1da73f43c452ac32a2cf675_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-c9238815f1da73f43c452ac32a2cf675_b.jpg""></figure><p>本项目的问答系统完全基于规则匹配实现，通过关键词匹配，对问句进行分类，医疗问题本身属于封闭域类场景，对领域问题进行穷举并分类，然后使用cypher的match去匹配查找neo4j，根据返回数据组装问句回答，最后返回结果。</p><p><b>问句中的关键词匹配：</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-a1be16695982e15f1ba95a9e32fee1be_b.jpg"" data-rawwidth=""1012"" data-rawheight=""284"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""1012"" data-original=""https://pic4.zhimg.com/v2-a1be16695982e15f1ba95a9e32fee1be_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1012'%20height='284'&gt;&lt;/svg&gt;"" data-rawwidth=""1012"" data-rawheight=""284"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""1012"" data-original=""https://pic4.zhimg.com/v2-a1be16695982e15f1ba95a9e32fee1be_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-a1be16695982e15f1ba95a9e32fee1be_b.jpg""></figure><p><b>根据匹配到的关键词分类问句</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-618a4bff33c10ef7fdaa6bddf74616d7_b.jpg"" data-rawwidth=""513"" data-rawheight=""689"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""513"" data-original=""https://pic2.zhimg.com/v2-618a4bff33c10ef7fdaa6bddf74616d7_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='513'%20height='689'&gt;&lt;/svg&gt;"" data-rawwidth=""513"" data-rawheight=""689"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""513"" data-original=""https://pic2.zhimg.com/v2-618a4bff33c10ef7fdaa6bddf74616d7_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-618a4bff33c10ef7fdaa6bddf74616d7_b.jpg""></figure><p><b>问句解析</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-df696ca23eab674c8659ee599b101064_b.jpg"" data-rawwidth=""569"" data-rawheight=""696"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""569"" data-original=""https://pic3.zhimg.com/v2-df696ca23eab674c8659ee599b101064_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='569'%20height='696'&gt;&lt;/svg&gt;"" data-rawwidth=""569"" data-rawheight=""696"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""569"" data-original=""https://pic3.zhimg.com/v2-df696ca23eab674c8659ee599b101064_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-df696ca23eab674c8659ee599b101064_b.jpg""></figure><p><b>查找相关数据</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-bccb159a2194da7ca931fb7ba7409ce2_b.jpg"" data-rawwidth=""583"" data-rawheight=""696"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""583"" data-original=""https://pic2.zhimg.com/v2-bccb159a2194da7ca931fb7ba7409ce2_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='583'%20height='696'&gt;&lt;/svg&gt;"" data-rawwidth=""583"" data-rawheight=""696"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""583"" data-original=""https://pic2.zhimg.com/v2-bccb159a2194da7ca931fb7ba7409ce2_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-bccb159a2194da7ca931fb7ba7409ce2_b.jpg""></figure><p><b>根据返回的数据组装回答</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-2c89400aae89f379959dcdeec5439fec_b.jpg"" data-rawwidth=""581"" data-rawheight=""702"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""581"" data-original=""https://pic3.zhimg.com/v2-2c89400aae89f379959dcdeec5439fec_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='581'%20height='702'&gt;&lt;/svg&gt;"" data-rawwidth=""581"" data-rawheight=""702"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""581"" data-original=""https://pic3.zhimg.com/v2-2c89400aae89f379959dcdeec5439fec_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-2c89400aae89f379959dcdeec5439fec_b.jpg""></figure><p><b>3.6 问答系统支持的问答类型</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-3c5e0792035b40acf9c46f3319ca374e_b.jpg"" data-rawwidth=""625"" data-rawheight=""573"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""625"" data-original=""https://pic3.zhimg.com/v2-3c5e0792035b40acf9c46f3319ca374e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='625'%20height='573'&gt;&lt;/svg&gt;"" data-rawwidth=""625"" data-rawheight=""573"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""625"" data-original=""https://pic3.zhimg.com/v2-3c5e0792035b40acf9c46f3319ca374e_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-3c5e0792035b40acf9c46f3319ca374e_b.jpg""></figure><h2><b>4、项目总结</b></h2><p>基于规则的问答系统没有复杂的算法，一般采用模板匹配的方式寻找匹配度最高的答案，回答结果依赖于问句类型、模板语料库的覆盖全面性，面对已知的问题，可以给出合适的答案，对于模板匹配不到的问题或问句类型，经常遇到的有三种回答方式：</p><blockquote>1、给出一个无厘头的答案；<br>2、婉转的回答不知道，提示用户换种方式去问；<br>3、转移话题，回避问题；</blockquote><p><b>例如，本项目中采用了婉转的方式回答不知道：</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-410bc779c73f3306b0edf32ca9952f4f_b.jpg"" data-rawwidth=""754"" data-rawheight=""274"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb"" width=""754"" data-original=""https://pic4.zhimg.com/v2-410bc779c73f3306b0edf32ca9952f4f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='754'%20height='274'&gt;&lt;/svg&gt;"" data-rawwidth=""754"" data-rawheight=""274"" data-size=""normal"" data-caption="""" class=""origin_image zh-lightbox-thumb lazy"" width=""754"" data-original=""https://pic4.zhimg.com/v2-410bc779c73f3306b0edf32ca9952f4f_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-410bc779c73f3306b0edf32ca9952f4f_b.jpg""></figure><p>基于知识图谱的问答系统的主要特征是知识图谱，系统依赖一个或多个领域的实体，并基于图谱进行推理或演绎，深度回答用户的问题，基于知识图谱的问答系统更擅长回答知识性问题，与基于模板的聊天机器人有所不同的是它更直接、直观的给用户答案。对于不能回答、或不知道的问题，一般直接返回失败，而不是转移话题避免尴尬。</p><p>整个问答系统的优劣依赖于知识图谱中知识的数量与质量。也算是利弊共存吧！知识图谱图谱具有良好的可扩展性，扩展了知识图谱也就是扩展了问答系统的知识库。如果问句在射程范围内，可轻松回答，但如果不幸脱靶，则体验大打折扣。</p><a data-draft-node=""block"" data-draft-type=""mcn-link-card"" data-mcn-id=""1194232135832248320""></a><p>从知识图谱的角度分析，大多数知识图谱规模不足，主要原因还是数据来源以及技术上知识的抽取与推理困难。</p><a data-draft-node=""block"" data-draft-type=""mcn-link-card"" data-mcn-id=""1194232263926267904""></a><p><br></p><p><a href=""http://link.zhihu.com/?target=http%3A//weixin.qq.com/r/qS_uto-EP9JArcvV93q7"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">weixin.qq.com/r/qS_uto-</span><span class=""invisible"">EP9JArcvV93q7</span><span class=""ellipsis""></span></a> (二维码自动识别)</p><blockquote><b>个人博客：<a href=""http://link.zhihu.com/?target=http%3A//www.bobinsun.cn"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://www.</span><span class=""visible"">bobinsun.cn</span><span class=""invisible""></span></a></b></blockquote><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-bd31dd2ada2f88ac0ad952b5bf70cd94_b.gif"" data-rawwidth=""640"" data-rawheight=""349"" data-size=""normal"" data-thumbnail=""https://pic1.zhimg.com/v2-bd31dd2ada2f88ac0ad952b5bf70cd94_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""640"" data-original=""https://pic1.zhimg.com/v2-bd31dd2ada2f88ac0ad952b5bf70cd94_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='640'%20height='349'&gt;&lt;/svg&gt;"" data-rawwidth=""640"" data-rawheight=""349"" data-size=""normal"" data-thumbnail=""https://pic1.zhimg.com/v2-bd31dd2ada2f88ac0ad952b5bf70cd94_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""640"" data-original=""https://pic1.zhimg.com/v2-bd31dd2ada2f88ac0ad952b5bf70cd94_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-bd31dd2ada2f88ac0ad952b5bf70cd94_b.gif""><figcaption>题图</figcaption></figure><p></p>",73,34,,阿拉灯神丁Vicky,https://api.zhihu.com/people/f5124fb87bf903348ab9d3e10ed32b28,赞同了文章
1581400530820,AI智见未来,,,,,https://api.zhihu.com/columns/aladengodman,,,,1581400530,,,,,,匿名用户,,关注了专栏
1581400487072,Kensho 为什么会被高盛青睐？,,,,,https://api.zhihu.com/questions/26793684,,,,1581400487,,,,0,,张醒,https://api.zhihu.com/people/bac47ff9db9212b40d02ea66166deba7,关注了问题
1581396752355,丁香园在语义匹配任务上的探索与实践,,,,,https://api.zhihu.com/articles/69356170,,,,1581396752,,"<blockquote>作者： <a class=""member_mention"" href=""http://www.zhihu.com/people/2ef0e6409af19dc24341e3419cb6b09c"" data-hash=""2ef0e6409af19dc24341e3419cb6b09c"" data-hovercard=""p$b$2ef0e6409af19dc24341e3419cb6b09c"">@henryWang</a> </blockquote><h2><b>前言</b></h2><p>语义匹配是NLP领域的基础任务之一，直接目标就是判断两句话是否表达了相同或相似意思。其模型框架十分简洁，通常包含 <b>文本表示</b> 和 <b>匹配策略</b> 两个模块，因而很容易扩展到相关应用场景，如搜索、推荐、QA系统等。此类模型通常依赖 <b>数据驱动</b> ，即模型的效果需要高质量的数据作为基础，而在医疗领域，文本数据的质量不光依赖语法，还要求相对严谨的实体描述，因而对数据的要求更为严格，也为模型训练调整增添了困难。</p><p>联系到丁香园团队的在线问诊、病症查询、医疗推荐等业务场景，我们希望能构建更高质量的语义表征模型，一方面对于文本中的疾病症状等实体词足够敏感，另一方面能够捕获长距离的组合信息。通过在语义匹配任务上的尝试，积累语义模型的特点，从而在各个具体应用场景下，针对性地构建模型结构。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-55f70c34e8490bcf0d8cedf914e90a4d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""942"" data-rawheight=""293"" class=""origin_image zh-lightbox-thumb"" width=""942"" data-original=""https://pic4.zhimg.com/v2-55f70c34e8490bcf0d8cedf914e90a4d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='942'%20height='293'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""942"" data-rawheight=""293"" class=""origin_image zh-lightbox-thumb lazy"" width=""942"" data-original=""https://pic4.zhimg.com/v2-55f70c34e8490bcf0d8cedf914e90a4d_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-55f70c34e8490bcf0d8cedf914e90a4d_b.jpg""></figure><p><br></p><p>本文介绍了丁香园大数据团队在医疗文本数据上，对语义匹配这一基础任务的实践。首先简单介绍早期的文本特征表示方法，然后列举几种Representation-Based和Interaction-Based的模型，最后介绍实践中的一些细节。</p><h2><b>传统文本表示</b></h2><p>经典的匹配模型普遍采用某种基于bow的统计文本表示，例如tf-idf和bm25，这类方法普遍存在两个问题，一是维数太高，二是丢失了context的特征。2013年的Word2Vec的出现改变了文本表示的方式，利用文本的context token训练每个token的低维度向量表示，使向量本身包含语义信息，此后成为文本类任务的主流预训练手段。</p><p>在对比传统方式和word2vec的效果后，丁香园也逐渐用词向量作为各个业务场景中的文本表示，并在此基础上，不断挖掘更深层的文本表示学习，语义匹配模型则是重要的探索方向。</p><h2><b>模型介绍</b></h2><p>常见的语义匹配模型有两种设计的思路：基于表示(Representation-Based)和基于交互(Interaction-Based)</p><ul><li><b>Representation-Based Model</b></li></ul><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-aa17fc245f6d7928f714713445f189ad_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""548"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-aa17fc245f6d7928f714713445f189ad_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='548'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""548"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-aa17fc245f6d7928f714713445f189ad_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-aa17fc245f6d7928f714713445f189ad_b.jpg""></figure><p>基于表示的模型很直接，就是用一个Encoder模块提取本文的特征信息，以可计算的数值向量输出，最后一层计算两个向量的匹配程度，可接一层MLP，也可直接算余弦相似。很明显这类模型的关键是中间Encoder模块的设计，目标是设计强大的特征提取器，在Encoder部分获得文本的语义表示。</p><ul><li><b>Interaction-Based Model</b></li></ul><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-d46460663223f6b62081dbedbcf914a5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""422"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-d46460663223f6b62081dbedbcf914a5_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='422'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""422"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-d46460663223f6b62081dbedbcf914a5_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-d46460663223f6b62081dbedbcf914a5_b.jpg""></figure><p><br></p><p>基于交互的模型认为，只用孤立的Encoder模块是不够的，而两个文本交互匹配本身就会产生有关匹配程度的信息，利用好这个信息可以提升模型的匹配能力。基于这个设想，再加上attention机制的流行，语义模型就开始变得复杂了。下面将列举几个有代表性的网络结构。</p><ul><li><b>DSSM</b></li></ul><p>DSSM是非常经典的语义模型，是标准的Representation-Based的结构模版，模型本身效果很好，且极易修改或扩展，因而广泛应用在相关业务场景中。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-03033dcb92caf405ea979a818619dfc8_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""456"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-03033dcb92caf405ea979a818619dfc8_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='456'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""456"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-03033dcb92caf405ea979a818619dfc8_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-03033dcb92caf405ea979a818619dfc8_b.jpg""></figure><p>Encoder部分，DSSM采用了MLP，在此基础上的扩展通常也是将MLP换成CNN或RNN结构，顶层直接计算cos，各部分都相对容易实现。不过，原文选用了BOW模型作为文本的表示方式，虽然也加入了trigrams等策略，但仍不可避免会失去词的context信息。此外，原文采用了point-wise的loss，我们团队在搜索精排序阶段尝试改进了DSSM，用RNN替换了MLP，并采用triplet loss训练模型，并与多种Encoder+point-wise-loss对比，结构显示了triplet loss在排序任务上有更大的优势。</p><ul><li><b>SiameseRNN</b></li></ul><p>孪生网络(Siamese network)是一类网络结构，其特点是Encoder共享权重。Encoder可根据需要替换。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-f5c123cb65e25e5d49d8c1ce84b455db_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""969"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-f5c123cb65e25e5d49d8c1ce84b455db_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='969'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""969"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-f5c123cb65e25e5d49d8c1ce84b455db_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-f5c123cb65e25e5d49d8c1ce84b455db_b.jpg""></figure><p><br></p><p>这里以RNN为例，是因为RNN结构的特性。DSSM结尾处提到了词的context信息，虽然词向量是利用context训练得到，但文本序列的词序信息仍需要Encoder捕捉，而RNN类结构本身就具有捕捉序列信息的能力，因此在语义匹配任务中，RNN常常会比CNN有更好的效果。这也是很多文本任务的模型会在Embedding层上加Bi-RNN的原因。</p><p> 不过RNN类模型训练耗时长，并行困难，这对科研调试和产业应用都不友好，特别对于应用落地，时间开销太大，再好的模型也没法上线。当然关于RNN并行也有很多相关研究，SRU和Sliced RNN应该是两种代表性的改进方式，不过仍然难以比拟天生就适合并行的CNN和Transformer。</p><ul><li><b>TextCNN</b></li></ul><p>上面提到了RNN具备提取词序信息的能力，其实就是说每个词和其前后的内容并不独立出现，但CNN是一个个窗口，窗口间的计算并不关联，如何用CNN抽取词序信息呢？本文是Kim在2014年发表的，代表性的CNN做文本特征提取的工作。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-8632e2df4684ca873cdf09ab7dee7348_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""450"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-8632e2df4684ca873cdf09ab7dee7348_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='450'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""450"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-8632e2df4684ca873cdf09ab7dee7348_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-8632e2df4684ca873cdf09ab7dee7348_b.jpg""></figure><p>原文模型用于句子分类任务，基本思路就是，让一个窗口的大小刚好装下k个词，对这种窗口的卷积操作就相当于抽取了k个词共同表达的信息，也起到了抽取词序信息的作用。不过这种CNN结构存在两个明显的问题，一是k的取值通常很小，意味着文中采用的单层CNN无法提取长序列的信息；二是文中的CNN包含max pooling操作，对于k个词对应的向量，池化操作只保留一个最大的值，这势必会丢掉一些信息，因此目前也有许多针对性的CNN改进策略，下一节将介绍CNN的改进以及与RNN，Transformer的对比。</p><ul><li><b>Transformer</b></li></ul><p>Transformer的出现应该是NLP界的大新闻，除了在机器翻译任务上超越了Seq2seq外，霸气的文章标题也令人记忆犹新。正如该题目所述，Transformer给了人们CNN和RNN外的其他选择。这里针对语义匹配，我们只关注其中的Encoder部分。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-380cc32d6c98ecc6a8c4c0fa4e03382f_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""826"" data-rawheight=""1125"" class=""origin_image zh-lightbox-thumb"" width=""826"" data-original=""https://pic4.zhimg.com/v2-380cc32d6c98ecc6a8c4c0fa4e03382f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='826'%20height='1125'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""826"" data-rawheight=""1125"" class=""origin_image zh-lightbox-thumb lazy"" width=""826"" data-original=""https://pic4.zhimg.com/v2-380cc32d6c98ecc6a8c4c0fa4e03382f_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-380cc32d6c98ecc6a8c4c0fa4e03382f_b.jpg""></figure><p>全文的重点是self-attention，通过这种attention计算，让文本序列中的每个词都和序列本身建立联系，计算过程非常简单。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-1248f7925de6ce4bb6c2551ee8fc888f_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""846"" data-rawheight=""152"" class=""origin_image zh-lightbox-thumb"" width=""846"" data-original=""https://pic3.zhimg.com/v2-1248f7925de6ce4bb6c2551ee8fc888f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='846'%20height='152'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""846"" data-rawheight=""152"" class=""origin_image zh-lightbox-thumb lazy"" width=""846"" data-original=""https://pic3.zhimg.com/v2-1248f7925de6ce4bb6c2551ee8fc888f_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-1248f7925de6ce4bb6c2551ee8fc888f_b.jpg""></figure><p>其中Q，K，V都是文本序列本身，分母上的sqrt(dk)作用是缓解Q和K的dot计算出现过大的值。不难想象，分子上的dot(Q,K)操作计算了序列中每个词与其他词之间的相似度，softmax对这一结果做归一化，得到的就是序列对自身的权重。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-e60ae28d0ec6380bea9d2e473a79f4c1_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""1449"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-e60ae28d0ec6380bea9d2e473a79f4c1_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='1449'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""1449"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-e60ae28d0ec6380bea9d2e473a79f4c1_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-e60ae28d0ec6380bea9d2e473a79f4c1_b.jpg""></figure><p>此外，文中提到了多次计算self-attention对特征提取是有帮助的，因此Encoder采用了Multi-Head Attention机制，多个self-attention独立的计算，原文采用了base和big两种结构，分别对应12和24个self-attention块，更宽更深的模型在文中的实验里取得了更好的效果，并且，由于self-attention本身计算不慢，又相互独立，这使得Multi-Head机制很容易并行操作。</p><p>当然，整个Encoder不光包含Multi-Head Attention，Transformer如此有效，或许跟Block结构本身也有很大关系，完全可以尝试将Multi-Head Attention替换成CNN或RNN结构，针对不同应用场景很可能会有奇效。</p><ul><li><b>ESIM</b></li></ul><p>上述的结构都是完全关注Encoder的Representation-Based Model，而事实上Interaction-Based Model早已成为此类任务的主流设计思路。一方面对比融合不同输入的信息符合人们的常识，另一方面诸如attention等融合机制所提供的改进的确在很多任务中表现出色。下面将介绍两个在文本分类，文本匹配等比赛中频频夺魁的模型，首先是ESIM。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-996ebecf8748afbf0f349aa5b7bf7de0_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1072"" data-rawheight=""1308"" class=""origin_image zh-lightbox-thumb"" width=""1072"" data-original=""https://pic4.zhimg.com/v2-996ebecf8748afbf0f349aa5b7bf7de0_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1072'%20height='1308'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1072"" data-rawheight=""1308"" class=""origin_image zh-lightbox-thumb lazy"" width=""1072"" data-original=""https://pic4.zhimg.com/v2-996ebecf8748afbf0f349aa5b7bf7de0_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-996ebecf8748afbf0f349aa5b7bf7de0_b.jpg""></figure><p>该模型专门为自然语言推理而设计，结构上并不复杂，重点是在两层Bi-LSTM间加入了interaction计算，文中称作 <b>local inference modeling</b> ，其实就是利用了 <b>soft attention</b> 计算两个文本的匹配矩阵，可以认为这个矩阵中包含了两个文本的交互信息。之后这个矩阵和两个Embedding后的输入文本做差和点基，再做concat，就得到增强后的文本特征向量。</p><p>当然，本文还有其他创新点，例如在句法分析类问题上可以用Tree-LSTM替换Bi-LSTM，不过这里重点关注提取interaction信息的结构设计。类似 <b>soft attention</b> 的结构已经是众多Interaction-Based Model的标配，计算量不大，实现简单，对模型的效果提升也很客观，是个性价比很高的trick。相比之下，两个Bi-LSTM层的计算耗时要显著的长。</p><ul><li><b>BiMPM</b></li></ul><p>BiMPM是和ESIM类似的Interaction-Based Model，interaction计算上下均有一层Bi-LSTM，不过作者在interaction计算上花了很多心思，并非简单采用 <b>soft attention</b> ，而是同时用4种匹配策略。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-3d0a2fa95d82b96b6b38d6e4573663e5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1066"" data-rawheight=""1044"" class=""origin_image zh-lightbox-thumb"" width=""1066"" data-original=""https://pic2.zhimg.com/v2-3d0a2fa95d82b96b6b38d6e4573663e5_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1066'%20height='1044'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1066"" data-rawheight=""1044"" class=""origin_image zh-lightbox-thumb lazy"" width=""1066"" data-original=""https://pic2.zhimg.com/v2-3d0a2fa95d82b96b6b38d6e4573663e5_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-3d0a2fa95d82b96b6b38d6e4573663e5_b.jpg""></figure><p>此模型在quora question pairs数据集上测试accuracy达到88，比当时最好的[L.D.C.]模型高3个点，比Siamese-LSTM高6个点。在类似文本匹配的比赛中，BiMPM和ESIM的表现也非常突出，也能一定程度上说明不同的interaction计算策略对匹配效果有不小的帮助。</p><p>不过，这两个模型都有一个明显的缺点————慢，而小数据集又无法发挥它们的效果，所以训练调参的过程会很耗时。像是quora question pairs数据集提供了40W对训练样本，不平衡程度也不大(正负比 3:5)，这时两个模型效果都不错，这也印证了数据驱动的重要性。</p><h2><b>实践体会</b></h2><ul><li><b>TextCNN的局限和改进</b></li></ul><p>上节提到，TextCNN通过调整卷积窗口来提取序列context信息，但是卷积窗口一次毕竟只能提取k个词的组合信息，这样仍然无法处理长序列。为了在长序列问题上比肩RNN，一个直接的应对方式就是堆叠层数，也是目前相对主流的方法，反正CNN计算比RNN快得多，多加几层也没什么问题。</p><p>话虽如此，但直接堆叠TextCNN的效果并不理想，业界也衍生出了不少对TextCNN结构的改进，这里笔者将介绍Facebook AI Research提出的ConvS2S结构。本文对应着Transformer，二者都是Seq2seq结构的改进，因此ConvS2S的Encoder完全可以用于文本匹配等任务。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-02e5584580089e5883d562cefd94596c_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""912"" data-rawheight=""1125"" class=""origin_image zh-lightbox-thumb"" width=""912"" data-original=""https://pic4.zhimg.com/v2-02e5584580089e5883d562cefd94596c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='912'%20height='1125'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""912"" data-rawheight=""1125"" class=""origin_image zh-lightbox-thumb lazy"" width=""912"" data-original=""https://pic4.zhimg.com/v2-02e5584580089e5883d562cefd94596c_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-02e5584580089e5883d562cefd94596c_b.jpg""></figure><p>就Encoder模块而言，ConvS2S为了有效堆叠CNN做出了两处改进，GLU(Gated Linear Units)和残差连接(Residual Connections)，使用glu作为非线性运算可以优化梯度传播，后者residual结构则为了保留了输入序列的信息。这样的修改可以帮助增加卷积层的深度，一定程度上提升CNN在序列任务上的特征提取能力，同时相比于RNN类模型，模型耗时和并行能力的优势依旧存在。笔者尝试用ConvS2S替换TextCNN，叠加5层后模型效果有明显提升，仅从Encoder模块对比，可以与LSTM效果持平，当然这样的对比无法证明CNN可以通过叠加取代RNN，毕竟实验结果与本文数据的关系更大，不过至少说明CNN仍然有改进的潜力。</p><ul><li><b>Positional Embedding</b></li></ul><p>CNN除了面对长序列乏力外，还存在一个问题，就是编码序列时会丢掉position信息。由于RNN是按时序处理序列的，所以序列的位置信息自然会被编码进状态变量中，但CNN做不到，尤其是卷积过后用pooling操作降维会损失的更严重，相同的问题transformer中也存在，看self-attention的公式很容易理解，每个词的地位是完全相同的，计算出的权重只和当前参与计算的两个词有关。因此，在ConvS2S和Transformer的文章中都提出了额外补充位置信息的策略，被称作 <b>position embedding</b> 。</p><p>ConvS2S提出的编码策略很简单，就是加一个类似词向量的位置向量参与训练，在模型训练过程中把位置向量也学出来；transformer则提出一种傅立叶变换生成词的位置编码，作者表示这种计算得到的结果与训练出的位置向量基本一致，但生成过程是一个确定的公式，不必训练生成。</p><p>对于这两种 <b>position embedding</b> 方法，笔者的实践结果并不好。学习位置向量相当于加了一层网络，完全寄希望与网络学习能力，可能在数据量充足的情况下会有效果(BERT也用了这种学习策略)，但小数据集的条件下想要学习出这组参数其实难度很大。至于Transformer配合使用的生成公式，也有不少人从信号系统角度出发给出相应解释，不过笔者认为说服力并不足够，这种生成方式本身是不是合理，以及是否与语言语法相关都值得商榷，不过好在计算很快，训练时加一组对比实验开销也不大。</p><p>总而言之，笔者认为，序列数据中存在位置信息这个设想是有道理的，对于非RNN类的提取器，如何更好的抽取这类信息，也是值得关注的方向，这里link一篇包含相关工作的(<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.02155.pdf"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">arxiv.org/pdf/1803.0215</span><span class=""invisible"">5.pdf</span><span class=""ellipsis""></span></a>)供大家学习参考。</p><ul><li><b>Encoder组合</b></li></ul><p>其实对于语义匹配这个任务，不同领域的数据源，不同的任务需求才是影响模型的关键因素，寄希望于sota得到的结果往往令人失望，原因也很好理解，大部分sota的成果都是针对一两个数据集做优化，其目的也是验证某些结构或trick的可行性，并不意味着sota等价于万能。因此在尝试过包括LDC，ESIM，BiMPM在内的十余种模型后，我们专门尝试了简单的编码器以及编码器组合的对比实验。</p><p>首先是用Representation-Based的结构，对比CNN，RNN，Transformer的效果。实验结果仍然是RNN最好，CNN用了ConvS2S的Encoder并堆叠层数后可以逼近RNN的结果，Transformer的效果则不理想。笔者认为，CNN虽然可以靠层数增强长序列捕捉能力，但毕竟增强有限，这篇文章(<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1808.08946v1.pdf"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">arxiv.org/pdf/1808.0894</span><span class=""invisible"">6v1.pdf</span><span class=""ellipsis""></span></a>)中对三种编码器的长序列特征捕捉能力做了比较，实验结果说明了CNN在此方面仍然有所不足。不过该文章中Transformer的能力应该是强于RNN的，这与笔者的实验结果不符，其原因在于笔者这里实验所用的数据量不足，以及Transformer的模型不够""重""。其实Transformer的强力早已在OpenAI-GPT一文中被展现，去年10月的BERT更是再次证明这一模型的强大，只是由于header的数量会极大影响Transformer的性能，所以在小数据集上单纯靠Transformer可能会让人失望。</p><p>对比这三类编码器后，我们也尝试了组合RNN和CNN，或在Encoder间加入attention提升模型，根据组合实验，我们认为Embedding层后首先用RNN的效果通常比较好，在RNN之上加CNN是有帮助的，不过用attention计算匹配矩阵的提升似乎更大。这里的组合方式都是最简单的拼接，或是向量线性运算，实现很容易，并且在数据集不大的条件下，结果也能逼近甚至超越ESIM，BiMPM这种强力模型。</p><ul><li><b>BERT</b></li></ul><p>最后介绍一下BERT，相信入坑超过半年的NLPer一定都知道BERT的威名，2018年10月一经问世立刻刷新了各个榜单，现如今搭载BERT的模型已经成为了榜单上的标配。网上关于模型本身的解读非常多，因此这里重点介绍下在小数据集上的语义匹配的实验过程。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-bd5d5375e1e506a9b0a7191890df66a6_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""277"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-bd5d5375e1e506a9b0a7191890df66a6_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='277'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""277"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-bd5d5375e1e506a9b0a7191890df66a6_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-bd5d5375e1e506a9b0a7191890df66a6_b.jpg""></figure><p>模型本身的结构非常简单，就是秉承深度模型理念的，做深做宽的巨型Transformer，即便是Base版，原始的模型尺寸也是显存无法接受的存在，笔者尝试过精简模型参数后自己做模型预训练，效果比直接用官方完成预训练的中文模型差很多，所以如果不是既有海量语料又有足够资源，还是不建议自己预训练的。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-c37a68f17f8bb6f0ec1be7c073fd879c_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""316"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-c37a68f17f8bb6f0ec1be7c073fd879c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='316'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""316"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-c37a68f17f8bb6f0ec1be7c073fd879c_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-c37a68f17f8bb6f0ec1be7c073fd879c_b.jpg""></figure><p>看模型结构在文中的占比就能感受到，巨型Transformer甚至都不是文章的重点，全文最着重描述的其实是BERT的pre-training思路，下面将尽量详细的介绍两个任务的训练过程。</p><p>预训练有两个任务，第一个是预测句子中的词，也就是训练文中的 <b>MLM(Masked LM)</b> ，如上图所示，模型的输入包含3种embedding，token，segment和position，其中的token和position好理解，与ConvS2S中的word embedding + position embedding是一回事，segment是和token中的[SEP]是为第二个任务服务的，我们稍后在介绍。首先为什么要用双向Transformer训练MLM呢，作者的解释大概可以总结成""为了训练足够深的双向模型""，这里对比的就是OpenAI-GPT的单向模型以及ELMo的两个单向的concat。那训练MLM到底是如何做呢，说白了就是随机的遮挡住句子中的某些词(用[MASK]替换)，让模型去预测这些词，不过作者也提到了，fine-tuning时文本中是没有[MASK]的，这和预训练时模型遇到的输入不同，为了缓解这种差异带来的影响，作者仅取15%的token，且以0.8的概率遮挡，0.1的概率保持不变，0.1的概率随便换一个词，原文提到，模型不知道那个词是被mask的还是被replace的，所以能force to学习每个输入的context信息，并且由于被replace的词仅有1.5%，因此不会降低模型的效果。</p><p>第二个任务是预测下一个句子，先解释segment是和token中的[SEP]，BERT中每行input都是由两句话拼成的，两句话间用[SEP]分割，第一句开头是[CLS]，第二句结尾是[SEP]，segment就是input每个位置属于第一句还是第二句，第一句的位置index都是0，第二句都是1。源码中，两个句子分别对应变量token_a和token_b，其中token_b的构造也有trick，每句都以50%的概率替换成随机某一篇文档中的随机某句话。这个任务其实是为了让模型学习到句间关系，文中的实验结果对比显示了此任务对模型的提升(在QNLI数据集上accuracy提高4个点)</p><p>以上就是两个关键的预训练的设计，完成后，BERT提供了一个输出层，针对不同任务，只需要对该层稍作修改即可，可以参考文中给出的结构图。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-61388bd5cf8a1d9bd787658d087475cb_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""857"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-61388bd5cf8a1d9bd787658d087475cb_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='857'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""857"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-61388bd5cf8a1d9bd787658d087475cb_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-61388bd5cf8a1d9bd787658d087475cb_b.jpg""></figure><p>这里不得不说，BERT不仅效果好，源码也很友好，想迁移到自己的任务上，只需要实现对应的数据处理类，调整模型输出层即可，即便是想在bert后接自己的模型，也只需要取到transformer最后一层作为自己模型的输入。这里需要注意，对于中文任务，官方的预训练是在字向量基础上做的，字典也必须用官方提供的，需要按词分的话就不得不重新预训练了，不过修改成分词的数据生成代码也只需要简单做几行改动，不过通常字典的size只是万级(BERT中文语料来自维基百科，字典大小21128)，而常用的语料中词典的大小至少有几十倍大，恐怕BERT这种预训练方式也难以收敛。</p><p>从ELMo到BERT，语言理解模型可能是2018年NLP领域最耀眼的成果。2013年的Word2Vec之于NLP可谓功勋卓著，它本身就有预训练的思路，但word embedding层面的trick还不够满足任务需求，并且一直以来NLP领域的模型总是做不深，即便是ELMo尝试堆叠Bi-LSTM，也只有3层，而且3层的时间开销已经让大部分团队难以接受，直到Transformer被OpenAI-GPT验证了有堆叠提升的潜力，紧接着BERT再次证明了Transformer的能力，同时也证明了深度模型的潜能。从工程角度出发，NLP的标注数据通常不易得，而这种利用大规模无标注数据做预训练的方法无疑大大降低成本，小团队或个人也有机会利用少量数据和计算资源fine-tuning出满足需求的模型。</p><h2><b>总结</b></h2><p>作为nlp领域的基础任务，语义匹配模型可以根据应用场景灵活调整，但核心都是挖掘文本序列特征的表示学习。为此，我们希望能把握不同结构中细节的作用，以便在不同任务场景中移植。同时，丁香园也在逐渐适应深度学习模型应用落地，在搜索的精排序阶段，大数据团队尝试接入改进后的LSTM-DSSM，利用Bi-LSTM的长序列捕捉能力，并采用triplet-loss训练模型，同时在召回阶段，我们也在探索改进CNN和Transformer以提高召回质量。此外，BERT源码易读，fine-tuning成本低，模型迁移性好，我们目前的实践结果也证明其在业务中的潜力，因此，我们未来也会跟进BERT工程化的工作。</p><p>另一方面，这次BERT的成功很大程度上是依靠新颖的预训练策略，以及对无标注语料的探索，近期OpenAI-GPT2.0的成功似乎也证明了高质量数据驱动的能量巨大。然而对于医疗场景而言，准确性和严谨性是重中之重，依靠数据驱动是远远不够的，这更需要基于图谱的知识表示和推理的复杂技术，结合数据驱动和专业信息来共同实现知识表示和语言理解，从而提供安全准确的医疗智能服务。</p><hr><p><b>参考文献</b></p><p>1.《Efficient Estimation of Word Representations in Vector Space》</p><p>2.《Simple Recurrent Units for Highly Parallelizable Recurrence》</p><p>3.《Sliced Recurrent Neural Networks》</p><p>4.《Signature Verification using a ""Siamese"" Time Delay Neural Netword》</p><p>5.《Sentence Similarity Learning by Lexical Decomposition and Composition》</p><p>6.《Convolutional Sequence to Sequence Learning》</p><p>7.《Self-Attention with Relative Position Representations》</p><p>8.《Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures》</p><p>9.《Deep contextualized word representations》</p><p>10.《Convolutional Neural Networks for Sentence Classification》</p><p>11.《Attention Is All You Need》</p><p>12.《Enhanced LSTM for Natural Language Inference》</p><p>13.《Bilateral Multi-Perspective Matching for Natural Language Sentences》</p><p>14.《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</p><p>15.<a href=""http://link.zhihu.com/?target=https%3A//www.microsoft.com/en-us/research/project/dssm/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://www.</span><span class=""visible"">microsoft.com/en-us/res</span><span class=""invisible"">earch/project/dssm/</span><span class=""ellipsis""></span></a></p><p></p>",106,13,,Yang Peter,https://api.zhihu.com/people/145a62cdf56d7365e61bb787e88ed015,赞同了文章
1581392300505,丁香园自然语言处理,,,,,https://api.zhihu.com/columns/c_1122934158459404288,,,,1581392300,,,,,,匿名用户,,关注了专栏
1574038583947,TensorFlow 真的要被 PyTorch 比下去了吗？,,,,,https://api.zhihu.com/questions/350445991,,,,1574038583,,,,5,,狗着,https://api.zhihu.com/people/222aced5e30e9c4fce03e12f0afb00b4,关注了问题
1573987850956,如何在一周内快速摸清一个行业？,大多数情况，人们调研一个行业，都是基于某个目的或者角度，很多咨询行业顾问都具备一周内搞清楚一个行业的能力。 具体怎么做？,671,45,123874,https://api.zhihu.com/questions/21324385,刘子宁,https://api.zhihu.com/people/fb8866289b499ff246798731cd1d2508,1373588703,1573987850,1588482896,"<p>曾经做了3年投资咨询，平均每个月一个项目，做的事情就是（尝试）深入了解各种行业和公司，为投资决策提供分析支持和建议，所以了解一个行业的方法论对我来说既是基本功，也是在面对不同行业后不断深化的一项技能。在这里将它整理出来，大家看看是否有帮助。</p><p>一周内不可能完全了解一个行业，这个我认可。但一周内可以大致了解一个行业，这个我同意<a class=""member_mention"" href=""http://www.zhihu.com/people/863d6b131f6720ff3288d4061837e56a"" data-hash=""863d6b131f6720ff3288d4061837e56a"" data-hovercard=""p$b$863d6b131f6720ff3288d4061837e56a"">@刀小白</a>的。只是我不认为必须从事这个行业才能了解这个行业，后面细说为什么。</p><p>一周内大致了解的内容，也大概是她说的几方面，引在下面好了：</p><blockquote>这个行业的盈利模式是什么？<br>这个行业的老大是谁？<br>这个行业的上游供应链，下游消费者是谁？<br>这个行业的产能如何？其在整体经济结构中的地位。<br>这个行业的法律监管如何？</blockquote><p>这些内容一般在一个投资机构/咨询机构，你会有一个 team，不是你一个人闷头干，所以快的话大概花 2-3 天左右就差不多明白了。再花几天可以进一步凿实的是，行业的竞争关系，几大竞争者各自的优势，近几年来此消彼长的关系，与国外同行业竞争者的差异。</p><p>在高效率的情况下，1周做到上述几点，大概完成了了解一个行业知识的5%吧。少得可怜（搞咨询的喜欢强行拍个数，这里只是阐述一个少得可怜感觉，大家感受一下就好）。大概达成的效果是什么呢？就是你跟普通人聊天，能显得你知道这个行业的一些宏观情况和基本常识，能聊10分钟左右，聊到10分钟基本没得聊了，要么换个话题，要么开始吹牛逼。</p><p>所以我觉得这肯定不能算摸清。</p><p>一般一个咨询项目是3-4周，那么接下来1-2周，根据客户要求，可能会具体到某一个公司，研究一下这个公司这几年来都有什么动作，给它造成了什么影响，市场地位发生了什么变化，公司现在的战略是什么，未来方向和预期是什么。如果有信息渠道可以走得再细点，可以把行业和公司的利润率拆解一下，比如公司的典型商品或服务的成本构成和利润空间，看看公司成本变化受什么影响，未来收入增长方向对利润率的影响。</p><p>这里做的工作，实际上是把一个较大的商业实体（公司，或者主要产品线）的典型行为摸清了；从投资的角度上说，把公司层面上的收入和利润变化搞清楚，就可以做一个粗浅的对收入和利润的预测模型了，进而可以形成一个粗浅的估值。这大概就是一个实习基本面分析师的工作了。如果把公司想象成一个人，那你目前的了解程度，大概就是看了这个人的简历，做了一些背调，面试问了些问题，了解了他的项目经历和能力水平，你对他的工作能力有个大致了解，但你真的了解他的方方面面吗？如果你有个单身的女儿，你愿意让女儿认识他吗？我看未必。</p><p>所以，这样做下来，差不多1周到2周的时间，你了解了一个行业多少呢？我觉得也就10-20%吧。你只是了解了行业中一个具体的公司的行为模式，首先这只有一个公司，其次公司作为一个行动单位还是太粗了。当然，这个过程中，如果你足够勤奋，你大概也会了解到竞争对手与他的差别，因此你可能会对这个行业在宏观层面上，因为几家公司的产品差异和战略差异，而造成的市场细分、聚合、或此消彼长的动态机制有了更深刻的理解。但你对行业的了解依然不会超过20%。当然，这个时候你的聊天素材应该可以支撑30分钟到1个小时了，至少你会有一些带有矛盾情节的故事可以讲（公司竞争嘛，总有故事的），可以跟非专业人士谈笑风生，如果你文笔好甚至可以写一篇10万+的公众号，但面对专业人士的问题，你可能并不能应付。</p><p>接下来想了解更多怎么办？直接去从事这个行业吗？不去从事可以不可以？当然可以。对于咨询顾问和基本面分析师们，要继续做2件事：1. 纵深挖掘，把从公司进一步挖掘到更细的经营实体和产品单位；2. 长期跟踪，从动态中提炼细节。</p><p>我举个大家都接触过的行业吧。快餐。如何进一步了解快餐行业呢？</p><p>1. 微观层面</p><p>对于快餐行业，纵深挖掘应该到什么程度呢？快餐行业有意义的最小单位是一个单店（比如北京阜成门麦当劳店）或一个单品（麦辣鸡腿堡）。</p><p>挖掘什么东西呢？看这个店的销售/人流/利润率是否随着市场的变化而变化，如何变化。比如，月初麦当劳推出了小黄人玩具，阜成门店的客流是否发生显著变化，客单价是否发生显著变化，整体销售额是否发生显著变化。因为快餐也是个季度性比较强的生意，所以变化要从同比来看，也就是相比去年，客流增长了多少，客单价增长了多少，销售额增长多少。如果下个月肯德基紧接着推出了变形金刚玩具，麦当劳的又如何变化？如果下下个月边上开了一家味千拉面，客流又如何变化？如果店门口开始修路了有什么影响呢？宏观经济人力成本上涨带来了运营上的哪些变化？员工减少了吗？服务质量下降了吗？服务对麦当劳门店的经营有影响吗？</p><p>当你追踪足够多的店、并且追踪时间足够长之后，你会发现一些单店运营的规律，以及影响经营的微观因素。对于快餐店来说，每个店都逃不掉的东西是，人流、促销、竞争、效率。关注并分析他们的变化，你会发现，一个最底层的销售单位，到底受什么影响，影响幅度有多大。而把这些微观因素加总起来，将直接影响最终的企业和市场层面。</p><p>单品同理，不赘述。</p><p><i>希望继续了解这一层分析思路的，可以去搜索关键词【unit economics】，即我前文所提到的，一个有意义的经营单位的收入成本利润分析。这是商业分析的基本思路。</i></p><p><br></p><p>2. 集群层面</p><p>单纯只是纵深挖掘并不能形成行业的 insight，细节工作做到一定量后，需要进行一些抽象总结，提炼出一些规律。抄近路的一个方式是直接从一个集群来看，因为快餐/零售的大型连锁企业一般都是分区域进行管理的，麦当劳全国几千家店，有很多种分类方式；北京市里几百家店也可以分类。事实上，在快餐企业日常经营管理中，也是对不同类型的店铺进行区别管理的，一般都会按区域或类别分开看，那么你可以直接加总一些相似特征的单店，来总结规律。比如麦当劳的店铺就可以分成商圈店、社区店、交通枢纽店，这是看客流性质的不同；一些零售品牌还有旗舰店/主力店/普通店的区分，这是看功能和定位差异；当然还可以更宏观地以城市特点做区分，比如可以分一二三四线城市，还可以分核心城市/旅游城市/工业城市。你还可以按自己的逻辑来进行分类，比如按照人口上升城市和人口下降城市来区分，按照房地产价格的走势来分，按照环境污染指数，按照降雨量变化等等等等……</p><p>集群层面的问题就不一样了，因为你要尝试总结和提炼一些规律出来，这里你要问的问题会大一些，你要观察的指标也是影响范围更广的动态指标，这同时也要要求你容忍一些群体内的误差和奇异点。</p><p>举个例子：<i>很多麦当劳开在中心商业区附近，那么城市周边地区兴起的商业地产，会不会稀释中心商业区的人流？这会对麦当劳造成什么影响？</i> </p><p>这个问题是针对中心商业区的麦当劳的问题，而且限定范围是在城市周边建设商业地产的城市，这样的城市全国有很多，所涉及的店铺也有很多，通过整合数据你会发现一些规律，但并不是所有你观察的店铺都符合规律，总会有一些中心店因为商圈翻新、地铁开通什么的不受分流影响，但这不代表分流效应不存在。站在集群层面思考问题，需要你学会忽略一些细节，以把握大方向/general sense为主。</p><p>为什么要学会看大问题？因为人口、地产开发流向、降雨量、旅游开发等等这些因素，在你看单店的时候，是无法总结和提炼出来的，必须走到一定高度才能看清。集群和区域的变化，需要通过长期、跨品牌、跨行业的观察和比较，因此，从这一层开始，你已经不能只局限在一个行业中了，你要开始涉猎其他行业的知识，观察他们的变化趋势，并将区域经济看成一个整体，来思考你的目标行业在其中的位置和作用是什么。</p><p><i>希望看到更多类似案例的，可以关注数据帝 <a class=""member_mention"" href=""http://www.zhihu.com/people/b09ea18d0114f41330c2734814b3f0c0"" data-hash=""b09ea18d0114f41330c2734814b3f0c0"" data-hovercard=""p$b$b09ea18d0114f41330c2734814b3f0c0"">@chenqin</a> 并阅读他的答案，看看他如何解构问题、提出新问题、并通过数据挖掘和分析来寻找和证明关联关系的方法，这类分析知乎上没人能比他做得更好，全中国应该也屈指可数。</i></p><p><br></p><p>3. 市场和经济层面</p><p>当你对经济的整体性思考越来越多时，你自然开始发现，你已经走到了市场和经济层面来看行业的问题了，你应该关注品牌之间的竞争策略，关注快餐行业和其他行业的互动，关注宏观经济结构的影响，等等。</p><p>在市场层面，长期观察更为重要。比如从品牌的竞争战略来看，肯德基素来以产品多样、本土化做得好著称，而麦当劳略逊一点。如果你只做一次项目，得到的只是一个时间点上，或者过去的一段时间内，双方的对比。而如果你长期做项目，看到的就是，肯德基在产品开发上的优势在如何变化，麦当劳的应对措施，哪些起效了，哪些没效果，为什么？是产品没有讨好消费者，还是营销策略不对？企业的哪些战略在起效果？通过这些变化和竞争，能否看出来中国消费者到底喜欢吃什么样的快餐？再宏观一些，快餐行业在电商行业发展的背景下地位是怎么变化的？门店客流是否因为不逛街而下降？O2O 行业、外卖行业的发展将怎样影响快餐行业的收入和成本结构？再宏观一些，在消费升级/降级的态势下，快餐在经济中的地位怎样变化？</p><p>这里看上去像是又回到了起点，回到了一开始你了解的那5%的信息，但实际上现在的你已经了解了更多的经营细节和宏观变量，你已经可以理解，一个市场变量是如何一步一步传导到最基本的经营单位上的，有多大影响，这个变量是否真的有意义，有短期意义还是长期意义。反过来，微观业务中出现的现象，是否是公司或行业宏观变化的征兆？这种变化是随机的、独立的，还是规律性的、系统性的？你观察到的微观和宏观变量，是否能形成一个行业判断？你的判断能否被验证？日日夜夜，周而复始，从前在行业报告和公司年报中你一知半解的观点，经过长期的观察、理解、判断和预测，你终于理解了他们的意义，你也开始学会分辨，哪些话是指鹿为马，哪些话是小题大做，哪些话是顾左右而言他。媒体？大多数媒体对行业的报道，信息量太浅了。我想，看到这里，学生们也应该明白了，自己看了一些科技媒体的行业报道就说自己了解XX行业，这在从业者和面试官的眼中，是多么呆萌的一种行为。你们还记得2016年媒体是怎么鼓吹 ofo 的吗？别说学生了，很多所谓专业人士也被忽悠了，当然，里面也有些人，不是蠢就是坏。</p><p>顺便说，了解行业的过程，实际上包含着对商业信息理解和分辨的能力的提升，在这个过程中，你的思维应该是越来越 sharp 的，你不仅会对浅薄的信息报道不满足，同时也应该对更深层的挖掘越来越好奇，并且自己开始动手做交叉分析，将不同的影响因素与行业变化关联起来思考。如果你动手和分析能力没提升，只是学会了批评媒体，那我觉得你也没进步。</p><p><i>希望在这个层面继续深造的同学们，不好意思，到了这个层面，没有别的建议了，多学、多做、多思考，高强度工作个几年，会有收获的。除此之外没有别的捷径可走。</i></p><p><br></p><p>结尾：</p><p>摸清一个行业，说白了，就是持续地思考，<b>驱动一个行业发展、决定一个企业成败、令市场上发生万千变化的元素和逻辑，到底是什么？</b>在不断地尝试回答这个问题时，你对行业的了解渐渐地上升到了30%、40%、50%或者更高，甚至可能在一些方面超过了资深从业者。为什么你有可能超过他们？首先当局者迷，行业内的人的判断，很多时候带着对 KPI 达成的乐观，带着对企业的感情，带着对同行或同事的偏见。其次，你作为一个投资人/咨询分析师，你不止接触一个行业，在你漫长的职业生涯中你会看到很多行业和公司，你会看到更多差异，也会总结出更多更本质的跨行业共性，如果你愿意思考，你会看到社会结构中的一些很本质的东西，这是只在某一个行业中深耕的人所难以触及的。当然这也很难。</p><p>但是，无论内行还是外行，没有人能100%了解一个行业。外行始终不会营运，不会做产品研发，不会做营销，可能也不懂管理。而内行则没有资源去“看看外面的世界”。其实，在这个世界，没有人能完全了解一个行业。我们大多数人，实际上是一知半解。</p><p>而这一知半解，也得经年累月啊。</p><p><br></p><p>后记：看很多朋友问到如何获取信息，以及一个更进阶的问题，如何做访谈。这里写了一篇关于做访谈的入门介绍，有兴趣从事咨询和商业分析行业的同学可以看看，其他朋友可以适当了解一下。</p><a href=""https://zhuanlan.zhihu.com/p/137913371"" data-draft-node=""block"" data-draft-type=""link-card"" class=""internal"">程毅南：如何做行业专家访谈</a><p></p>",21303,248,10860,程毅南,https://api.zhihu.com/people/9f987239282bc362f5b9ecbee51a14bc,赞同了回答
1573886368948,出手就是9.4分，这片稳了，连刷3遍决定推给你！,,,,,https://api.zhihu.com/articles/84668794,,,,1573886368,,"<p></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-836c6cf65ce20d60331a9743e8fe2e2b_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""390"" data-thumbnail=""https://pic2.zhimg.com/v2-836c6cf65ce20d60331a9743e8fe2e2b_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""600"" data-original=""https://pic2.zhimg.com/v2-836c6cf65ce20d60331a9743e8fe2e2b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='600'%20height='390'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""390"" data-thumbnail=""https://pic2.zhimg.com/v2-836c6cf65ce20d60331a9743e8fe2e2b_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""600"" data-original=""https://pic2.zhimg.com/v2-836c6cf65ce20d60331a9743e8fe2e2b_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-836c6cf65ce20d60331a9743e8fe2e2b_b.gif""></figure><p>多久没和爸妈一起看片了？</p><p>这年头，要找一部适合全家观看，不尴尬又有内涵，还能增进家人情感的影片，越来越难...</p><p>今天，推荐一部适合全家观看，不尴尬，不狗血，不让人难看的影片：<b>《塞伦盖蒂》</b></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-bae3c892bd4ef365c62e1e9d5209536c_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""1080"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-bae3c892bd4ef365c62e1e9d5209536c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='1080'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""1080"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-bae3c892bd4ef365c62e1e9d5209536c_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-bae3c892bd4ef365c62e1e9d5209536c_b.jpg""></figure><p><br></p><p><b>壹丨被追杀的母亲</b></p><p><br></p><p>非洲，塞伦盖蒂大草原。</p><p>一只母狮，带着三只幼崽，奔驰在亡命的旅途中...</p><p>路途坎坷不平，孩子饥渴难耐，母亲步伐踉跄，但他们不能停留，因为，稍有迟疑，全家必死无疑…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-4ceb146f46071f94e148d82b35c29ec4_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-4ceb146f46071f94e148d82b35c29ec4_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-4ceb146f46071f94e148d82b35c29ec4_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-4ceb146f46071f94e148d82b35c29ec4_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-0f3f37b78999213935b96ce83c5d13a9_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-0f3f37b78999213935b96ce83c5d13a9_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-0f3f37b78999213935b96ce83c5d13a9_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-0f3f37b78999213935b96ce83c5d13a9_b.gif""></figure><p><br></p><p>这头名为卡丽的母狮，爱上了外来者，并且生下了不容于狮群的私生子...</p><p>然而，狮群很快发现了这个不伦的秘密…</p><p>掌权的雄狮，驱逐了卡丽，并且一路追杀…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-7baabb1ee9fd4b9eb5b3fa069b9e9118_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-7baabb1ee9fd4b9eb5b3fa069b9e9118_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-7baabb1ee9fd4b9eb5b3fa069b9e9118_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-7baabb1ee9fd4b9eb5b3fa069b9e9118_b.jpg""></figure><p><br></p><p>孤立无援的卡丽，选择踏上生死难测的亡命之旅...</p><p>尽管前路艰险，有狡猾的鬣狗，有凶悍的野牛，有残酷无情的大自然…</p><p>然而，身边的孩子，让她心生勇气，一往无前...</p><p>卡丽要做最坚强的母亲…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-dcbca86aa012c65b3ee33241ed0e5348_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-dcbca86aa012c65b3ee33241ed0e5348_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-dcbca86aa012c65b3ee33241ed0e5348_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-dcbca86aa012c65b3ee33241ed0e5348_b.jpg""></figure><p><br></p><p>狮群派出了杀手，千里追击，似乎一定要将她和孩子置于死地…</p><p>最接近的时候，杀手与她的距离，不过短短数十米…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-346e93ea107cdef9303b755af3a2d561_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-346e93ea107cdef9303b755af3a2d561_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-346e93ea107cdef9303b755af3a2d561_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-346e93ea107cdef9303b755af3a2d561_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-15c50a6f2f5e41b0ba9ce405dd84d6a8_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-15c50a6f2f5e41b0ba9ce405dd84d6a8_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-15c50a6f2f5e41b0ba9ce405dd84d6a8_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-15c50a6f2f5e41b0ba9ce405dd84d6a8_b.jpg""></figure><p><br></p><p>对于卡丽来说，就是天堂与地狱的距离。</p><p>她不敢停下步伐，只能继续穿越风沙、穿越枯草、穿越漫无边际的荒原…</p><p>每一个漫漫长夜，卡丽总是在担惊受怕…</p><p>她不知道，杀手会是否会发起偷袭。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-feb530c09a0a742ddb3c8e817c44a96a_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-feb530c09a0a742ddb3c8e817c44a96a_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-feb530c09a0a742ddb3c8e817c44a96a_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-feb530c09a0a742ddb3c8e817c44a96a_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-29df8b2e55b00ae6aeff7fa13924fc61_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-29df8b2e55b00ae6aeff7fa13924fc61_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-29df8b2e55b00ae6aeff7fa13924fc61_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-29df8b2e55b00ae6aeff7fa13924fc61_b.jpg""></figure><p><br></p><p>终于，这一天，被逼疯的卡丽，带着孩子，孤掷一注地朝着杀手走过去…</p><p>热风烈烈，卡丽高昂着头颅，低声咆哮。</p><p>她知道：是时候了结这无尽的苦难了…</p><p>按理说，一只母狮想要扑杀一只成年公狮，几乎是不可能的事情...</p><p>但身为母亲，卡丽别无选择，她决定赌命一搏...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-1872ff6327ad5862c23b26a667155bed_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-1872ff6327ad5862c23b26a667155bed_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-1872ff6327ad5862c23b26a667155bed_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-1872ff6327ad5862c23b26a667155bed_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-3fcd228d6284c3fecd9015a3fa80039c_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-3fcd228d6284c3fecd9015a3fa80039c_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-3fcd228d6284c3fecd9015a3fa80039c_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-3fcd228d6284c3fecd9015a3fa80039c_b.gif""></figure><p><br></p><p><b>贰丨接盘侠父亲</b></p><p><br></p><p>大岩石，是狒狒部落的根据地。</p><p>最近，一只凶残的雄狒狒，成功夺权，掌管了部落。</p><p>热衷铁血统治的新王，决定掀起一场血雨腥风…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-6e1a45adc6df5eafea50f2108b9e5034_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-6e1a45adc6df5eafea50f2108b9e5034_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-6e1a45adc6df5eafea50f2108b9e5034_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-6e1a45adc6df5eafea50f2108b9e5034_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-82a4a056361e463dbe01405dfa1e0c2f_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-82a4a056361e463dbe01405dfa1e0c2f_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-82a4a056361e463dbe01405dfa1e0c2f_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-82a4a056361e463dbe01405dfa1e0c2f_b.gif""></figure><p><br></p><p>拉飞奇，是一只普通的公狒狒，老实巴交，勤勤恳恳....</p><p>这一天，拉飞奇被绿了。</p><p>他的女朋友，被新王抢去，当上了王妃...</p><p>曾经举案齐眉的夫妻，如今只能趁着新王不注意，偷偷幽会…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-6a26cad880ff39b1e21652cde72a4d20_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-6a26cad880ff39b1e21652cde72a4d20_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-6a26cad880ff39b1e21652cde72a4d20_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-6a26cad880ff39b1e21652cde72a4d20_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-18475376a651874e116c5bb4d0eb9e30_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-18475376a651874e116c5bb4d0eb9e30_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-18475376a651874e116c5bb4d0eb9e30_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-18475376a651874e116c5bb4d0eb9e30_b.jpg""></figure><p><br></p><p>暴躁的新王，时常会把他们抓奸在床，“隔壁老王”总免不得吃一顿老拳…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-e4c45ce759f573ec1c0ca99b9d29b55a_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-e4c45ce759f573ec1c0ca99b9d29b55a_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-e4c45ce759f573ec1c0ca99b9d29b55a_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic3.zhimg.com/v2-e4c45ce759f573ec1c0ca99b9d29b55a_b.gif""></figure><p><br></p><p>可是，拉飞奇依旧跟在女神屁股后面转个不停...</p><p>有吃的，先给人家吃。</p><p>有虱子了，帮忙捉。</p><p>花豹来了，拉飞奇也不惜以身涉险，挡在女神前面…</p><p>拉飞奇，努力扮演着保镖、仆从、情人等多重角色...</p><p>他成了部落里人尽皆知的：完美的舔狗。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-52e09840a5277ceb5a06f15246380692_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-52e09840a5277ceb5a06f15246380692_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-52e09840a5277ceb5a06f15246380692_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-52e09840a5277ceb5a06f15246380692_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-30a5b1b84913748db052cde1a4fefc02_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-30a5b1b84913748db052cde1a4fefc02_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-30a5b1b84913748db052cde1a4fefc02_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-30a5b1b84913748db052cde1a4fefc02_b.jpg""></figure><p><br></p><p>这一天，悲剧发生了：狒狒群遭遇蟒蛇攻击，拉飞奇的前女友不幸身亡…</p><p>所有的狒狒，早就四散离开，逃命去了...</p><p>只有拉飞奇，还在试图拉走蟒蛇缠绕的前女友尸体…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-c22db36a0abe92a194553bcd98d8c33b_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-c22db36a0abe92a194553bcd98d8c33b_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-c22db36a0abe92a194553bcd98d8c33b_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-c22db36a0abe92a194553bcd98d8c33b_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-173f119c9c7c0e63f104ea8251e324f2_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-173f119c9c7c0e63f104ea8251e324f2_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-173f119c9c7c0e63f104ea8251e324f2_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-173f119c9c7c0e63f104ea8251e324f2_b.jpg""></figure><p><br></p><p>他不愿接受现实，可是另一边，女友跟新王生的孩子，还没长大...</p><p>年轻的王子，失去了母亲，父皇又不管不问，眼看也活不长了…</p><p>此时，拉飞奇决定：收养这只毫无血缘关系的幼仔…</p><p>于是，拉飞奇成了一个莫名其妙的继父。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-328252ef9f8663a36ae7934a395e0ee4_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-328252ef9f8663a36ae7934a395e0ee4_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-328252ef9f8663a36ae7934a395e0ee4_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-328252ef9f8663a36ae7934a395e0ee4_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-6a4f249df5f08fe0ae2fe0bd4c8b705b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-6a4f249df5f08fe0ae2fe0bd4c8b705b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-6a4f249df5f08fe0ae2fe0bd4c8b705b_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-6a4f249df5f08fe0ae2fe0bd4c8b705b_b.jpg""></figure><p><br></p><p>作为一个单身汉，拉飞奇从来没照顾过小孩。</p><p>和每一个笨手笨脚的新手爸爸一样，他甚至连怎么抱小孩都不知道…</p><p>而蛮横的新王，还时时刻刻冲着拉飞奇，冷嘲热讽，恐吓羞辱…</p><p>有一天，还直接把屁股怼到了拉飞奇脸上…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-a3d21602bff6629b875d70e0f22363e6_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-a3d21602bff6629b875d70e0f22363e6_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-a3d21602bff6629b875d70e0f22363e6_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-a3d21602bff6629b875d70e0f22363e6_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-ee8f7804685ff75295a4092332b7de0d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-ee8f7804685ff75295a4092332b7de0d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-ee8f7804685ff75295a4092332b7de0d_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-ee8f7804685ff75295a4092332b7de0d_b.jpg""></figure><p><br></p><p>拉飞奇紧紧抱着孩子，他只能低头忍受，他只能委曲求全，他必须忍辱负重...</p><p>因为，他已为人父。</p><p>即使，这个孩子，并非亲生，但那毕竟是挚爱的遗腹子啊...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-5e5b30936cb60b3729b1a537ea01d639_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-5e5b30936cb60b3729b1a537ea01d639_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-5e5b30936cb60b3729b1a537ea01d639_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-5e5b30936cb60b3729b1a537ea01d639_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-982d0f366305ebd50cc6987522aa95ce_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-982d0f366305ebd50cc6987522aa95ce_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-982d0f366305ebd50cc6987522aa95ce_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-982d0f366305ebd50cc6987522aa95ce_b.jpg""></figure><p><br></p><p>尽管，拉飞奇很想扮演好父亲的角色，可是孩子需要奶水，去哪找呢？</p><p>在茫茫的非洲大草原上，在适者生存的舞台上，在弱肉强食的世界里...</p><p>这个痴情的男子，这个深情的父亲，举步维艰...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-bce245cf6a99bde19b16020cbbb57674_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-bce245cf6a99bde19b16020cbbb57674_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-bce245cf6a99bde19b16020cbbb57674_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-bce245cf6a99bde19b16020cbbb57674_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-3ec9be203f0288da63f62e474583a018_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-3ec9be203f0288da63f62e474583a018_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-3ec9be203f0288da63f62e474583a018_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-3ec9be203f0288da63f62e474583a018_b.jpg""></figure><p><br></p><p><b>叁丨疲惫的母亲</b></p><p><br></p><p>花豹基基，最近成为了母亲。</p><p>这是她第一次做母亲，一切都很新鲜，也很手足无措…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-a6da8e69f7c5556a2e64fe95967b2c02_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-a6da8e69f7c5556a2e64fe95967b2c02_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-a6da8e69f7c5556a2e64fe95967b2c02_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-a6da8e69f7c5556a2e64fe95967b2c02_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-fef4965e6e335058b274e1a91083977c_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-fef4965e6e335058b274e1a91083977c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-fef4965e6e335058b274e1a91083977c_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-fef4965e6e335058b274e1a91083977c_b.jpg""></figure><p><br></p><p>做母亲，意味着要改变自己的生活方式：</p><p>居住的地方，需要换；</p><p>狩猎的方式，需要换；</p><p>就连每天都作息，都不再相同…</p><p>职场妈妈，白天疲于奔命，晚上还得回家操心奶娃…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-b287c0cb3e9126a60a54a39c76286f75_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-b287c0cb3e9126a60a54a39c76286f75_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-b287c0cb3e9126a60a54a39c76286f75_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-b287c0cb3e9126a60a54a39c76286f75_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-115ce3a5babcc33d59cf3ef93a73eccf_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-115ce3a5babcc33d59cf3ef93a73eccf_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-115ce3a5babcc33d59cf3ef93a73eccf_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-115ce3a5babcc33d59cf3ef93a73eccf_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-75413cdf914bfa5830cd74524a1ca40a_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-75413cdf914bfa5830cd74524a1ca40a_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-75413cdf914bfa5830cd74524a1ca40a_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-75413cdf914bfa5830cd74524a1ca40a_b.jpg""></figure><p><br></p><p>更麻烦的是，新搬来的邻居，个个不是善茬：</p><p>狡猾的胡狼，图谋不轨...</p><p>丑陋的鬣狗，狼子野心...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-344594ecb8fc1699b9e72275cd33cf52_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-344594ecb8fc1699b9e72275cd33cf52_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-344594ecb8fc1699b9e72275cd33cf52_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-344594ecb8fc1699b9e72275cd33cf52_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-c7e4ee0d8611533db3e1358225de61b1_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-c7e4ee0d8611533db3e1358225de61b1_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-c7e4ee0d8611533db3e1358225de61b1_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/v2-c7e4ee0d8611533db3e1358225de61b1_b.gif""></figure><p><br></p><p>尽管形影单只，尽管时常饥肠辘辘，但是基基从未想要放弃。</p><p>因为身后，是嗷嗷待哺的孩子…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-1728adeb9412a1f7687a0386b3708c7e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-1728adeb9412a1f7687a0386b3708c7e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-1728adeb9412a1f7687a0386b3708c7e_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-1728adeb9412a1f7687a0386b3708c7e_b.jpg""></figure><p><br></p><p>这一天，基基好不容易打猎完回来，最糟糕的事发生了：</p><p>无论她如何呼唤，孩子都没有出现…</p><p>他家门口，有一条剧毒绿蝰蛇，缓缓游过…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-10366ba2d9343ebc56cc2b86f95fb473_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-10366ba2d9343ebc56cc2b86f95fb473_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-10366ba2d9343ebc56cc2b86f95fb473_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/v2-10366ba2d9343ebc56cc2b86f95fb473_b.gif""></figure><p><br></p><p>找了一大圈，基基发现：孩子并没有遭遇不测，而是全钻树上去了。</p><p>她来不及高兴，心塞的事又发生了：好不容易打回来的猎物，被胡狼直接叼走了…</p><p>基基坐在地上，当场崩溃：<b>单亲妈妈，真难啊！</b></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-64c44838aa953f9830c6b329fb822c0d_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-64c44838aa953f9830c6b329fb822c0d_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-64c44838aa953f9830c6b329fb822c0d_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-64c44838aa953f9830c6b329fb822c0d_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-238599dba7f904279775d74f4dec7fc4_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-238599dba7f904279775d74f4dec7fc4_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-238599dba7f904279775d74f4dec7fc4_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic3.zhimg.com/v2-238599dba7f904279775d74f4dec7fc4_b.gif""></figure><p><br></p><p>这部《塞伦盖蒂》，是于今年7月推出的6集纪录片，一经推出，圈粉无数，豆瓣评分高达9.4分。</p><p>有网友评论道：<b>满分是最低分，可以更多的话应该给十倍的满分！每一帧都如此有温度，每一幕都是无与伦比的大片！</b></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-bd689a89f838476624c1fd0d7639279b_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-bd689a89f838476624c1fd0d7639279b_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-bd689a89f838476624c1fd0d7639279b_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-bd689a89f838476624c1fd0d7639279b_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-8a9c27af42bc83be57e42e2234e750fe_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-8a9c27af42bc83be57e42e2234e750fe_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-8a9c27af42bc83be57e42e2234e750fe_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-8a9c27af42bc83be57e42e2234e750fe_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-eb310bf6c02d9f46023414e1b9ddd2f7_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-eb310bf6c02d9f46023414e1b9ddd2f7_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-eb310bf6c02d9f46023414e1b9ddd2f7_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-eb310bf6c02d9f46023414e1b9ddd2f7_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-94d5085691515549e3c21013ba0aed10_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-94d5085691515549e3c21013ba0aed10_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-94d5085691515549e3c21013ba0aed10_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-94d5085691515549e3c21013ba0aed10_b.jpg""></figure><p><br></p><p>BBC出品的纪录片，气势不凡。</p><p>无论是光影、构图、色彩，都堪称无与伦比，世所罕见…</p><p>有些镜头，更是让人会心一笑：</p><p>经常摔跤的幼象...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-52b34228174c8c2c0c78adf0481c2439_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-52b34228174c8c2c0c78adf0481c2439_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-52b34228174c8c2c0c78adf0481c2439_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-52b34228174c8c2c0c78adf0481c2439_b.gif""></figure><p><br></p><p>我太难了</p><p>什么都不会，只会卖萌的小狮子...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-eee0e133e9b8942a1cd521fdf480462b_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-eee0e133e9b8942a1cd521fdf480462b_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-eee0e133e9b8942a1cd521fdf480462b_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-eee0e133e9b8942a1cd521fdf480462b_b.gif""></figure><p><br></p><p>想躲老婆唠叨而爬上树的公狮子，被蜜蜂叮得落荒而逃…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-ba85c06f917dbcbbf05f23a7f3e0b85e_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-ba85c06f917dbcbbf05f23a7f3e0b85e_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-ba85c06f917dbcbbf05f23a7f3e0b85e_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-ba85c06f917dbcbbf05f23a7f3e0b85e_b.gif""></figure><p><br></p><p>值得一提的是，几首为动物量身定做的主题曲，更是恰如其分，婉转动听…</p><p>这是一部质量上佳的合家欢影片，没有尴尬，消弭代沟。</p><p>无论是老人还是孩子，无论是公司白领还是厨房小工，无论是抠脚大汉还是文雅淑女…</p><p>在这部影片面前，没有认知障碍，没有矛盾分歧。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-851ade7e7142c25f30ced21b11732323_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-851ade7e7142c25f30ced21b11732323_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-851ade7e7142c25f30ced21b11732323_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-851ade7e7142c25f30ced21b11732323_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-b8478899e23fc48e4b966e4aed1287b3_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-b8478899e23fc48e4b966e4aed1287b3_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-b8478899e23fc48e4b966e4aed1287b3_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic3.zhimg.com/v2-b8478899e23fc48e4b966e4aed1287b3_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-cdc2d3ca950f3d642bc89cd447ed6821_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-cdc2d3ca950f3d642bc89cd447ed6821_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-cdc2d3ca950f3d642bc89cd447ed6821_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-cdc2d3ca950f3d642bc89cd447ed6821_b.jpg""></figure><p><br></p><p>想想在地铁公交上心力憔悴的自己，再看看草原的长河落日、万兽奔腾、生死相搏、优胜劣汰…</p><p>我们是人类，我们也是动物...</p><p>它们是动物，他们也有近似我们的人性...</p><p>忍不住想为它们鼓掌呐喊、热泪盈眶...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-31f34a878f3f21106321aed1c0589e23_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-31f34a878f3f21106321aed1c0589e23_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-31f34a878f3f21106321aed1c0589e23_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/v2-31f34a878f3f21106321aed1c0589e23_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-f01e4bb6c916d582223ee261889277de_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-f01e4bb6c916d582223ee261889277de_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-f01e4bb6c916d582223ee261889277de_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-f01e4bb6c916d582223ee261889277de_b.jpg""></figure><p><br></p><p>这部《塞伦盖蒂》以秃鹫的视角，记录了动物家庭的野性故事：</p><p>狮子、狒狒、猎豹、大象、杂色狼、鬣狗、猫鼬、疣猪、斑马、鸵鸟…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-25c05000a84ffd680860e09a40befe6b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-25c05000a84ffd680860e09a40befe6b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-25c05000a84ffd680860e09a40befe6b_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-25c05000a84ffd680860e09a40befe6b_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-ff683448ef3f291ccbb24c9787191b31_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-ff683448ef3f291ccbb24c9787191b31_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-ff683448ef3f291ccbb24c9787191b31_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-ff683448ef3f291ccbb24c9787191b31_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-4922e9a5f14148842ad01bf836823f68_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-4922e9a5f14148842ad01bf836823f68_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-4922e9a5f14148842ad01bf836823f68_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-4922e9a5f14148842ad01bf836823f68_b.jpg""></figure><p><br></p><p>它充满了强烈的戏剧色彩，让人感觉这就是个亲子节目：</p><p>拉飞奇的熊孩子，特别皮，一不留神，就要上房揭瓦…</p><p>他跟每一个新手爸妈一样，被精力旺盛的娃，折腾到完全睡不够。</p><p>以致于带孩子带到一半，就会不小心睡过去…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-476f56b2336763dbed259a3915440a48_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-476f56b2336763dbed259a3915440a48_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-476f56b2336763dbed259a3915440a48_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/v2-476f56b2336763dbed259a3915440a48_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-97c3c6935cbef23719064ca91e15eccd_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-97c3c6935cbef23719064ca91e15eccd_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-97c3c6935cbef23719064ca91e15eccd_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-97c3c6935cbef23719064ca91e15eccd_b.gif""></figure><p><br></p><p>卡丽给孩子们找了个后爹，指望着后爹能在她出去打猎时候，好好照顾孩子。</p><p>没想到，她前脚出门，后爹就直接睡着了…</p><p>等卡丽回来时候，孩子一个个都早已完成了泥猴…</p><p>这个后爹，生动诠释了什么叫：丧偶式育儿。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-76d163eacf344d1a1a231bbb60749192_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-76d163eacf344d1a1a231bbb60749192_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-76d163eacf344d1a1a231bbb60749192_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-76d163eacf344d1a1a231bbb60749192_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-aa8855bc96908e501701aaef977fe536_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-aa8855bc96908e501701aaef977fe536_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-aa8855bc96908e501701aaef977fe536_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-aa8855bc96908e501701aaef977fe536_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-754892b4ef82b3a55bc36665861c6420_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-754892b4ef82b3a55bc36665861c6420_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-754892b4ef82b3a55bc36665861c6420_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-754892b4ef82b3a55bc36665861c6420_b.gif""></figure><p><br></p><p>大象家族，迎来了一个新成员。</p><p>然而，和每一个二胎家庭一样，老大有点吃醋，不开心…</p><p>他拼命表现着自己，试图把长辈们的注意力，转到自己身上。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-9d0e050227d323008305287ac1243d30_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-9d0e050227d323008305287ac1243d30_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-9d0e050227d323008305287ac1243d30_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-9d0e050227d323008305287ac1243d30_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-d0a393cddd459180abc9849a36526e67_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-d0a393cddd459180abc9849a36526e67_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-d0a393cddd459180abc9849a36526e67_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/v2-d0a393cddd459180abc9849a36526e67_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-a9d5db11def6b03ac6a3eabc619e83cc_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-a9d5db11def6b03ac6a3eabc619e83cc_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-a9d5db11def6b03ac6a3eabc619e83cc_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/v2-a9d5db11def6b03ac6a3eabc619e83cc_b.gif""></figure><p><br></p><p>花豹基基历经千辛万苦，终于把三个孩子抚养长大。</p><p>她看着孩子学会了捕猎，在确认孩子能够独立生活后，她选择了悄悄离开…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-21104f82dd360d97556bea8cf47b71b8_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-21104f82dd360d97556bea8cf47b71b8_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-21104f82dd360d97556bea8cf47b71b8_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-21104f82dd360d97556bea8cf47b71b8_b.gif""></figure><p><br></p><p>这辈子，基基与孩子们，都不太可能母子重逢。</p><p>但是，无论孩子远行到哪里，他们都会走上与母亲相同的道路，用母亲教会他们的知识，在塞伦盖蒂，努力生存下去。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-7d15a101d5dc968957315477c0ca7a2f_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-7d15a101d5dc968957315477c0ca7a2f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-7d15a101d5dc968957315477c0ca7a2f_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-7d15a101d5dc968957315477c0ca7a2f_b.jpg""></figure><p><br></p><p>你会发现，原来，这世上的爱恨情仇与聚散离合，并非是人类独有。</p><p>在非洲大草原上，在动物世界里，除了弱肉强食，除了物竞天择，也有春风化雨，也有父爱如山…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-9f166b7411ff454d6616d33ee1224741_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-9f166b7411ff454d6616d33ee1224741_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-9f166b7411ff454d6616d33ee1224741_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic3.zhimg.com/v2-9f166b7411ff454d6616d33ee1224741_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-66a0fc1a15f14af80807c17879f429b2_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-66a0fc1a15f14af80807c17879f429b2_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-66a0fc1a15f14af80807c17879f429b2_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic3.zhimg.com/v2-66a0fc1a15f14af80807c17879f429b2_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-3e1823a80ddb45801ebcf93a96eca1a4_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-3e1823a80ddb45801ebcf93a96eca1a4_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-3e1823a80ddb45801ebcf93a96eca1a4_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-3e1823a80ddb45801ebcf93a96eca1a4_b.jpg""></figure><p><br></p><p>全剧最悲壮的一幕，来自于最不讨喜的鬣狗家族。</p><p>当女儿遭遇狮子军团的袭击，首领母亲选择挺身而出，转移了狮子军团的注意力。</p><p>最终，母亲用死亡，换取了女儿的平安…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-e29df7955b7b105d46feef8742d1c897_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-e29df7955b7b105d46feef8742d1c897_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-e29df7955b7b105d46feef8742d1c897_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic3.zhimg.com/v2-e29df7955b7b105d46feef8742d1c897_b.gif""></figure><p><br></p><p>在《塞伦盖蒂》里，我们能看到：</p><p>一个身处险境的母亲，破釜沉舟，为孩子挺身而出；</p><p>一个备受欺凌的父亲，做低伏小，为孩子忍辱负重；</p><p>一个内外交困的母亲，咬牙前行，为孩子遮风避雨…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-08bc69846406a1a347c3a15eb7667df1_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-08bc69846406a1a347c3a15eb7667df1_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic1.zhimg.com/v2-08bc69846406a1a347c3a15eb7667df1_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/v2-08bc69846406a1a347c3a15eb7667df1_b.gif""></figure><p><br></p><p>一只久久不愿在早夭孩子身边离去的长颈鹿；</p><p>一只为了孩子，奋力一搏挣脱泥潭的斑马；</p><p>一只在孩子面前，难得露出温柔一面的鳄鱼…</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-8c0b5ceee23fb45f2565a4e73aa016ed_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-8c0b5ceee23fb45f2565a4e73aa016ed_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-8c0b5ceee23fb45f2565a4e73aa016ed_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-8c0b5ceee23fb45f2565a4e73aa016ed_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-3d0377fb925bf889d388f77da750a0ca_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-3d0377fb925bf889d388f77da750a0ca_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='608'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""608"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-3d0377fb925bf889d388f77da750a0ca_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-3d0377fb925bf889d388f77da750a0ca_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-90b4632733856c03827f72845cb798aa_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-90b4632733856c03827f72845cb798aa_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic2.zhimg.com/v2-90b4632733856c03827f72845cb798aa_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic2.zhimg.com/v2-90b4632733856c03827f72845cb798aa_b.gif""></figure><p><br></p><p>它们走向漫天风沙，走向狂风暴雨，走向饥饿与痛苦，甚至走向死亡泥潭…</p><p>这些我们认为没有人性的动物，为了庇护家人，它们同样选择了悲壮厚重，他们同样践行着爱与责任...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-1115f96ac2344c67223d3e9cce0e7784_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-1115f96ac2344c67223d3e9cce0e7784_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-1115f96ac2344c67223d3e9cce0e7784_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-1115f96ac2344c67223d3e9cce0e7784_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-cfb718744027d12ef73394f645f73c37_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""607"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-cfb718744027d12ef73394f645f73c37_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='607'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""607"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-cfb718744027d12ef73394f645f73c37_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-cfb718744027d12ef73394f645f73c37_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-56d997f333b801991c22915cab6f3273_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""607"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-56d997f333b801991c22915cab6f3273_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='607'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""607"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-56d997f333b801991c22915cab6f3273_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-56d997f333b801991c22915cab6f3273_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-bda9ad8bce9726ce046c3d6af924aeec_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""606"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-bda9ad8bce9726ce046c3d6af924aeec_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='606'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""606"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-bda9ad8bce9726ce046c3d6af924aeec_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-bda9ad8bce9726ce046c3d6af924aeec_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-02d612daa431a6e7edd1911037ad2926_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""570"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-02d612daa431a6e7edd1911037ad2926_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='570'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""570"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-02d612daa431a6e7edd1911037ad2926_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-02d612daa431a6e7edd1911037ad2926_b.jpg""></figure><p><br></p><p>或许，你可以说，动物的这些所谓的“感情”不过是基因选择，不过是原始本能...</p><p>但回头看看我们自己，看看某些“连畜牲都不如”的人类，或许，是我们低估了动物，也是我们高估了自己...</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-f385684b0992b4fe9d91bb6b1e013db0_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-f385684b0992b4fe9d91bb6b1e013db0_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic3.zhimg.com/v2-f385684b0992b4fe9d91bb6b1e013db0_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic3.zhimg.com/v2-f385684b0992b4fe9d91bb6b1e013db0_b.gif""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-018b87ed45c337c4dd1fd19129f3cbe2_b.gif"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-018b87ed45c337c4dd1fd19129f3cbe2_b.jpg"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""400"" data-rawheight=""225"" data-thumbnail=""https://pic4.zhimg.com/v2-018b87ed45c337c4dd1fd19129f3cbe2_b.jpg"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic4.zhimg.com/v2-018b87ed45c337c4dd1fd19129f3cbe2_b.gif""></figure><p></p>",416,23,,乌鸦电影,https://api.zhihu.com/people/94a691b9d8988331214d56b3ecbda773,赞同了文章
1573638507209,Bilibili（哔哩哔哩）上有哪些值得一看的纪录片？,,1858,40,144328,https://api.zhihu.com/questions/315958918,Phyllis,https://api.zhihu.com/people/1ff450604ecd94fcfa7714653168b503,1552574617,1573638507,1590989144,"<p>太激动了！一定要和大家分享这些宝藏！</p><p>（<b>尤其是家长和学生朋友</b>）</p><p>纪录片的世界是在太广阔了，估计一个人花费一生的时间都看不完！</p><p>那怎么办？</p><p>傲梦君从<b>天文物理、地理、数学、IT/互联网、伟大人物传记</b>等几个方面，</p><p>精选出全网高分优质（<b>绝大部分超过9分</b>）的纪录片，</p><p>并依照<b>豆瓣评分</b>从高到低进行了排列！（这个标准比较能保证纪录片品质！！！）</p><p><br></p><p><b>11部天文物理纪录片</b>，见识宇宙之辽阔，惊叹人类之渺小；</p><p><b>11部地理纪录片</b>，探索人类栖息之星球，自然现象之奥秘；</p><p><b>10部数学纪录片</b>，领略抽象数学之美，探寻万物的本质规律；</p><p><b>8部IT/互联网纪录片</b>，探寻传奇背后的故事，感受剧变的时代风潮；</p><p><b>5 部科学界伟大人物纪录片</b>，站在巨人的肩膀上，看得更远。</p><p><br></p><p>好的纪录片，比干巴巴的教科书更形象生动有趣，足以打开一个又一个新奇的宇宙！</p><p>好，废话不多，请收好！ </p><p><b><i>以下是正文：</i></b></p><h2><b>一、11部天文物理纪录片，见识宇宙之辽阔，惊叹人类之渺小</b> </h2><p><b>NO. 01</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-465444d99e743831b1838c97fa14ec78_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""297"" data-default-watermark-src=""https://pic2.zhimg.com/v2-9b748ae8acad4a8f556d9cd5d8e2014d_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic3.zhimg.com/v2-465444d99e743831b1838c97fa14ec78_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='297'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""297"" data-default-watermark-src=""https://pic2.zhimg.com/v2-9b748ae8acad4a8f556d9cd5d8e2014d_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic3.zhimg.com/v2-465444d99e743831b1838c97fa14ec78_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-465444d99e743831b1838c97fa14ec78_b.jpg""></figure><blockquote> 片名：宇宙时空之旅<br>豆瓣评分：9.7</blockquote><p>《宇宙时空之旅》该系列共十三集，由知名天文学家奈尔·德葛拉司·泰森博士主持，影片叙述我们如何发现大自然法则以及找到我们在宇宙与时间中的座标。这个系列生动地叙述前所未闻的故事，描述对于知识的伟大探索，带观众到崭新世界，并且穿越宇宙，带领观众以最宏观和最微观的角度来审视宇宙。 </p><p><b>NO. 02</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-b8bd53178dabd201fa9f934a8c07d75f_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""233"" data-default-watermark-src=""https://pic2.zhimg.com/v2-43a8fcf7c2926c0ce46222b339700cf5_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic1.zhimg.com/v2-b8bd53178dabd201fa9f934a8c07d75f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='233'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""233"" data-default-watermark-src=""https://pic2.zhimg.com/v2-43a8fcf7c2926c0ce46222b339700cf5_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic1.zhimg.com/v2-b8bd53178dabd201fa9f934a8c07d75f_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-b8bd53178dabd201fa9f934a8c07d75f_b.jpg""></figure><blockquote>  片名：行星<br>豆瓣评分：9.6</blockquote><p>138亿年前宇宙的故事有了开端，仅银河系就有数千颗恒星，而我们生活的太阳系行星渺小如尘埃，不为人知的行星故事在过去40亿年中华丽上演。随着人类观测技术的不断进步，宇宙飞船带领我们探索各个行星，此部纪录片用独特的拟人化手法解构八大行星的故事，并结合最先进的科技给我们最华丽的视觉享受。</p><p><b>NO.  03</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-8b9498de3211c4ba83be4bf83b308ced_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""229"" data-default-watermark-src=""https://pic2.zhimg.com/v2-dc8aa29859b07ce7a925d2c62deb8708_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-8b9498de3211c4ba83be4bf83b308ced_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='229'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""229"" data-default-watermark-src=""https://pic2.zhimg.com/v2-dc8aa29859b07ce7a925d2c62deb8708_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-8b9498de3211c4ba83be4bf83b308ced_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-8b9498de3211c4ba83be4bf83b308ced_b.jpg""></figure><blockquote>片名：从太空看地球<br>豆瓣评分：9.6</blockquote><p>从太空俯瞰地球，我们的地球之美令人心旷神怡。与此同时，我们生活的时代，地球表面正在发生着前所未有的迅速变化，我们目睹着人类行为给地球带来积极/消极的影响。我们试着换一个角度，借助架设在地面、空中、太空中的拍摄设备，用更辽阔的视角讲述地球上的生命故事，以及前所未见的地球家园。 </p><p><b>NO.  04</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-f2d06479a141d0e538592ff5e7b03049_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""281"" data-default-watermark-src=""https://pic3.zhimg.com/v2-4251bb68fe66a6009bfc1c095828197c_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-f2d06479a141d0e538592ff5e7b03049_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='281'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""281"" data-default-watermark-src=""https://pic3.zhimg.com/v2-4251bb68fe66a6009bfc1c095828197c_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-f2d06479a141d0e538592ff5e7b03049_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-f2d06479a141d0e538592ff5e7b03049_b.jpg""></figure><blockquote>片名：了解宇宙是如何运行的<br>豆瓣评分：9.6</blockquote><p>宇宙是怎样形成的？宇宙是怎样运行的？这些都是人类经常提出的问题。作为宇宙的一分子，我们都希望可以对这个神秘国度有更深入的认识。《解读宇宙》将为天文爱好者拆解种种谜团，节目将会探究宇宙的基本要素，发掘连串迷人的天文现象，以及解释其成因，带领观众了解宇宙的结构、太阳系、银河系等。《解读宇宙》与观众一同探索这个奥妙神秘、浩瀚无边的宇宙国度。目前已经更新到了第三季，每一季都是高水准。</p><p><b>NO.  05</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-f0149419c6b239393e9ee60e631af9e6_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""237"" data-default-watermark-src=""https://pic4.zhimg.com/v2-5eb5ac3e373f2b85181fc66288808471_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-f0149419c6b239393e9ee60e631af9e6_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='237'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""237"" data-default-watermark-src=""https://pic4.zhimg.com/v2-5eb5ac3e373f2b85181fc66288808471_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-f0149419c6b239393e9ee60e631af9e6_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-f0149419c6b239393e9ee60e631af9e6_b.jpg""></figure><blockquote>片名：宇宙的构造<br>豆瓣评分：9.5</blockquote><p>这是基于著名物理学家，畅销书作家布莱恩·格林（Brian Greene）的同名著作拍摄的一部4集纪录片。它向我们展现了科学家对于空间、时间最复杂图景的理解，以及宇宙中最有悖于我们常识的一面。</p><p>在我们习以为常的现实世界之下，是另一个令人惊叹的世界，在这里，我们对于宇宙的大部分理解都是错的。物理学家兼畅销书作家布莱恩·格林，带你开始一段颠覆常识的旅程。</p><p>但是事情怎么会这样？对于我们熟悉的事物，我们为何错得如此离谱？看完之后，也许会颠覆你很多习以为常的观念。 </p><p><b>NO.  06</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-4fecff8b26db1c9a21a621b26fd0a5c8_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""295"" data-default-watermark-src=""https://pic1.zhimg.com/v2-e88864f43a0bcf4464970f08cb4ee861_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic4.zhimg.com/v2-4fecff8b26db1c9a21a621b26fd0a5c8_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='295'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""295"" data-default-watermark-src=""https://pic1.zhimg.com/v2-e88864f43a0bcf4464970f08cb4ee861_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic4.zhimg.com/v2-4fecff8b26db1c9a21a621b26fd0a5c8_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-4fecff8b26db1c9a21a621b26fd0a5c8_b.jpg""></figure><blockquote>片名：当我们离开地球：美国国家航空航天局的太空行动<br>豆瓣评分：9.5</blockquote><p>美国国家航空航天局打开了他们的保险箱，将珍贵的影片资料借给美国探索频道成就了这部惊人的高清纪录片。该片讲述了人类最伟大的探险的迷人故事，并且由探索者亲自讲述。从早期的水星计划到载人航天，到影响深远的登月，从联盟号航天飞机对接到Bruce McCandless的人类第一次无绳太空行走，这就是太空时代的来临。本片汇集了古老的视频样片以及宇航员们亲自摄录的关键录像，加上飞船上的摄像机的珍贵记录，让本片能够以前所未见的角度给大家讲述这个太空探索的故事。</p><p><b>NO.  07</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-faf3e2d20f400293e9b9bd8500462203_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""219"" data-default-watermark-src=""https://pic1.zhimg.com/v2-4b2d26825b90e18531c480923a94fed6_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic1.zhimg.com/v2-faf3e2d20f400293e9b9bd8500462203_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='219'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""219"" data-default-watermark-src=""https://pic1.zhimg.com/v2-4b2d26825b90e18531c480923a94fed6_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic1.zhimg.com/v2-faf3e2d20f400293e9b9bd8500462203_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-faf3e2d20f400293e9b9bd8500462203_b.jpg""></figure><blockquote>片名：旅行者号：冲出太阳系<br>豆瓣评分：9.4</blockquote><p>人类目前探索宇宙中，最为非同寻常的旅程，最为彪悍的空间探测器—旅行者号们。1977年两艘无人驾驶宇宙飞船由NASA发射升空，飞向遥远的宇宙空间。孤独的旅行者号从发射出发，可能以后也不会回到地球了，承载着人类的探索精神，一直前行。</p><p><b>NO.  08</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-e9aa7f07e528a29e01be37cf31600b77_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""276"" data-default-watermark-src=""https://pic2.zhimg.com/v2-3953775af775897f06e581a6bef83ecf_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-e9aa7f07e528a29e01be37cf31600b77_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='276'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""276"" data-default-watermark-src=""https://pic2.zhimg.com/v2-3953775af775897f06e581a6bef83ecf_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-e9aa7f07e528a29e01be37cf31600b77_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-e9aa7f07e528a29e01be37cf31600b77_b.jpg""></figure><blockquote>片名：行星旅行指南<br>豆瓣评分：9.4</blockquote><p>纪录片《星际旅行指南》为6集，按照距离太阳的最近和最远，依次介绍了金星和水星、火星、木星、土星、海王星和天王星以及冥王星等行星。</p><p>在纪录片《星际旅行指南》中，不但能将太阳系行星的神秘景观尽收眼底，还能深入了解形成这些景观的原因所在。在天文学家和航天专家通俗易懂的讲述中，在一幅幅令人叹为观止的太空影像中，飞越地球，穿梭于太阳系各大行星之间，完成一次奇妙而兴奋的星际之旅。 </p><p><b>NO.  09</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-bf289159ba319637e59e7e98f79524c9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""249"" data-default-watermark-src=""https://pic1.zhimg.com/v2-bee40bb5e47a838674b56eb87dcbad38_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-bf289159ba319637e59e7e98f79524c9_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='249'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""249"" data-default-watermark-src=""https://pic1.zhimg.com/v2-bee40bb5e47a838674b56eb87dcbad38_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-bf289159ba319637e59e7e98f79524c9_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-bf289159ba319637e59e7e98f79524c9_b.jpg""></figure><blockquote>片名：旅行到宇宙边缘<br>豆瓣评分：9.3</blockquote><p>影片从离我们最近的月球开始，一路引导我们探索太阳系到银河系以至人类所认知的宇宙的“边缘”。细致呈现太阳系八大行星的神奇地貌，奇幻的气象变化，3D制作效果极致真实，带领你逐步走出太阳系，饱览整个宇宙的神奇景象，科幻和天文爱好者绝对不能错过的好片！ </p><p>如果我们能够前往宇宙的边缘，我们会看见什么? 是什么力量凝聚了宇宙，现在有什么力量导致宇宙的最终毁灭?这部片子将带领观众进行跨越宇宙的史诗之旅，详细观察恒星如何诞生、宇宙的终极命运、其他行星上的生命、以及最神秘也最骇人的现象：黑洞。</p><p><b>NO.  10</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-2bfa78d050ce5ed9d7475ef1adf318b5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""243"" data-default-watermark-src=""https://pic3.zhimg.com/v2-2fb6c3d3cd43e5e806aec7d4b8bb6675_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-2bfa78d050ce5ed9d7475ef1adf318b5_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='243'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""243"" data-default-watermark-src=""https://pic3.zhimg.com/v2-2fb6c3d3cd43e5e806aec7d4b8bb6675_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-2bfa78d050ce5ed9d7475ef1adf318b5_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-2bfa78d050ce5ed9d7475ef1adf318b5_b.jpg""></figure><blockquote> 片名：恒星七纪<br>豆瓣评分：9.2</blockquote><p>每当太阳落山，夜幕降临，坐在大地，仰望星空，就能看到一部史诗巨作在眼前上演，这部巨作拥有数十亿演员——恒星。每一颗都有属于自己的传奇。从尘埃和气体云中孕育，点燃核聚变的火花，度过辉煌灿烂的一生，晚年步入红巨星。之后或是变为白矮星，悄然消失；或是成为超新星，上演宇宙中最壮观的烟火秀，留下中子星，甚至是谜一般的黑洞。在恒星步入死亡的过程中，却悄然点燃了生命的种子，散播到宇宙深处……。 </p><p><b>NO.  11</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-baaef91185ac385f39735bc58187fa51_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""207"" data-default-watermark-src=""https://pic3.zhimg.com/v2-a78fca7e9248f8cb2bb8bc8ef0290756_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-baaef91185ac385f39735bc58187fa51_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='207'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""207"" data-default-watermark-src=""https://pic3.zhimg.com/v2-a78fca7e9248f8cb2bb8bc8ef0290756_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-baaef91185ac385f39735bc58187fa51_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-baaef91185ac385f39735bc58187fa51_b.jpg""></figure><blockquote> 片名：大自然启示录<br>豆瓣评分：9.2</blockquote><p>在三集惊人的剧集中，NatureTech以清新的眼光看世界，自然与科技携手并进。 本系列探讨苍蝇如何改善搜索和救援; 白蚁粪便有助于建设者; 松果激发新时代服装和鲨鱼奥运泳衣; 纳米技术模仿飞蛾; 北极罂粟影响生态友好的建筑; 蟑螂启发太空工程师; 鱼变成了汽车，飞机变成了鸟的形状。</p><p>NatureTech获得了Magic of Motion的自然纪录片/戏剧娱乐类别的电影摄影艾美奖，金牌家长选择基金奖以及杰克逊霍尔野生动物电影节的最佳限量系列奖，以及其他提名。 </p><h2><b>二、11部地理纪录片，探索人类栖息之星球，自然现象之奥秘</b></h2><p><b>NO. 01</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-17ab44d23e66acef6345fd8115d61c19_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""246"" data-default-watermark-src=""https://pic2.zhimg.com/v2-aee8c1b4777ce033191e61158a337571_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-17ab44d23e66acef6345fd8115d61c19_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='246'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""246"" data-default-watermark-src=""https://pic2.zhimg.com/v2-aee8c1b4777ce033191e61158a337571_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-17ab44d23e66acef6345fd8115d61c19_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-17ab44d23e66acef6345fd8115d61c19_b.jpg""></figure><blockquote> 片名：冰冻星球<br>豆瓣评分：9.8</blockquote><p>英国BBC电视台耗时5年制作，这部耗资巨大的纪录片用镜头真实的展现了正在逐渐溶解的地球两极，以及生活在这里的各种生物，片中种种景象让人叹为观止。</p><p>纪录片的解说员大卫·艾登堡爵士称，这可能是人类在地球气 候产生剧烈变化前欣赏到这一景象的最后的机会了。现年85岁的艾登堡爵士说，“这部纪录片捕捉了此前从未记录下来的很多行为和现象。随着时间的流逝，这些影像将会变得越来越珍贵，因为这很可能是我们最后的机会去记录下这些珍贵的场景。虽然在我们到达前的数百年甚至几千年前，地球两极的景象非常壮观，但是最近一个世纪以来，很多变化已经超过了人们的认识。” </p><p><b>NO.  02</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-91ea6be3a146586a0ec12236483b8989_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""246"" data-default-watermark-src=""https://pic1.zhimg.com/v2-d592357ba0a434a34721a46f3034fb1f_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic3.zhimg.com/v2-91ea6be3a146586a0ec12236483b8989_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='246'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""246"" data-default-watermark-src=""https://pic1.zhimg.com/v2-d592357ba0a434a34721a46f3034fb1f_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic3.zhimg.com/v2-91ea6be3a146586a0ec12236483b8989_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-91ea6be3a146586a0ec12236483b8989_b.jpg""></figure><blockquote> 片名：BBC：非洲<br>豆瓣评分：9.8</blockquote><p>《Africa（非洲）》是一部大型原生态纪录片，镜头将跟随主持人大卫·爱丁·保罗夫一起穿越神奇的非洲大陆，探索那些从未被发现、被记录的生物物种和壮观的非洲奇迹。完美的剧情方式拍摄出一部纪录片，绝美的画面和完美的配乐及那略带沧桑感的解说，光是片头就强烈感受到了那震撼心灵的自然之美。横亘在赤道两旁，作为世界第二大的非洲大陆拥有着富饶的野生动物和自然风光。庞大凶猛的猎食动物咆哮着驰骋在众多的草食动物群中；而猩猩、猴子和蛇却占据着浓密幽暗的树林。</p><p><b>NO.  03</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-3f74212edac65d8850cd9fb828b90f63_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""297"" data-default-watermark-src=""https://pic1.zhimg.com/v2-c7d2ed6e5c6ee72c4d846340f0293f40_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic4.zhimg.com/v2-3f74212edac65d8850cd9fb828b90f63_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='297'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""297"" data-default-watermark-src=""https://pic1.zhimg.com/v2-c7d2ed6e5c6ee72c4d846340f0293f40_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic4.zhimg.com/v2-3f74212edac65d8850cd9fb828b90f63_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-3f74212edac65d8850cd9fb828b90f63_b.jpg""></figure><blockquote>片名：地球脉动<br>豆瓣评分：9.7</blockquote><p>从南极到北极，从赤道到寒带，从非洲草原到热带雨林，再从荒凉峰顶到深邃大海，难以数计的生物以极其绝美的身姿呈现在世人面前。我们看到了洪水的涨落及其周边赖以生存的动物们的生存状态，看到了罕见的雪豹在漫天大雪中猎食的珍贵画面；看到了冰原上企鹅、北极熊、海豹等生物相互依存的严苛情景，也见识了生活在大洋深处火山口高温环境下的惊奇生物。当然还有地球各地的壮观美景与奇特地貌，无私地将其最为光艳的一面展现出来。</p><p><b>NO.  04</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-4b8ba9bb26231d5f419df10d70bb55a0_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""259"" data-default-watermark-src=""https://pic4.zhimg.com/v2-405a5702e23bc41bc8a471bbbfeef513_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-4b8ba9bb26231d5f419df10d70bb55a0_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='259'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""259"" data-default-watermark-src=""https://pic4.zhimg.com/v2-405a5702e23bc41bc8a471bbbfeef513_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-4b8ba9bb26231d5f419df10d70bb55a0_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-4b8ba9bb26231d5f419df10d70bb55a0_b.jpg""></figure><blockquote>片名：南太平洋<br>豆瓣评分：9.7</blockquote><p>BBC摄制队于不同岛屿发现了南太平洋一系列令人赞叹的自然景观，包括难得一见的海底火山爆发、壮丽的宝石珊瑚礁、虎鲨猎获信天翁的一刻以及能撕开椰子的陆上巨蟹……在一些被隔离的孤岛上，更发现了特别进化的食肉毛虫、有耐寒能力的吸血虫、交配时会发出像牛蛙一样叫声的怪叫鹦鹉、及尾巴像猴子尾的小蜥蜴。</p><p>南太平洋的土著是地球上最僻远的人类社会，他们会为大家细说祖先们是如何由千里多外迁徙到岛上定居。时至今日，当地居民仍依赖庄稼和捕猎为生，他们拥有独特生存技巧及保留了一些奇异的风俗仪式。</p><p><b>NO.  05</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-46801098ecd49b011615758c7be7da6b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""251"" data-default-watermark-src=""https://pic4.zhimg.com/v2-37ead8685a745738fd103372ac9ac896_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-46801098ecd49b011615758c7be7da6b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='251'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""251"" data-default-watermark-src=""https://pic4.zhimg.com/v2-37ead8685a745738fd103372ac9ac896_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-46801098ecd49b011615758c7be7da6b_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-46801098ecd49b011615758c7be7da6b_b.jpg""></figure><blockquote>片名：人类星球<br>豆瓣评分：9.7</blockquote><p>BBC8集大型电视系列片 - Human Planet （人类星球），探讨人与自然的关系。8集节目分别探讨极地、山区、海洋、丛林、草原、河流、沙漠和城市的人类活动。世界一流的自然与人类专家以及摄影师，从空中、陆地和水下抓拍珍贵镜头。BBC摄制组前往世界80个地方，抓拍了从未在电视屏幕上出现过的罕见精彩的人类活动。</p><p><b>NO. 06</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-faac44ff0b2e16a2f3c7be1babe3324d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""269"" data-default-watermark-src=""https://pic4.zhimg.com/v2-27c8c1c4bed979b41449fec5774cac7c_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-faac44ff0b2e16a2f3c7be1babe3324d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='269'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""269"" data-default-watermark-src=""https://pic4.zhimg.com/v2-27c8c1c4bed979b41449fec5774cac7c_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-faac44ff0b2e16a2f3c7be1babe3324d_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-faac44ff0b2e16a2f3c7be1babe3324d_b.jpg""></figure><blockquote>片名：自然界大事件<br>豆瓣评分：9.6</blockquote><p>摄制队的专家们将带领你进入人迹罕至的北冰洋，在冰雪融化后初春，万物复苏呈现生机无限；超过百万牛羚和斑马奔腾于一望无际的大草原；素有海洋“威武之师”之称的海豚、鲨鱼、鲸鱼、海豹和鲣鸟，在深海中追捕猎杀数十亿沙丁鱼，在生生不息的大自然中上演弱肉强食的戏份；在奥卡万戈爆发的大洪水意外将4000平方英里的干旱平原，顷刻变成一个美丽的湿地。 </p><p>随着环境恶化全球变暖，极地冰川正“被”加速消亡，北极熊们不得不冒险游得更远去捕食海豹，当镜头聚焦北极熊一家孤独而无助地被困于漂离的冰块上，假如浮冰随着水流漂离陆地，熊就会在挨饿淹死等困境中丧生。北极熊迷失在大海中的画面固然触动人心，但极地生态恶化程度只是冰山一角，需要更多的有心人去关注人类和自然和谐相处问题。</p><p><b>NO.  07</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-77589600b0a6d6b5167ae158eef98d58_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""230"" data-default-watermark-src=""https://pic2.zhimg.com/v2-95052de6bf26aeacd905ad872248a20c_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic4.zhimg.com/v2-77589600b0a6d6b5167ae158eef98d58_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='230'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""230"" data-default-watermark-src=""https://pic2.zhimg.com/v2-95052de6bf26aeacd905ad872248a20c_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic4.zhimg.com/v2-77589600b0a6d6b5167ae158eef98d58_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-77589600b0a6d6b5167ae158eef98d58_b.jpg""></figure><blockquote>片名：蓝色星球<br>豆瓣评分：9.6</blockquote><p>蓝色星球是历年来首套全面探索海洋世界的自然历史专辑。由BBC自然历史名主持人戴维·艾登堡录精采旁白，史诗般的海洋全纪录，独家拍摄的珍贵画面，让您见识海洋最恐怖与最具魅力的一面,更揭开它最隐藏的秘密。本节目带您进入令人叹为观止的美丽海洋世界,探索新的物种,造访无人所至的生态,并亲眼目睹从未在镜头前出现过的求生实录。</p><p><b>NO.  08</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-2539aa7573d14949aa94f08815cd317b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""250"" data-default-watermark-src=""https://pic4.zhimg.com/v2-7606557ca49b8e399debf16fbb1a0710_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic2.zhimg.com/v2-2539aa7573d14949aa94f08815cd317b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='250'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""250"" data-default-watermark-src=""https://pic4.zhimg.com/v2-7606557ca49b8e399debf16fbb1a0710_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic2.zhimg.com/v2-2539aa7573d14949aa94f08815cd317b_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-2539aa7573d14949aa94f08815cd317b_b.jpg""></figure><blockquote> 片名：地球的力量<br>豆瓣评分：9.4</blockquote><p>地球，这是一个伟大壮丽的世界，充满了大自然的奇迹。在地球形成的45亿年中，是什么力量塑造了地球如今的模样？我们对地球的认识又有多少？本系列节目将追随著名数学家伊安•斯图尔特（Iain Stewart）博士的足迹，以全新的角度揭示地球最基本，但却鲜为人知的一面，我们将了解到形成地球的诸多因素：火山、大气层、冰、海洋等等。地球的诞生和生命的存在其实是建立在无数的巧合之上，地球是上天赐予的瑰宝。</p><p><b>NO.  09</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-ee6a9a8ef74b59a12158f8ffab6e2fde_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""264"" data-default-watermark-src=""https://pic2.zhimg.com/v2-6ef0a0e0cbca0b8186854761850d5cc2_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic3.zhimg.com/v2-ee6a9a8ef74b59a12158f8ffab6e2fde_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='264'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""264"" data-default-watermark-src=""https://pic2.zhimg.com/v2-6ef0a0e0cbca0b8186854761850d5cc2_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic3.zhimg.com/v2-ee6a9a8ef74b59a12158f8ffab6e2fde_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-ee6a9a8ef74b59a12158f8ffab6e2fde_b.jpg""></figure><blockquote>片名：美丽中国<br>豆瓣评分：9.3</blockquote><p>从灯火通明的大都市，到人烟稀少的深山老林；从广阔无垠的大草原，到人迹罕至的沙漠戈壁；从长年积雪的高海拨山区到一望无际的平原；从浩瀚大地到碧海蓝天……中国，这片古老而又神奇的土地，囊括了多种特质的地貌，哺育着各族儿女，也孕育着各类珍奇野兽，奇花异草。这次，通过镜头，我们去探访散落在这片土地上的美丽奇景。沿着长江流域，去看喀斯特地貌造就的奇石怪林；去云南的热带雨林，寻找亚洲野象、滇金丝猴；来到青藏高原，奔赴内蒙草地，攀爬山脉，眺望西部边陲。这里，是你我熟悉又陌生的美丽中国。</p><p><b>NO.  10</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-666e533baf9ecd9c293875e8e2bcb144_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""239"" data-default-watermark-src=""https://pic1.zhimg.com/v2-c7c478adbb60d8053e7bf4820426acb6_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-666e533baf9ecd9c293875e8e2bcb144_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='239'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""239"" data-default-watermark-src=""https://pic1.zhimg.com/v2-c7c478adbb60d8053e7bf4820426acb6_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-666e533baf9ecd9c293875e8e2bcb144_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-666e533baf9ecd9c293875e8e2bcb144_b.jpg""></figure><blockquote>片名：地球的故事<br>豆瓣评分：9.2</blockquote><p>本节目筹备3年，花费3百万英镑，走遍世界各地，将这个人类居住的行星背后的秘密带到眼前。从活跃的火山口，到无底的深渊。即使是摄影机也未能到的时间与空间，透过最尖端的动画科技，为您详述。将揭露转动不停的地球令人惊讶的变化。</p><p>在过去的四亿年间，生命已由简单的单细胞进化成今日复杂而种类繁多的动植物。当科学家对地球的历史有进一步的了解时，它们发现改变地球的力量，对生物的进化过程有深远的影响。地版的移动，不单重组了陆地，亦令地貌不停的改变，使新生命进化成新型态。</p><p><b>NO.  11</b> </p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-34330a133082507a19f04bfbd235b2ba_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""243"" data-default-watermark-src=""https://pic3.zhimg.com/v2-1231124bb685d09112b0801e2d6bef6b_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic4.zhimg.com/v2-34330a133082507a19f04bfbd235b2ba_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='243'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""243"" data-default-watermark-src=""https://pic3.zhimg.com/v2-1231124bb685d09112b0801e2d6bef6b_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic4.zhimg.com/v2-34330a133082507a19f04bfbd235b2ba_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-34330a133082507a19f04bfbd235b2ba_b.jpg""></figure><blockquote>片名：野性南美洲<br>豆瓣评分：9.1</blockquote><p>一连六集，探索南美洲大陆上繁杂独特、惊险刺激的野生生态。从赤道几乎伸展至南极，广袤的南美大陆地貌悬殊，既有热带海洋，也有冰冠雪山。地球上最大的河系、最长的山系、最大的雨林和最乾的沙漠，都在南美洲。 <br>摄影队运用红外线夜视摄影机和最先进的摄影技术，把鲜为人知的动物呈现在我们眼前。而空中摄影师则飞跃大陆，以全新角度记录南美洲那变化多端的地貌。 这六集特辑带我们深入亚马逊盆地，登上安第斯山脉的雪峰，横穿大草原，穿梭生气勃勃的雨林，最后饱览南美洲那壮观的海岸线——这旅程终身难忘。</p><h2><b>三、10部数学纪录片，领略抽象数学之美，探寻万物的本质规律</b></h2><p><b>NO.  01</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-74181231c652aa06aef4ed6366ee91a8_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""231"" data-default-watermark-src=""https://pic3.zhimg.com/v2-99992ccee5e269c09075fa4a31cbf487_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-74181231c652aa06aef4ed6366ee91a8_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='231'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""231"" data-default-watermark-src=""https://pic3.zhimg.com/v2-99992ccee5e269c09075fa4a31cbf487_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-74181231c652aa06aef4ed6366ee91a8_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-74181231c652aa06aef4ed6366ee91a8_b.jpg""></figure><blockquote>片名：维度：数学漫步<br>豆瓣评分：9.3</blockquote><p>这是一部两小时长的CG科普电影，讲述了许多深奥的数学知识。最引人注目的话题，自然是如四维空间了。作为一个生活在三维世界的人，我们如何通过影响去理解四维空间呢？很多观众纷纷表示看不懂！</p><p>当然了，尽管是科学家，也未必可以完全解释清楚四维理论。四维理论的初衷就是为了理解宇宙世界的物质规律，若放到微观世界中，可能想破脑袋也无法理解。但不管怎么说，从这些大开脑洞的理论中理解数学，感受规律之美，依然是一次很好的体验。</p><p><b>NO.  02</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-b77f756a5f10434035fd8b5abf98a203_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""243"" data-default-watermark-src=""https://pic4.zhimg.com/v2-1b7b763d7c6263468412519f771feca1_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic3.zhimg.com/v2-b77f756a5f10434035fd8b5abf98a203_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='243'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""243"" data-default-watermark-src=""https://pic4.zhimg.com/v2-1b7b763d7c6263468412519f771feca1_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic3.zhimg.com/v2-b77f756a5f10434035fd8b5abf98a203_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-b77f756a5f10434035fd8b5abf98a203_b.jpg""></figure><blockquote>片名：神秘的混沌理论<br>豆瓣评分：9.1</blockquote><p>混沌理论，一直是困扰人类数千年的一个迷。这种在动态系统中无法用单一的数据关系解释和预测的神秘理论，在科学界只有初步的了解。在本部纪录片中，吉姆·奥卡利里教授将带领我们探索这神秘的混沌理论，试图揭开这层归因于神奇或者上帝力量的神秘面纱。</p><p>吉姆教授带着宇宙是如何由从尘埃中诞生，又是如何孕育出智慧生命的难题，开始了他的探秘旅程。混沌中的数学可以解释宇宙中从无序中产生有序的神奇景象，他揭示出大自然那些令人叹为观止的美丽和结构中隐藏的科学规律，它是物理定律的固有的组成。看过本片之后，你眼中的世界会变得与众不同。</p><p><b>NO.  03</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-45708e01b8689998b8fbea15680d415d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""258"" data-default-watermark-src=""https://pic1.zhimg.com/v2-9a6bfe5605eacc6856f3ad8a902248d8_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic4.zhimg.com/v2-45708e01b8689998b8fbea15680d415d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='258'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""258"" data-default-watermark-src=""https://pic1.zhimg.com/v2-9a6bfe5605eacc6856f3ad8a902248d8_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic4.zhimg.com/v2-45708e01b8689998b8fbea15680d415d_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-45708e01b8689998b8fbea15680d415d_b.jpg""></figure><blockquote>片名：数学的故事<br>豆瓣评分：9.0</blockquote><p>一提到数学，很多大人都头疼，但是这又是一门基础学科，那么如何提高孩子对于数学的兴趣呢？那么最好的方式就是回首看看世界上最聪明的大脑为何会对数学上瘾。《数学的故事》一共4集，是BBC的科学史纪录片。孩子可以通过故事了解带数学的进化史，从伟人的角度去看待问题、解决问题，从中真真切切地感觉到数学这门学科的魅力，激发数学的学习兴趣。本片带你走访数学家的故乡，真实地呈现牛顿、莱布尼兹、高斯等数学家探索著名理论的历程，就连所用的公式、解题时列出的方程都一一放出来。前两集讲述数学的起源，主要是各文明古国的先辈们在生产生活中的发明和创造。第三集主要介绍自希腊帝国灭亡之后、工业革命之前的数学世界，第四集是最近300年的演变历史。</p><p><b>NO.  04</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-b64834e32efeb0cb253e455f5f4d4c11_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""231"" data-default-watermark-src=""https://pic1.zhimg.com/v2-a3107dfedc4a1bda39b7f265accba692_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic3.zhimg.com/v2-b64834e32efeb0cb253e455f5f4d4c11_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='231'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""231"" data-default-watermark-src=""https://pic1.zhimg.com/v2-a3107dfedc4a1bda39b7f265accba692_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic3.zhimg.com/v2-b64834e32efeb0cb253e455f5f4d4c11_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-b64834e32efeb0cb253e455f5f4d4c11_b.jpg""></figure><blockquote>片名：约翰·纳什：伟大的疯狂<br>豆瓣评分：8.8</blockquote><p>大部分人都是通过电影《美丽心灵》了解诺贝尔经济学奖获得者约翰·纳什，而这部仅仅在1年之后拍摄的纪录片却不为人所知。事实上，比起《美丽心灵》，这部片子更值得一看，因为它更完整地记录了纳什的生平，并且对理论发现的全过程进行了更深入的探索。</p><p>在上世纪70年代，纳什的博弈论被运用到了美国的经济建设和国际贸易等实际领域，产生了巨大的影响力。等到纳什1994年荣获诺贝尔奖时，他已经从精神分裂走到康复，并重新获得了人们的接纳。导演在最后，还说过这样一席让人深思的话：“我们要去欣赏一个人在某些方面的天赋，即使他有些古怪，看问题的角度与众不同，那些人通常都是有真正非凡的洞察力。”</p><p><b>NO. 05</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-cf6ec7e64c60847b9bb8ef4551ae3cd9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""233"" data-default-watermark-src=""https://pic4.zhimg.com/v2-9958896b0a43787b12e67de817ab1152_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-cf6ec7e64c60847b9bb8ef4551ae3cd9_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='233'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""233"" data-default-watermark-src=""https://pic4.zhimg.com/v2-9958896b0a43787b12e67de817ab1152_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-cf6ec7e64c60847b9bb8ef4551ae3cd9_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-cf6ec7e64c60847b9bb8ef4551ae3cd9_b.jpg""></figure><blockquote>片名：寻找隐秘的维度<br>豆瓣评分：8.8</blockquote><p>大家都会被科幻电影的特效吸引住，你知道背后居然隐藏着一个高深莫测的数学原理吗？而且，这个原理还被运用到股票市场、心脏病的观察中……</p><p>本片为你揭晓，这些看似毫无关联的东西之所以能联结在一起，全靠一位特立独行的数学家Beroit Mandel——他发现了“分形（fractal）”理论，彻底改变了人们的思维。</p><p>这是Beroit自己创造的一个新名词，用于描述那些看上去参差不齐、残缺不全的图形，他说，你可以自己创造一个分形。理论出来之后，帮助了很多人解决了绘制和设计的难题，比如《星际迷航》就运用了这种想法，实现了特效的绘制。“分形”虽是人造的概念，但它遍布我们的生活。从细胞组成到大树的分枝，再到巨大无比的山峰，无处不在。分形连接了一个革命性的新的数学分支，改变了我们看世界的角度。数学家们开发这些奇怪的不规则碎片，也许是单纯的好奇心驱动，但却影响到我们对每一个分科的理解，甚至宇宙的命运。</p><p><b>NO.  06</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-d973fff744fa591536411fffd661903b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""226"" data-default-watermark-src=""https://pic4.zhimg.com/v2-57e4d43729181196117df2560a5146a7_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic3.zhimg.com/v2-d973fff744fa591536411fffd661903b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='226'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""226"" data-default-watermark-src=""https://pic4.zhimg.com/v2-57e4d43729181196117df2560a5146a7_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic3.zhimg.com/v2-d973fff744fa591536411fffd661903b_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-d973fff744fa591536411fffd661903b_b.jpg""></figure><blockquote>片名：密码<br>豆瓣评分：8.6</blockquote><p>数学规律、几何定律，影响了生物的繁衍和星体运行，支撑着教堂的穹顶。这是一个神奇而迷人的世界，它与我们真实生活的世界大不相同又息息相关。我们赖以生存的世界，是数学的世界。这部同样来自BBC的纪录片分别从“数字、形状、预测”三个方向探寻隐藏在宗教、建筑、艺术、生物等大自然和人类生活中的终极密码——数学。 </p><p><b>NO.  07</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-68846895271df2de633f241bdb01acab_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""246"" data-default-watermark-src=""https://pic3.zhimg.com/v2-0807c235ad268359427c2aeea1cb9e49_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-68846895271df2de633f241bdb01acab_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='246'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""246"" data-default-watermark-src=""https://pic3.zhimg.com/v2-0807c235ad268359427c2aeea1cb9e49_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-68846895271df2de633f241bdb01acab_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-68846895271df2de633f241bdb01acab_b.jpg""></figure><blockquote>片名：统计的乐趣<br>豆瓣评分：8.5</blockquote><p>看到“统计学”，你首先想到什么？在本片中，明星教授Hans Rosling将用新奇的方式、先进的技术和幽默的语言，给我们讲述很多奇奇怪怪的统计案例。比如说，平均数是统计中一个很重要的概念，但是光看它，却会得出一个荒谬结论：把马云和6个穷人放在一起，平均每个人都坐拥好几个亿的财富。这些例子，都在告诉我们——统计学不是简单地处理数字，而是用相关性揭示一些人们忽略的原理。单凭一个指标，根本无法洞察事物的本质。因此，数据再重要，也无法代替人们的思考。但是，掌握分析数据的能力，能让我们学会跳出直觉，更理性、严谨地思考。</p><p><b>NO.  08</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-f073caaced59944d645756112dcea655_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""226"" data-default-watermark-src=""https://pic2.zhimg.com/v2-036a1e010efb2e1088043a466656ec89_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic4.zhimg.com/v2-f073caaced59944d645756112dcea655_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='226'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""226"" data-default-watermark-src=""https://pic2.zhimg.com/v2-036a1e010efb2e1088043a466656ec89_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic4.zhimg.com/v2-f073caaced59944d645756112dcea655_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-f073caaced59944d645756112dcea655_b.jpg""></figure><blockquote>片名：新星：数学大谜思<br>豆瓣评分：8.3</blockquote><p>只要细心留意大自然，就会发现很多不为人知的“巧合”——把几朵花放在一起，你可能会从中读出13世纪时希腊数学家所发现的斐波那契数列：1，1，2，3，5，8，13……相邻的两个数相加，总能得到下一个数。这个数列，不仅有趣，还时常出现在人体的美学、股票市场甚至是大自然之中：向日葵的种子、松果的底部，都呈现这种规律。</p><p>更让人惊讶的是，动物们的数学能力远远超乎人类的想象。比如，灵长类动物狐猴就特别聪明，在没有任何标示的情况下，它们可以自己选出数值更多的食物，哪怕是面对颜色不同、形状不同的元素，也可以轻易判断出来。</p><p>看似抽象的数学，散布在大自然的角落之中。有很多迷思，甚至至今都无法解释。本片就是从这些极富趣味的细节切入，带观众接触最可爱的数学，如果您家孩子以为数学是门枯燥的学科，提不起兴趣，那更要陪他一起观看此片，来一次别开生面的数学启蒙了。</p><p><b>NO.  09</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-a2f87a99595166aeb35756664638a7a9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""248"" data-default-watermark-src=""https://pic3.zhimg.com/v2-9eed709b42bdc06f11bcb0493e02e2f2_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-a2f87a99595166aeb35756664638a7a9_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='248'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""248"" data-default-watermark-src=""https://pic3.zhimg.com/v2-9eed709b42bdc06f11bcb0493e02e2f2_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-a2f87a99595166aeb35756664638a7a9_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-a2f87a99595166aeb35756664638a7a9_b.jpg""></figure><blockquote>片名：危险的知识<br>豆瓣评分：8.4</blockquote><p>这部纪录片聚焦四位无与伦比的数学家：乔治·康托尔、路德维格・玻尔兹曼、哥德尔和阿兰·图灵，他们的天才光耀千古，他们的智慧深刻地影响了后世。但是，同时代的人却用尖刻、猛烈的态度批评这四位“异见者”，使这四位天才精神错乱，最终都以自杀结束了自己的生命。</p><p>此外，本片还与目前最著名的学者进行了对话，包括Greg Chaitin——纽约IBM TJ华生研究中心的数学家，以及罗杰·彭罗斯等人，他们仍在孜孜不倦地探寻世上到底是否有数学家不能弄明白的事。</p><p><b>NO.  10</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-100c1200998d22344f374462b231b8f9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""225"" data-default-watermark-src=""https://pic3.zhimg.com/v2-43a56ddba0ff1ea72faa8d6eae82fbdd_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-100c1200998d22344f374462b231b8f9_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='225'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""225"" data-default-watermark-src=""https://pic3.zhimg.com/v2-43a56ddba0ff1ea72faa8d6eae82fbdd_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-100c1200998d22344f374462b231b8f9_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-100c1200998d22344f374462b231b8f9_b.jpg""></figure><blockquote>片名：地平线系列：一根绳子有多长？<br>豆瓣评分：8.0</blockquote><p>一根绳子有多长？只要能找到一把尺子，只要有基本的数学常识，任何人都能在十秒之内解决这个问题。那么，为什么BBC要花60分钟来深入探究这个看似浅显的问题？</p><p>原来，在我们看来再平常不过的“测量”问题，居然被数学家看作数学的起源。绳子本身没有数学意义，是“测量”赋予了它们“长度”的概念。在本纪录片中，主持人和一位特约数学家，将以一条再普通不过的绳子为载体，带我们走近国家物理实验所。</p><p>这是一个专注于测量的地方，看看他们如何“花样百出”地诠释一条绳子的长度。纪录片给我们介绍了不少有趣的新知识，比如早期的埃及人喜欢用手肘到中指的长度作为标准，一个绝对精确的校准器则需要用混合金属制造而成…… </p><h2><b>四、8部IT/互联网纪录片，探寻传奇背后的故事，感受剧变的时代风潮</b></h2><p><b>NO.  01</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-a69213d4675951e9d0853df0d7ed32f9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""244"" data-default-watermark-src=""https://pic4.zhimg.com/v2-f3615d6e59c223029e4e8f782c56de37_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic3.zhimg.com/v2-a69213d4675951e9d0853df0d7ed32f9_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='244'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""244"" data-default-watermark-src=""https://pic4.zhimg.com/v2-f3615d6e59c223029e4e8f782c56de37_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic3.zhimg.com/v2-a69213d4675951e9d0853df0d7ed32f9_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-a69213d4675951e9d0853df0d7ed32f9_b.jpg""></figure><blockquote> 片名：互联网之子<br>豆瓣评分：9.1</blockquote><p>本片《互联网之子》讲的是编程天才和信息活动家亚伦·斯沃茨的故事。从参与基础互联网协议RSS到联合创办Reddit，斯沃茨的足迹遍及整个互联网。但斯沃茨在社会公正和政治组织方面的开创性工作，以及对信息存取的雄心壮志，使他陷入了一场两年之久的法律噩梦。 亚伦的故事也触动了对他如雷贯耳的网络社区之外的人们。</p><p><b>NO.  02</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-0f956e7f37b2201c83fd9d75ded373f3_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""242"" data-default-watermark-src=""https://pic3.zhimg.com/v2-7cae08df21c05c45165107e46caa1976_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic4.zhimg.com/v2-0f956e7f37b2201c83fd9d75ded373f3_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='242'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""242"" data-default-watermark-src=""https://pic3.zhimg.com/v2-7cae08df21c05c45165107e46caa1976_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic4.zhimg.com/v2-0f956e7f37b2201c83fd9d75ded373f3_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-0f956e7f37b2201c83fd9d75ded373f3_b.jpg""></figure><blockquote>片名：硅谷<br>豆瓣评分：8.7</blockquote><p>硅谷讲述了开创性科学家的故事，他们把圣克拉拉县农村改造成了我们现在称为硅谷的技术创新中心。1957年，就在史蒂夫·乔布斯(Steve Jobs)发明苹果(Apple)或马克·扎克伯格(Mark Zuckerberg)创建Facebook的几十年前，八名杰出的年轻人从肖克利半导体公司叛逃，开始他们自己的晶体管生意。他们的领导人是29岁的Robert Noyce，一位头脑聪明、天生的推销员和蔼可亲的物理学家，他们共同发明的微芯片是当今几乎所有现代电子产品的重要组成部分，包括计算机、汽车、手机和家用电器。</p><p><b>NO.  03</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-c44ed52aa8c964767039374f9fbd46e4_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""233"" data-default-watermark-src=""https://pic2.zhimg.com/v2-b1b9189767ce35da3090c2ed20fa3029_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-c44ed52aa8c964767039374f9fbd46e4_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='233'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""233"" data-default-watermark-src=""https://pic2.zhimg.com/v2-b1b9189767ce35da3090c2ed20fa3029_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-c44ed52aa8c964767039374f9fbd46e4_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-c44ed52aa8c964767039374f9fbd46e4_b.jpg""></figure><blockquote>片名：代码奔腾<br>豆瓣评分：8.7</blockquote><p>这是一部讲述Netscape和Mozilla的故事的纪录片。Netscape是一家伟大的公司，它发明了img标签、cookie、ssl安全协议，当然还有如今HTML5时代的明星语言Javascript！但是由于众所周知的原因，它失败了。Netscape最终将浏览器代码开源，这个新项目就是Mozilla！摄制团队横跨了期间重要的几个时间点，整整跟踪了程序员们一年的时间，最终制成这部纪录片。独立制片人从1998年3月到1999年4月跟随Mozilla的团队，他们曾经曾开发了网景浏览器的源代码，并走向世界，如今在尽最后的努力来挽救公司。</p><p>其结果是计算机历史上一个了不起的瞬间，拍摄做这工作的，是第一个内部测试版的人，此刻杰米Zawinski撰写上传的第一个建立公开的应用程序，通过集体会议宣布AOL的收购。它开启了了全国性的PBS在2000年3月，也是互联网泡沫崩溃的开始。</p><p><b>NO.  04</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-19c4ee6566d24e89fed48de639e02df1_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""222"" data-default-watermark-src=""https://pic2.zhimg.com/v2-d71f0a214441d7226c0635800b90612b_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-19c4ee6566d24e89fed48de639e02df1_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='222'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""222"" data-default-watermark-src=""https://pic2.zhimg.com/v2-d71f0a214441d7226c0635800b90612b_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-19c4ee6566d24e89fed48de639e02df1_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-19c4ee6566d24e89fed48de639e02df1_b.jpg""></figure><blockquote>片名：操作系统革命<br>豆瓣评分：8.6</blockquote><p>曾经微软作为互联网世界的巨无霸，垄断了整个互联网操作系统，在这种情况下互联网没有真正的自由。不少先锋人物站出来反抗微软帝国，并努力建立一种新的操作系统——没有人为的限制，任何人都可以自由地使用。为了记录这些人的艰苦历程，J.T.S. Moore拍摄了全新的记录片――REVOLUTON OS，向公众介绍这些建立Linux操作系统，奋起反抗垄断的斗士的人生经历。现在微软已经明显感到了来自Linux的压力。</p><p>微软的首席执行官去年6月公开表示：""Linux 是一种癌症！""。但这丝毫不能影响Linux发展的步伐。REVOLUTION OS（操作系统革命）这部片子记录了Linux的创建人Linus Torvalds以及Richard Stallman, Bruce Perens, Eric Raymond, Brian Behlendorf, Michael Tiemann, Larry Augustin, Frank Hecker, Rob Malda等人的生活经历或者采访记录。</p><p><b>NO.  05</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-562b693dfcc8458687f84a085ed7232e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""259"" data-default-watermark-src=""https://pic4.zhimg.com/v2-95d8e8e3d0b4dae8afe2dd79448f4c2d_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-562b693dfcc8458687f84a085ed7232e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='259'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""259"" data-default-watermark-src=""https://pic4.zhimg.com/v2-95d8e8e3d0b4dae8afe2dd79448f4c2d_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-562b693dfcc8458687f84a085ed7232e_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-562b693dfcc8458687f84a085ed7232e_b.jpg""></figure><blockquote>片名：维基解密的抗争<br>豆瓣评分：8.5</blockquote><p>这是一部关于维基解密及其幕后人物的第一部深度纪录片！瑞典电视台一直在关注神秘的媒体网络维基解密及其神秘的主编朱利安·阿桑奇。记者Jesper Huor和Bosse Lindquist访问了维基解密运营的关键国家，采访了顶层成员，如阿桑奇、新发言人克里斯汀·赫拉芬松以及像丹尼尔·多姆施伊特-伯格这样的人，他们现在正在创建自己的版本<a href=""http://link.zhihu.com/?target=http%3A//-OpenLeaks.org"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">-OpenLeaks.org</span><span class=""invisible""></span></a>！秘密组织的去向是什么？比以往更强大，还是被美国打破了？谁是阿桑奇：自由的捍卫者，间谍还是强奸犯？他的目标是什么？互联网的后果是什么？</p><p><b>NO.  06</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-0b4306213ba9a8002b1e7c341fc59be3_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""213"" data-default-watermark-src=""https://pic2.zhimg.com/v2-bd517c28bf8dc2304929267537e863b5_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic2.zhimg.com/v2-0b4306213ba9a8002b1e7c341fc59be3_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='213'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""213"" data-default-watermark-src=""https://pic2.zhimg.com/v2-bd517c28bf8dc2304929267537e863b5_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic2.zhimg.com/v2-0b4306213ba9a8002b1e7c341fc59be3_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-0b4306213ba9a8002b1e7c341fc59be3_b.jpg""></figure><blockquote>片名：现代生活的秘密规则<br>豆瓣评分：8.4</blockquote><p>我们没有注意到，现代生活已经被算法占据了。从互联网上的搜索引擎到卫星导航和信用卡数据安全，算法都隐藏其中。它们甚至可以帮助我们周游世界，找到爱，拯救生命。数学家马库斯·杜斯托伊教授揭开了隐藏的算法世界的神秘面纱。通过向我们展示一些对我们的生活最重要的算法，他揭示了这两千年历史的问题解决者是从哪里来的。他们是如何工作的，他们取得了什么成就，他们现在是多么的先进，以至于他们甚至可以自己编程。</p><p><b>NO.  07</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-a764c1ca2323182eb2dcc8f883ef0c5b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""214"" data-default-watermark-src=""https://pic4.zhimg.com/v2-d77d7e1842849dd9796b4377f9358f62_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic4.zhimg.com/v2-a764c1ca2323182eb2dcc8f883ef0c5b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='214'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""214"" data-default-watermark-src=""https://pic4.zhimg.com/v2-d77d7e1842849dd9796b4377f9358f62_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic4.zhimg.com/v2-a764c1ca2323182eb2dcc8f883ef0c5b_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-a764c1ca2323182eb2dcc8f883ef0c5b_b.jpg""></figure><blockquote>片名：互联网时代<br>豆瓣评分：8.3</blockquote><p>这是中央电视台继《华尔街》、《金砖之国》、《大国重器》等一系列大型制作之后的再度重磅出击。全片以互联网对人类社会的改变为基点，从历史出发，以国际化视野和面对未来的前瞻思考，深入探寻互联网时代的本质，思考这场变革对经济、政治、社会、人性等各方面的深远影响。该作品旨在引导全社会更准确、全面地认识和理解互联网，更深刻地思考互联网，有准备地迎接一个新时代的到来。</p><p><b>NO.  08</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-639ea475b2af5d1605d973e2b716e4ca_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""234"" data-default-watermark-src=""https://pic3.zhimg.com/v2-4a182acb756dadd51e51b49eef20d233_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic4.zhimg.com/v2-639ea475b2af5d1605d973e2b716e4ca_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='234'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""234"" data-default-watermark-src=""https://pic3.zhimg.com/v2-4a182acb756dadd51e51b49eef20d233_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic4.zhimg.com/v2-639ea475b2af5d1605d973e2b716e4ca_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-639ea475b2af5d1605d973e2b716e4ca_b.jpg""></figure><blockquote>片名：BBC：阿兰·图灵<br>豆瓣评分：8.2</blockquote><p>阿兰·图灵（Alan Turing）这个名字无论是在计算机领域、数学领域、人工智能领域还是哲学、逻辑学等领域，都可谓“掷地有声”。图灵是计算机逻辑的奠基者，许多人工智能的重要方法也源自这位伟大的科学家。他在24岁时提出了图灵机理论，31岁参与了Colossus（二战时，英国破解德国通讯密码的计算机）的研制，33岁时构思了仿真系统，35岁提出自动程序设计概念，38岁设计了“图灵测试”，在后来还创造了一门新学科——非线性力学。</p><p>虽然图灵去世时只有42岁，但在其短暂而离奇的生涯中的那些科技成就，已让后人享用不尽。人们仰望着这位伟大的英国科学家，把“计算机之父”、“人工智能之父”、“破译之父”等等头衔都加冕在了他身上，甚至认为，他在技术上的贡献及对未来世界的影响几乎可与牛顿、爱因斯坦等巨人比肩。 </p><h2><b>五、科学界伟大人物纪录片，站在巨人的肩膀上，看得更远</b></h2><p><b>NO.01</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-e181a73821c1baf0d721e795f02874fe_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""249"" data-default-watermark-src=""https://pic4.zhimg.com/v2-f615c5515cf3e77c606c5f5523b7b3af_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-e181a73821c1baf0d721e795f02874fe_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='249'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""249"" data-default-watermark-src=""https://pic4.zhimg.com/v2-f615c5515cf3e77c606c5f5523b7b3af_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-e181a73821c1baf0d721e795f02874fe_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-e181a73821c1baf0d721e795f02874fe_b.jpg""></figure><blockquote>片名：梁思成与林徽因<br>豆瓣评分：9.4</blockquote><p>《梁思成·林徽因》讲述了“大时代背景下跌宕起伏的个人命运”：一对著名夫妇的人生轨迹和中国近现代的激荡史密切交织。这个故事所蕴涵的精神气质，是我们追索和展现这一传奇故事的动因。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-b6125cd8fbd47346cec11e5dc705ccbf_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""299"" data-rawheight=""288"" data-default-watermark-src=""https://pic1.zhimg.com/v2-25282be2553ff36a4c928338f682c81e_b.jpg"" class=""content_image"" width=""299""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='299'%20height='288'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""299"" data-rawheight=""288"" data-default-watermark-src=""https://pic1.zhimg.com/v2-25282be2553ff36a4c928338f682c81e_b.jpg"" class=""content_image lazy"" width=""299"" data-actualsrc=""https://pic1.zhimg.com/v2-b6125cd8fbd47346cec11e5dc705ccbf_b.jpg""></figure><p> 梁思成是中国著名的建筑学家和建筑教育家。毕生从事中国古代建筑的研究和建筑教育事业。系统地调查、整理、研究了中国古代建筑的历史和理论，是这一学科的开拓者和奠基者。曾参加人民英雄纪念碑等设计，是新中国首都城市规划工作的推动者，建国以来几项重大设计方案的主持者。是新中国国旗、国徽评选委员会的顾问。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-bbffddf96efd8cb0a9223506796b9255_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""270"" data-rawheight=""383"" data-default-watermark-src=""https://pic1.zhimg.com/v2-04647ca3c608a297de984355b0b5c5f1_b.jpg"" class=""content_image"" width=""270""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='270'%20height='383'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""270"" data-rawheight=""383"" data-default-watermark-src=""https://pic1.zhimg.com/v2-04647ca3c608a297de984355b0b5c5f1_b.jpg"" class=""content_image lazy"" width=""270"" data-actualsrc=""https://pic1.zhimg.com/v2-bbffddf96efd8cb0a9223506796b9255_b.jpg""></figure><p> 林徽因，建筑学家和作家，为中国第一位女性建筑学家，同时也被胡适誉为中国一代才女。三十年代初，与夫婿梁思成用现代科学方法研究中国古代建筑，成为这个学术领域的开拓者，后来在这方面获得了巨大的学术成就，为中国古代建筑研究奠定了坚实的科学基础。她的文学著作包括散文、诗歌、小说、剧本、译文和书信等，其中代表作为《你是人间四月天》，小说《九十九度中》等。1955年4月1日清晨去世，年仅51岁。</p><p>在林徽因的感情世界里有三个男人，一个是梁思成，一个是诗人徐志摩，一个是学界泰斗、为她终身不娶的金岳霖。 </p><p><b>NO.02</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-6fc4e603457b958665576553457408c3_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""236"" data-default-watermark-src=""https://pic3.zhimg.com/v2-9e8d7853085d092c147c05992476aeaf_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic4.zhimg.com/v2-6fc4e603457b958665576553457408c3_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='236'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""236"" data-default-watermark-src=""https://pic3.zhimg.com/v2-9e8d7853085d092c147c05992476aeaf_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic4.zhimg.com/v2-6fc4e603457b958665576553457408c3_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-6fc4e603457b958665576553457408c3_b.jpg""></figure><blockquote>片名：N是一个数：保罗·厄多斯的写真<br>豆瓣评分：9.1</blockquote><p>保罗·厄多斯(1913-1996)是一位匈牙利的数学家。其父母都是匈牙利的高中数学教师。保罗·厄多斯，1983年以色列政府颁给十万美元“沃尔夫奖金”（WolfPrize）就是由他和华裔美籍的陈省身教授平分。厄多斯是当代发表最多数学论文的数学家，也是全世界和各种各样不同国籍的数学家合作发表论文最多的人。他发表了近1000多篇的论文，平均一年要写和回答1500多封有关于数学问题的信。甚至有人说，不跟厄多斯合作过发表论文的数学家，不可以被称为数学家。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-520dab99fa1b5e78240986af4d9ce6c7_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""268"" data-rawheight=""285"" data-default-watermark-src=""https://pic3.zhimg.com/v2-a1f439932fdf9f056438bb51f5dc691a_b.jpg"" class=""content_image"" width=""268""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='268'%20height='285'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""268"" data-rawheight=""285"" data-default-watermark-src=""https://pic3.zhimg.com/v2-a1f439932fdf9f056438bb51f5dc691a_b.jpg"" class=""content_image lazy"" width=""268"" data-actualsrc=""https://pic4.zhimg.com/v2-520dab99fa1b5e78240986af4d9ce6c7_b.jpg""></figure><p> 他可以和任何大学的数学家合作研究，他每到一处演讲就能和该处的一两个数学家合作写论文，据说多数的情形是人们把一些本身长期解决不了的问题和他讨论，他可以很快就给出了问题的解决方法或答案，于是人们赶快把结果写下来，然后发表的时候放上他的名字，厄多斯的新的一篇论文就这样诞生了。</p><p><b>NO.03</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-496f05bb2edf80f9cf522d00c0b6c33b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""228"" data-default-watermark-src=""https://pic2.zhimg.com/v2-ff2763886dfb43be88e45af7d3430591_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic1.zhimg.com/v2-496f05bb2edf80f9cf522d00c0b6c33b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='228'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""228"" data-default-watermark-src=""https://pic2.zhimg.com/v2-ff2763886dfb43be88e45af7d3430591_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic1.zhimg.com/v2-496f05bb2edf80f9cf522d00c0b6c33b_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-496f05bb2edf80f9cf522d00c0b6c33b_b.jpg""></figure><blockquote>片名：特斯拉：闪电的主人<br>豆瓣评分：9.0</blockquote><p>尼古拉斯·特斯拉是塞尔维亚裔美国科学家，电气工程师和发明家，他的研究为现代电气和通信系统奠定了基础。尽管他的成绩卓着，但是世人对他还是知之甚少，他拥有700项专利并且这一切来自于令人钦佩的各方面的造诣，包括交流电系统、无线电、特斯拉感应线圈变压器，无线传输，和荧光灯。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-376ef5f8b8661b075b5cae2b968380c0_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""314"" data-rawheight=""410"" data-default-watermark-src=""https://pic4.zhimg.com/v2-4fbbe59c14bb98d0866fea1d95f55570_b.jpg"" class=""content_image"" width=""314""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='314'%20height='410'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""314"" data-rawheight=""410"" data-default-watermark-src=""https://pic4.zhimg.com/v2-4fbbe59c14bb98d0866fea1d95f55570_b.jpg"" class=""content_image lazy"" width=""314"" data-actualsrc=""https://pic2.zhimg.com/v2-376ef5f8b8661b075b5cae2b968380c0_b.jpg""></figure><p>1856年7月10日，特斯拉在克罗埃西亚出生。他的父亲是一个传统的牧师和作家同时也是一位诗人；他的母亲是一个发明工具的家庭主妇，打蛋器就是她发明的，这些发明的收入用来补充日常的开销。还是一个在校生的时候，特斯拉就能够快速的进行复杂计算，以致于他时常被指责是在作弊。为了进入奥地利格拉茨的技术大学和布拉格大学他受过工程师培训。</p><p>自从1884年他们移民到美国之后，特斯拉和托马斯·爱迪生一起工作。然而，这两位科学家并不太合得来，随后特斯拉很快建立了他自己的实验室。从不同方面看他是一个天才，空想家，哲学家也是一个怪人，特斯拉会时常直觉怀疑一些科学真理随后就会利用科学方法来证明他的假说。</p><p>尽管他对现代技术的贡献如此的大，在科学王国之外他却很少为人知。按照特斯拉的传记作者Margaret Cheney的观点：他是一个落伍的人，有许多的原因导致了这个发明家人生比较暗淡。首先，特斯拉从不用参加任何组织。此外，他的研究太超前，特斯拉的同时代的人通常都无法理解他的工作。然而许多人从未听说过他的主要原因是其他的科学家通常对特斯拉的成就不十分信任。通常会把特斯拉在交流电方面的工作之归功于爱迪生，而也不可避免的把马可尼当成无线电的发明家。事实上，爱迪生从来不从事交流电方面的工作，这是因为它他在直流电方面的工作有竞争。</p><p>此外，虽然马可尼在1904年得到了无线电的专利权，但是这一授权情况多少有些黑暗，这是因为特斯拉在1900年已经获得了无线电的专利权。在1943年，联邦最高法院恢复了特斯拉是无线电的发明人这一身份同时维护他在1900年获得的专利权。不幸地是，法院的修正对于特斯拉来说太晚了。特斯拉于1943年1月7日死于纽约市。在他的葬礼上，三位诺贝尔奖得主为他致词（他自己本身并没有得到诺贝尔奖）到""作为一个世界上杰出的智者，他为未来多种技术的开发铺平了道路。""最后人们用特斯拉名字作为磁感应密度的标准单位以纪念他的成就。</p><p><b>NO.04</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-9ebed987e97d2c1331da9c18b51df99e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""249"" data-default-watermark-src=""https://pic3.zhimg.com/v2-4c60c8cadf68bbbb305eb456715e679b_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""555"" data-original=""https://pic2.zhimg.com/v2-9ebed987e97d2c1331da9c18b51df99e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='555'%20height='249'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""555"" data-rawheight=""249"" data-default-watermark-src=""https://pic3.zhimg.com/v2-4c60c8cadf68bbbb305eb456715e679b_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""555"" data-original=""https://pic2.zhimg.com/v2-9ebed987e97d2c1331da9c18b51df99e_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-9ebed987e97d2c1331da9c18b51df99e_b.jpg""></figure><blockquote>片名：山长水远：陈省身的一生 <br>豆瓣评分：9.0</blockquote><p>“山长水远：陈省身的一生”（2010年）深入介绍这位卓越数学家的生平。他不仅有令人钦佩的数学贡献，他不遑多让的做事方法和远见也促成了中国和西方沟通的桥梁。这部传记纪录片跟随陈省身经历20世纪一幕幕的剧烈变动，刻画出一位毕生奉献给纯粹数学的古典中国哲人。</p><p>陈省身（1911年-2004年）被誉为现代微分几何学之父。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-77a574261e52743676e838d0ccdddc9e_b.jpg"" data-size=""normal"" data-rawwidth=""580"" data-rawheight=""387"" data-default-watermark-src=""https://pic2.zhimg.com/v2-c93fdaca9153a12102f0d03e2b064943_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""580"" data-original=""https://pic2.zhimg.com/v2-77a574261e52743676e838d0ccdddc9e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='580'%20height='387'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""580"" data-rawheight=""387"" data-default-watermark-src=""https://pic2.zhimg.com/v2-c93fdaca9153a12102f0d03e2b064943_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""580"" data-original=""https://pic2.zhimg.com/v2-77a574261e52743676e838d0ccdddc9e_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-77a574261e52743676e838d0ccdddc9e_b.jpg""><figcaption>陈省身与老师，约摄于1930年</figcaption></figure><p> 陈省身启蒙于传统中国教育，1930年代至德国汉堡和法国巴黎进修数学，并以发展当时顶尖的微分几何学家埃利·嘉当（Elie Cartan）的理论崭露头角。</p><p>他最重要的贡献始于1940年代，他简洁漂亮地证明高斯－博内定理，引进示性类，现称为“陈类”。二战及战后期间他在普林斯顿的高等研究所（Institute for Advanced Study ）和中国工作，并培养新一代的中国数学家，直到1949年。同年他回到美国，任教于芝加哥大学。1960年他转任加州大学伯克利分校，并创建几何学中心，之后于1981年成为伯克利“数学科学研究所”（MSRI）的共同创办人。1980年代，由于他响亮的名声，中国邀请他协助重建中国的数学研究。陈省身安排优秀的中国学者至欧美进修，并邀请欧美数学家至中国举办专题研讨会。到了1986年，他在天津的南开大学设立了南开数学研究所，即今日的“陈省身数学研究所”。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-fc40043bbc4a9d0fc5e50c13309223ae_b.jpg"" data-size=""normal"" data-rawwidth=""327"" data-rawheight=""211"" data-default-watermark-src=""https://pic3.zhimg.com/v2-44a2b7d640d070d59d2214253abf16dc_b.jpg"" class=""content_image"" width=""327""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='327'%20height='211'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""327"" data-rawheight=""211"" data-default-watermark-src=""https://pic3.zhimg.com/v2-44a2b7d640d070d59d2214253abf16dc_b.jpg"" class=""content_image lazy"" width=""327"" data-actualsrc=""https://pic2.zhimg.com/v2-fc40043bbc4a9d0fc5e50c13309223ae_b.jpg""><figcaption>陈省身与邓小平</figcaption></figure><p> 在其长远与卓越的职业生涯中，陈省身致力培养回归古典中国哲学的价值观。 他在数学领域的高度成就，赢得国内各界的尊重 。有了他们的大力协助，陈省身得以复兴中国的数学研究，也培养出新一代的优秀中国数学家。数位世界知名人士，例如田刚、丘成桐都视陈省身为导师。到了1980年代，陈省身开始定期造访中国之时，他已经成为名人。每位学子都听过他的大名，只要他踏出南开大学数学研究所，电视台镜头就跟着他，记录他的一举一动。</p><p>1999年陈省身从伯克利搬回中国定居，并于2004年在天津病逝。</p><p>本纪录片透过访问杰出数学家、朋友、子女，追溯陈省身的一生与成就。大部分片段于2010年在伯克利、麻省理工学院、纽约、普林斯顿、北京以及天津拍摄。本片包括档案影片和2000年陈省身于伯克利数学科学研究所的访问片段，这部片探索陈省身成为开创性的数学家和研究机构创立者的成功根源。</p><p>“陈省身他能让人信服、推动事务的不凡影响力从何而来？” 本片提出这个问题。他散发一种特别的威严感。“如果你认识他，就知道你进了核心圈”，伯特伦·科斯坦特这么说。 </p><p>“他散发一种特别的威严感；如果你认识他，就知道你进了核心圈。”－伯特伦·科斯坦特</p><p>“每当我们要向校长提出特殊要求的时候，我们总是带陈省身一起去，而总是奏效。” 加州大学伯克利分校的数学家罗伯·柯比说：“他有一种份量，一种庄严感。不知怎么，他总能让人听他的，照他的意思去做。”</p><p>陈省身的权威地位部分来自他1940年代的早期数学成就。“陈省身在高级研究所完成两项非常重要的成就，”罗伯特·奥瑟曼如此解释。 奥瑟曼与陈省身合作著书，也曾是伯克利数学科学研究所的同事 。“第一项是他证明了高斯－博内定理，从那衍生了‘陈类’，这项几何学、拓扑学和代数几何学中最根本的复示性类。”“无法想像微分几何没有陈类的存在”，与陈省身共同创办数学科学研究所的卡尔文·摩尔补充道。现在陈类已经无所不在。 数学科学研究所所长罗伯特·布莱恩特补充，“人们大量学习陈类，成了我们思考方式的一部分。”</p><p>这些成就替他水涨船高的声望铺路。他代表中国数学家所作的努力，和在天津建立一所新研究所的贡献，都确立他作为中国的一位现代文化典范的名望。</p><p><b>NO.05</b></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-fce6423427909f9da4baac8f1253ca49_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""239"" data-default-watermark-src=""https://pic3.zhimg.com/v2-d5d8da0282b858f218b85c7f87ec4524_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""556"" data-original=""https://pic4.zhimg.com/v2-fce6423427909f9da4baac8f1253ca49_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='556'%20height='239'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""556"" data-rawheight=""239"" data-default-watermark-src=""https://pic3.zhimg.com/v2-d5d8da0282b858f218b85c7f87ec4524_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""556"" data-original=""https://pic4.zhimg.com/v2-fce6423427909f9da4baac8f1253ca49_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-fce6423427909f9da4baac8f1253ca49_b.jpg""></figure><blockquote>片名：改变世界的方程 <br>豆瓣评分：8.6</blockquote><p>每个人都听说过它，但是E = mc2，这个世界上最着名的等式，究竟是什么意思？为什么它会改变世界？NOVA戏剧化了一位名不见经传的年轻专利文员阿尔伯特·爱因斯坦如何提出他在1905年的发现，即物质和能量的领域不可避免地联系在一起。这部片子简直是一部悬疑的史诗，揭示了他在人类历史进程中取得惊人突破的根源。</p><p>这部片子包含丰富的成就和失败的故事，仇恨和口是心非，爱情和竞争，政治和报复：迈克尔·法拉戴，一个身无分文的铁匠的儿子，他嘲笑科学机构；安托万·拉瓦锡，冷静的实验者，成为断头台的受害者；还有Lise Meitner，这位物理学家在她发现原子分裂的道路上经受了纳粹迫害和个人背叛。这部片子展示了爱因斯坦的传奇源自于这些先驱者的伟大贡献，以及他们多年的坚持，独创，牺牲和英勇的斗争。 </p><p><b>（未完待续）</b></p><hr><h2>读完文章后，记得2件小事哦：</h2><p><b>1、点赞：</b>您的点赞可以让优质内容传递给更多人；</p><p><b>2、评论：</b>亮出你的观点，说不定会碰出不同火花；</p><p>感恩，ღ( ´･ᴗ･` )比心。</p><p><br></p><p><b>版权声明：</b></p><p>以上内容部分来自：豆瓣、时光网、百度百科等</p><p>商业转载请联系品牌方获得授权，非商业转载请注明出处。</p>",49037,631,62840,傲梦青少儿编程,https://api.zhihu.com/people/5b0d7d292cd2948a7bf35e965dfd081c,赞同了回答
1573535186503,大家身边极度聪明的人是什么样子？,注意，是“极度”！ 一般形容人聪明的级别是，机灵<聪明<极度聪明。前两种都相对常见，比如能同时思考2-3件事的人不算罕见。但大概因为自己智商圈子的缘故，极度聪明的人却不常见到，或者也不认识。 我非常好奇极度聪明的小伙伴们是什么样子？？？在智商上、情商上等等，有没有特别的具体事情能体现谁极度聪明？ 这个问题问出来，万一极度聪明的人来回答会不会看不懂呢T.T 以及，是不是集中在数学、物理、哲学方向呢,885,32,22718,https://api.zhihu.com/questions/26073846,匿名用户,,1413535985,1573535186,1576970252,<p>我一般认为聪明的人大致有两种：1.）自律，领导力突出，沟通能力强，有规划，反应力快，在人际关系中也较为强势，总体都表现的很突出。2.) 说话稍微比较少，比1中的人少，但是每一次一说话，你能听出来他绝对是思考过很久的，很有建设性，脾气比较温和，但有的时候很倔强。在团队中不会有压倒的气场，但是有一定的压迫性，不是一直都有。交流的时候，表达不一定特别的清晰，但是你会观察到他在一边说话一边想，这种人很多时候是因为思维比他说话的速度要快很多才会导致这样的结果。<br><br>再打个比方吧，比如在一所学校里，1.是那种你会碰到那种学生会主席。他总是策划各种活动，是所有学弟学妹眼中发光的角色。他走到任何地方都可以发光发热，干好自己的事情，搞好人际关系。但这类人的一个缺点就可能是太过于强势的性格会招惹一些不满，有的时候在团队中的话可能不会顾及组员的感受，所以在无意识的时候可能会中伤他人。2.是那种在自己的小圈子做的特别好的人。如果他是一个学生，他会是那种整天在图书馆看自己的喜欢的书的那种人，看的书都不是热门的，也都不是一般人看的。这种人的目的性比1.要小。1是那种永远都有动力，有想要“出人头地”这种远大愿望的人。很自信。2是那种对绞尽脑汁要在一个群体去成功这件事没有什么想法的。因为觉得太累。他的思想适合花在更抽象的地方。1在别人开会时会随时围绕主题提出建议。2在开会的时候可能在想“进化论到底是正确的还是错误的”这种问题，并且在脑子里找证据。然后时不时抓抓关键点发言就行。<br><br>不知道有多少人看过银河英雄传说，里面的两个男主，莱因哈特和杨威利，就是这种性格相反的，但是都贯彻了自己的理想的人。莱因哈特从小便心有抱负，想要推翻帝国的统治，一是为了救出自己的姐姐，二是因为目睹帝国内部腐败，民不聊生。另外一个男主，杨威利，从小和作为商人的父亲游历在各个星系之间，年纪很小却很老成，看透了许多人和事。长大后只想专心研究历史，研究人类社会古往今来的趋势，然后拿退休金2333。莱因哈特永远把目光放在眼前和未来。他强盛的自尊心不会允许自己的失败。他坚信如果要有人赢，那一定是他。对于莱因哈特来说，无能即是一个人的原罪，因为命运不会给人他/她承受不了的使命。明知道自己无能，还不去努力抗争，则是无可救药，不值得怜惜，因为他/她已经失去了最基本的向前的欲望。相反，对于杨威利来说，历史就像是一个舞台，开幕，谢幕，没有谁是真正的主角。纵使一个人有着天大的信念，在无数外因的作用下，也终究有永远不可及之物。空有信念却不去考量自身才能和客观因素，是愚不可及的价值观。杨威利永远都是上帝视角，只要不违背本心，身外之物都与他无关。可以看出来，这两种人的人生观几乎是相反的。但不阻碍他们的成功。</p><p>两句语录贯彻两人一生：</p><p>莱因哈特：“我的征途是星辰大海! ”</p><p>杨威利：“尽了力而还做不好就不要勉强；伸手不能及之处，不管再怎么担心也够不着，不如就委托给想做的人去做，这才是明智之举。”</p><p>一个是当领导的人才，一个是当学者的人才。</p><hr><p>更新一下：</p><p>再说一个历史人物，马可 奥勒留，古罗马皇帝之一。我觉得他就是典型的明明是2的性格和价值观，却坐上了1的位置的人。和杨威利一样，明明比谁都洞晓人心，却比谁都厌恶权利斗争；明明厌恶战争，却一生戎马，屡战屡胜。越是了解政治和人性，才越想避而远之。</p><p>第一种人是领导者，革命家，军事家，谈判家，帝王将相。是我们在历史书中读到的，那些万人敬仰的群星。他们就如同像太阳一样的恒星，几乎所有毗邻的物体都会被其光芒给压制，进而化为灰烬。这股力量是直面而来的，似乎连躲藏的地方都找不到。跟随他或许不用担心迷失信念和方向。第二种人，他们是哲学家，文学家，历史学家，数学家，甚至隐士。他们就像黑洞一样。你不知道他在哪里，什么时候出现，也不知道它的广度和深度。但当你往里面瞧，却觉得深空地不可思议。太阳让你觉得热的想逃，黑洞则让你浑身上下都冷地颤栗。第一种人容易在社会上成功，是通才。但要论对事物的感知能力，学习能力，和理解能力，闭口不谈为人处事的话，我认为第二种人的思想境界是难以匹敌的。</p><p>1和2都推进了人类的发展。不同的是，1通过自己和周遭人和物的关系，来学习，来用自己的观点去影响他们，然后再以此影响千千万万的人。而2类人，影响社会的方式是通过传播他们自己与自己深度交流的产物，而不是和其他人深度沟通的产物。1的影响力是向外的辐射，2的影响力来自于自我意识深处的回声。</p><hr><p>二次更新一下。说些之前没来得及补充的。</p><p>以下的想法我想了也比较久了。如果你觉得自己是1，并且是意志坚定，执行力强的1，ok，恭喜你，你的存在甚至会在某一阶段让2感觉到威胁。如果你是成熟的2，那么也恭喜你，你时不时流露出的真实一面也会让1觉得自己地位不保。但如果你觉得自己是2，但是却认为1的风格才是最理想的，那下面这些就是我想要说的了。</p><p>评论里很多小伙伴都说自己或许和2靠近一些，但想要成为1的念头时不时都会进入脑海。这里就要牵扯到我之前想说但忘了说的一个点，2类人的缺点。</p><p>上面我说了，2类的人为什么思维境界比较高深？因为他们随时都会有一种崇高感。2的崇高感和思维深度使得他们常把自己置身于像宇宙这样的尺度，而不是其他世俗的实体中，进而导致他们在获得无上的崇高感同时，会把自身的存在感缩小和降低。</p><p>换句话说吧，有的时候1不管有没有成功，都不太有b数（通俗一点，无贬义）；而2的弱点呢，就是太有b数。我上面指出的2的例子，像杨威利，是已经“觉醒”了的2，对自身的能力有一个很稳定的认识。这样的人哪怕是在1面前，也不会去贬低自己。但是这其实对于心智不成熟的2来讲，是几乎无法做到的事。当不成熟或没有想通的2看到1的时候，会羡慕与1所获得的广人脉和社会关系。初期的2会羡慕1那种被人包围，处处被朋友环绕的感觉，羡慕那种雷厉风行的处事模式，咄咄逼人的待人风格。</p><p>还有我发现，1类人做事是很有计划的，完全不拖延。为什么？因为他们目标小而具体。他们的思维不是发散的。2类人有很多是有拖延症的，有的时候给人懒懒散散的感觉。这又为什么？因为他们的目标和精神支柱有的时候太过抽象，甚至连他们自己都会怀疑这个目标是否值得夺取。</p><p>但我要说的是，1是容易获得社会性成功的人。这就是他们人格所带来的最大的一个闪光点，因为1的心里所想和他们的行为几乎是一致的，所以更容易被人理解。由此一来，能够看到1的聪明之处的人，包括1类人本身，2类人，和平常人。但是2呢？要知道，2所表达出来的行为和语言是有过滤的。2类人有着很强的过滤和自我处理问题自我反省的能力，所以展现出来的让人觉得聪明的点不易被察觉。所以，能看出2类人聪明的人，几乎只有2类人本身，当然包括小部分的1。当然，最讽刺的是，有一部分的1类人，在不了解2类人的恐怖之处的时候，甚至会贬低2类人。他们会觉得从表面上来看，2类人是一群缺乏干劲，没有梦想的人。</p><p>但要知道，2类人不是没有信念，他们只是不会完全在“信念”这个概念上下全部的赌注。2类人很矛盾，一边否认自己的梦想和信念，一边又把预期定的很高。1把信念瓜分成一小段一小段路，每一段路都动力十足。但是1类的人倾尽全力的方式是比2类人稍微偏向于“生理”上的，比如说用了多少时间，花了多少精力，每天少睡多少觉之类的。当然，他们精神上也付出了，不过和2类人的精神付出还有些不一样。2类人一般是那种，信念不多，可能只有一个，然后可以为此粉身碎骨，两耳不闻窗外事地投入的人。说句实话，最极端的2类人发起疯来连自己的生活起居都不会管的。有时候说不定看起来还没什么自理能力（只是有可能，并不是每个2类人都这样）。但是1类人在努力的同时就把自己方方面面照顾地很好。1类人可以把自己从一个精神世界里抽离，然后再次进入。但2类人全情投入钻研一个东西的时候，就很难再钻出来了，只能越陷越深。陷得越深，就像之前提到的，离日常生活中的小事就越来越远了，这一点其实是很危险的。</p><p>1类人因为接触的圈子和事物比较多，所以他们的许多个人技能都很强大，敢于思考和敢于去探索的领域也比2多。如果可以的话，他们会运用世界上的所有资源，尝试一切并挑战一切。2类人很大程度上只愿意在某一个自己感兴趣并觉得有安全感的领域上钻研，并非要以己之力将人类对这个领域的认知刷新了才甘心。但是说实话，历史上和现代社会中有许多2类人就是至死都没有得以在某个特殊领域得到他/她们期待的成功，所以直接崩溃。</p><p>2类人其实走的很艰难，比1要艰难。每走一步，就是一个对过去自己审视的过程。</p><p>强大的2类人是一边心如止水，一边发狠的。</p><p>外界觉察不到，这种“狠劲”完全存在于他们与自我的对话中。</p><hr><p>我没有料到这个回答能得到这么多赞和感谢，比较出乎意料哈哈哈。</p><p>应该会有第三次更新，但是最近我在忙申请，所以我应该会在下周初更新和回复大家的评论！</p><p>谢谢。</p><hr><p>更新一下，说一些事情。不好意思这次更新不是大家希望看到的关于本答案的扩展。</p><p>答主最近生活和学习上遇到了一些问题。昨天刚刚经历了一次mental breakdown，不论怎么说明父母也不理解，唉。</p><p>所以，等一月份申请季结束材料都上去了估计会好点然后就应该可以开始写了。</p><p>这次答主回国可能会去看一下心理医生有必要的话吃吃药什么的，不然的话感觉自己的心理状态估计坚持不了很久了。</p><p>总之，谢谢大家阅读，喜欢，点赞，感谢，和收藏这个答案。</p>,5402,424,1653,Yuner Jiang,https://api.zhihu.com/people/dcc3ac19ff610e59d7b4c93ba4e7c8b3,赞同了回答
1573471435339,如何训练思维的深度和缜密度？,,185,9,37467,https://api.zhihu.com/questions/19553569,成远,https://api.zhihu.com/people/a928a43908c9f02d2da1f7df66de4d16,1295276200,1573471435,1372607676,"想回答这个问题很久了，谈谈个人心得吧。要先声明一句废话，这篇回答是我朝这个方向努力的时候的一些体会，显然并不代表本人的水平已经达到了这个境界。<br><br><b>首先</b>要明白的是，除了数学家生活的完美世界之外，<b>所有问题都会有一定程度的抽象和假设</b>——你要做的是<b>找到这些抽象和实际问题的边界</b>。如果你仔细考察抽象的边界，很多不缜密的地方就会暴露出来。比如说这样一个常识：冰的熔点是零摄氏度，实际上包含了一大堆抽象和假设。如果你就这些领域深入思考下去，任何一个方向都会很有深度<br><ul><li>如果不是标准大气压，冰的熔点还是零摄氏度吗？真空中冰的熔点是多少呢？</li><li>如果不是纯水，各种溶液的冰点是什么样子呢？</li><li>如果是重水，冰的熔点又是多少？为什么同位素氘会影响冰的熔点？</li><li>等等……</li></ul>实际上，仅仅举例第一个方向Wiki上有现成的答案。比如气压非常低的时候，水是不存在的，温度升高冰会直接升华为零度的水蒸气！而冰会有16种不同的状态！可想而知，每一种新状态的发现都会有大量的实验和漂亮的论文出现，这就是深度和缜密。<figure><noscript><img src=""https://pic3.zhimg.com/9f160a5292217bf26db23610fd37272f_b.jpg"" data-rawwidth=""725"" data-rawheight=""612"" class=""origin_image zh-lightbox-thumb"" width=""725"" data-original=""https://pic3.zhimg.com/9f160a5292217bf26db23610fd37272f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='725'%20height='612'&gt;&lt;/svg&gt;"" data-rawwidth=""725"" data-rawheight=""612"" class=""origin_image zh-lightbox-thumb lazy"" width=""725"" data-original=""https://pic3.zhimg.com/9f160a5292217bf26db23610fd37272f_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/9f160a5292217bf26db23610fd37272f_b.jpg""></figure><br><br><br><br><br><b>其次，不要怕提问和讨论</b>，伟大的剑客绝不是武侠小说中那样闭门修炼若干年一出关就可以一鸣惊人的，他们一定在很多地方跌倒爬起的过程中领悟了剑道的精髓。我很幸运从大学开始一直有很多朋友能一起讨论和抬杠。在这过程中我从讨论过程中学习到了很多他们思维的方法，看到了他们思考问题的角度。这也会帮助提升思维的深度和缜密程度。<b>在讨论的攻防过程中，你的自尊心和斗志会逼迫自己去发现别人观点的漏洞，努力弥补自己没有考虑到的漏洞。</b>即使你被别人说服了，你也会学到很多。举例来讲，我发现同西方国家的人讨论问题，他们总有问不完的问题——因为他们在问问题之前总在思考，避免自己问出太愚蠢的问题。最后你会发现他们问的问题越来越精彩，滔滔不绝能说半天，而中国同事通常就在那里发呆打盹（附送个人小窍门，如何避免开会打盹：<a href=""http://www.zhihu.com/question/20812957/answer/16265080"" class=""internal"">会议有催眠的作用么？</a>）<figure><noscript><img src=""https://pic1.zhimg.com/6ee4dca7c847bef67e1d783fc2d32034_b.jpg"" data-rawwidth=""400"" data-rawheight=""400"" class=""content_image"" width=""400""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='400'%20height='400'&gt;&lt;/svg&gt;"" data-rawwidth=""400"" data-rawheight=""400"" class=""content_image lazy"" width=""400"" data-actualsrc=""https://pic1.zhimg.com/6ee4dca7c847bef67e1d783fc2d32034_b.jpg""></figure><br><br>还有，<b>保持对新事物的好奇心和敏感，努力</b><b>扩大自己的视野。</b>这点对于上面两点都非常有帮助。视野狭窄的毛病很容易理解：一个从来没有见过月亮的盲人，是不可能提出“月亮为什么有阴晴圆缺”这样更有深度的问题，更不可能为之深入思考。我面试技术人员的时候经常会问一些同当前技术关系不那么大的问题，比如问这个人的兴趣爱好，看看他对这个兴趣爱好究竟了解得多深，这个兴趣爱好同哪些知识相关，这些知识他又理解到什么深度。一个对周围新鲜事物不敏感没有热情的人是很可怕的，我很难相信他/她会有思维的深度和缜密。而且，不要忘记，<b>事物之间是有千丝万缕联系的，即使是毫无关系的两个学科的知识也许会通过类比或者其他途径触发你的灵感，给你启发和突破口</b>。<figure><noscript><img src=""https://pic4.zhimg.com/62de53c552f672f51c186ba08c652551_b.jpg"" data-rawwidth=""325"" data-rawheight=""325"" class=""content_image"" width=""325""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='325'%20height='325'&gt;&lt;/svg&gt;"" data-rawwidth=""325"" data-rawheight=""325"" class=""content_image lazy"" width=""325"" data-actualsrc=""https://pic4.zhimg.com/62de53c552f672f51c186ba08c652551_b.jpg""></figure><br><br>还有啊，<b>可以建立一套自己的思维和理论体系，但是一定要时不时跳出自己的框框来看看。</b>中国古人很善于建立各种各样未经检验的理论，比如阴阳、五行等等，都能自圆其说，甚至有时候还能解释点儿道理。可是遇到解释不下去的时候，老祖宗的办法是硬往里套——譬如中学为体西学为用等等。可惜这些流毒也多多少少影响到国人思维的深度和缜密。反观西方的思维和理论体系，都是通过不停地跳出自己的框框得来的进步，大家熟知的相对论，就是在跳出又不否定牛顿力学的适用性情况下，打了一个更缜密的补丁——当然，其深度和深远影响力也是不言而喻的。<figure><noscript><img src=""https://pic3.zhimg.com/62b8620f4509ed914e32596c93f2380b_b.jpg"" data-rawwidth=""640"" data-rawheight=""480"" class=""origin_image zh-lightbox-thumb"" width=""640"" data-original=""https://pic3.zhimg.com/62b8620f4509ed914e32596c93f2380b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='640'%20height='480'&gt;&lt;/svg&gt;"" data-rawwidth=""640"" data-rawheight=""480"" class=""origin_image zh-lightbox-thumb lazy"" width=""640"" data-original=""https://pic3.zhimg.com/62b8620f4509ed914e32596c93f2380b_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/62b8620f4509ed914e32596c93f2380b_b.jpg""></figure><br>哦，还有前面的知友提到，我也深刻赞同。纸上得来终觉浅，<b>一定要自己动手才会发觉细节和疏漏所在</b>——终日思考而没有切身的体会，这种思考也会流于浮夸的抬杠，而缺乏对于关键细节的把握。前一阵有个合作伙伴给我提交了一份测试报告，我看来看去觉得不对劲，但是又说不上来在哪里，于是要来他们测试的数据自己编程序分析，一下子发现他们测量方法和测试结果的问题，并且共同讨论了改进的方案。（顺便贩卖一个黄金鸡蛋的实验：<a href=""http://www.zhihu.com/question/20886080/answer/16503802"" class=""internal"">疯狂晃动鸡蛋，煮熟后便得到了一个黄金鸡蛋，这是真的吗？</a>——我从这个实验也发现了别人实验失败的原因，并且试图较系统地表达出来，我想这点你一定看得出来。）<br><figure><noscript><img src=""https://pic2.zhimg.com/3b78f947d515980a2193acbd840a4d7e_b.jpg"" data-rawwidth=""480"" data-rawheight=""360"" class=""origin_image zh-lightbox-thumb"" width=""480"" data-original=""https://pic2.zhimg.com/3b78f947d515980a2193acbd840a4d7e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='480'%20height='360'&gt;&lt;/svg&gt;"" data-rawwidth=""480"" data-rawheight=""360"" class=""origin_image zh-lightbox-thumb lazy"" width=""480"" data-original=""https://pic2.zhimg.com/3b78f947d515980a2193acbd840a4d7e_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/3b78f947d515980a2193acbd840a4d7e_b.jpg""></figure><br><br>差点忘记说了，<b>一定要学好英语</b>。多数原创性的成果还是来自外文文献，这能帮你快点儿找到有深度的成果来学习，同时他们的论证也更严谨一些。<b>不要让你的工具制约你的思维</b>。<br><br>就是这些了，想到再补充吧。",2397,69,1102,DD YY,https://api.zhihu.com/people/3078f698e4c13265f12a364179033b8f,赞同了回答
1573465969558,为什么交叉熵（cross-entropy）可以用于计算代价？,两个数的差值可以表示距离，这个很好理解，但是为什么交叉熵也可以用于计算“距离”？,31,1,1509,https://api.zhihu.com/questions/65288314,刘博,https://api.zhihu.com/people/4751c3974182d4ef9e5ba3f7555118c5,1505283486,1573465969,1588874120,"<p>通用的说，熵(Entropy)被用于描述一个系统中的不确定性(the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。</p><p><b>要想明白交叉熵(Cross Entropy)的意义，可以从熵(Entropy) -&gt; KL散度(Kullback-Leibler Divergence) -&gt; 交叉熵这个顺序入手。</b>当然，也有多种解释方法[1]。</p><p>先给出一个“接地气但不严谨”的概念表述：</p><ul><li>熵：可以表示一个事件A的自信息量，也就是A包含多少信息。</li><li>KL散度：可以用来表示从事件A的角度来看，事件B有多大不同。</li><li>交叉熵：可以用来表示从事件A的角度来看，如何描述事件B。</li></ul><p>一句话总结的话：<b>KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价</b>。</p><p>我知道你现在看着有点晕，但请保持耐心继续往下看。<i>*为了通俗易懂，我没有严格按照数学规范来命名概念，比如文中的“事件”指的是“消息”，望各位严谨的读者理解。</i></p><h2><b>1. 什么是熵(Entropy)？</b></h2><p>放在信息论的语境里面来说，就是一个事件所包含的信息量。我们常常听到“这句话信息量好大”，比如“昨天花了10万，终于在西二环买了套四合院”。</p><p>这句话为什么信息量大？<b>因为它的内容出乎意料，违反常理。</b>由此引出：</p><ul><li><b>越不可能发生的事件信息量越大，</b>比如“我不会死”这句话信息量就很大<b>。而确定事件的信息量就很低，</b>比如“我是我妈生的”，信息量就很低甚至为0<b>。</b></li><li><b>独立事件的信息量可叠加。</b>比如“a. 张三今天喝了阿萨姆红茶，b. 李四前天喝了英式早茶”的信息量就应该恰好等于a+b的信息量，如果张三李四喝什么茶是两个独立事件。</li></ul><p>因此熵被定义为 <img src=""https://www.zhihu.com/equation?tex=S%28x%29+%3D+-%5Csum_%7Bi%7DP%28x_%7Bi%7D%29log_%7Bb%7DP%28x_%7Bi%7D%29"" alt=""S(x) = -\sum_{i}P(x_{i})log_{b}P(x_{i})"" eeimg=""1""> ， <img src=""https://www.zhihu.com/equation?tex=x"" alt=""x"" eeimg=""1""> 指的不同的事件比如喝茶， <img src=""https://www.zhihu.com/equation?tex=P%28x_%7Bi%7D%29"" alt=""P(x_{i})"" eeimg=""1""> 指的是某个事件发生的概率比如和红茶的概率。对于一个一定会发生的事件，其发生概率为1， <img src=""https://www.zhihu.com/equation?tex=S%28x%29+%3D-+log%281%29+%2A+1+%3D+-0%2A1%3D0"" alt=""S(x) =- log(1) * 1 = -0*1=0"" eeimg=""1""> ，信息量为0。</p><h2><b>2. 如何衡量两个事件/分布之间的不同（一）：KL散度</b></h2><p>我们上面说的是对于一个随机变量x的事件A的自信息量，如果我们有另一个独立的随机变量x相关的事件B，该怎么计算它们之间的区别？</p><p><b>此处我们介绍默认的计算方法：KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同</b>。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度<b>不具备有对称性</b>。在距离上的对称性指的是A到B的距离等于B到A的距离。</p><p>举个不恰当的例子，事件A：张三今天买了2个土鸡蛋，事件B：李四今天买了6个土鸡蛋。我们定义随机变量x：买土鸡蛋，那么事件A和B的区别是什么？有人可能说，那就是李四多买了4个土鸡蛋？这个答案只能得50分，因为忘记了""坐标系""的问题。换句话说，对于张三来说，李四多买了4个土鸡蛋。对于李四来说，张三少买了4个土鸡蛋。<b>选取的参照物不同，那么得到的结果也不同</b>。更严谨的说，应该是说我们对于张三和李四买土鸡蛋的期望不同，可能张三天天买2个土鸡蛋，而李四可能因为孩子满月昨天才买了6个土鸡蛋，而平时从来不买。</p><p><b>KL散度的数学定义：</b></p><ul><li>对于<b>离散事件</b>我们可以定义事件A和B的差别为(2.1)： <img src=""https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%2A%7D+D_%7BKL%7D%28A%7C%7CB%29+%3D+%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29+log%5Cbigg%28%5Cfrac%7BP_%7BA%7D%28x_i%29%7D%7BP_%7BB%7D%28x_i%29%7D+%5Cbigg%29+%3D+%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29log%28P_%7BA%7D%28x_i+%29%29-+P_%7BA%7D%28x_i%29log%28P_%7BB%7D%28x_i%29%29+%5Cend%7Bequation%2A%7D"" alt=""\begin{equation*} D_{KL}(A||B) = \sum_{i}P_{A}(x_i) log\bigg(\frac{P_{A}(x_i)}{P_{B}(x_i)} \bigg) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- P_{A}(x_i)log(P_{B}(x_i)) \end{equation*}"" eeimg=""1""></li><li>对于<b>连续事件</b>，那么我们只是把求和改为求积分而已(2.2)。<img src=""https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%2A%7D+D_%7BKL%7D%28A%7C%7CB%29+%3D+%5Cint+a%28x%29+log%5Cbigg%28%5Cfrac%7Ba%28x%29%7D%7Bb%28x%29%7D+%5Cbigg%29+%5Cend%7Bequation%2A%7D"" alt=""\begin{equation*} D_{KL}(A||B) = \int a(x) log\bigg(\frac{a(x)}{b(x)} \bigg) \end{equation*}"" eeimg=""1""></li></ul><p>从公式中可以看出：</p><ul><li><b>如果</b> <img src=""https://www.zhihu.com/equation?tex=P_A%3DP_B"" alt=""P_A=P_B"" eeimg=""1""> ，即两个事件分布完全相同，那么KL散度等于0。</li><li><b>观察公式2.1，可以发现减号左边的就是事件A的熵，请记住这个发现</b>。</li><li>如果颠倒一下顺序求 <img src=""https://www.zhihu.com/equation?tex=D_%7BKL%7D%28B%7C%7CA%29"" alt=""D_{KL}(B||A)"" eeimg=""1"">，那么就需要使用B的熵，答案就不一样了。<b>所以KL散度来计算两个分布A与B的时候是不是对称的，有“坐标系”的问题</b>，<img src=""https://www.zhihu.com/equation?tex=D_%7BKL%7D%28A%7C%7CB%29%5Cne+D_%7BKL%7D%28B%7C%7CA%29"" alt=""D_{KL}(A||B)\ne D_{KL}(B||A)"" eeimg=""1""></li></ul><p>换句话说，KL散度由A自己的熵与B在A上的期望共同决定。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是求 A与B之间的对数差 在 A上的期望值。 </p><h2><b>3. KL散度 = 交叉熵 - 熵？</b></h2><p>如果我们默认了用KL散度来计算两个分布间的不同，那还要交叉熵做什么？</p><p>事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分(公式2.1)：A和B的交叉熵 = A与B的KL散度 - A的熵。 <img src=""https://www.zhihu.com/equation?tex=D_%7BKL%7D%28A%7C%7CB%29+%3D+-S%28A%29%2BH%28A%2CB%29+"" alt=""D_{KL}(A||B) = -S(A)+H(A,B) "" eeimg=""1""></p><p>对比一下这是KL散度的公式：</p><p><img src=""https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%2A%7D+D_%7BKL%7D%28A%7C%7CB%29+%3D+%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29+log%5Cbigg%28%5Cfrac%7BP_%7BA%7D%28x_i%29%7D%7BP_%7BB%7D%28x_i%29%7D+%5Cbigg%29+%3D+%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29log%28P_%7BA%7D%28x_i+%29%29-+P_%7BA%7D%28x_i%29log%28P_%7BB%7D%28x_i%29%29+%5Cend%7Bequation%2A%7D"" alt=""\begin{equation*} D_{KL}(A||B) = \sum_{i}P_{A}(x_i) log\bigg(\frac{P_{A}(x_i)}{P_{B}(x_i)} \bigg) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- P_{A}(x_i)log(P_{B}(x_i)) \end{equation*}"" eeimg=""1""></p><p>这是熵的公式：</p><p><img src=""https://www.zhihu.com/equation?tex=S%28A%29+%3D+-%5Csum_%7Bi%7DP_A%28x_%7Bi%7D%29logP_A%28x_%7Bi%7D%29"" alt=""S(A) = -\sum_{i}P_A(x_{i})logP_A(x_{i})"" eeimg=""1""></p><p>这是交叉熵公式：</p><p><img src=""https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%2A%7D+H%28A%2CB%29%3D+-%5Csum_%7Bi%7DP_%7BA%7D%28x_i%29log%28P_%7BB%7D%28x_i%29%29+%5Cend%7Bequation%2A%7D"" alt=""\begin{equation*} H(A,B)= -\sum_{i}P_{A}(x_i)log(P_{B}(x_i)) \end{equation*}"" eeimg=""1""></p><p><b>此处最重要的观察是，如果</b> <img src=""https://www.zhihu.com/equation?tex=S%28A%29"" alt=""S(A)"" eeimg=""1""><b>是一个常量，那么</b> <img src=""https://www.zhihu.com/equation?tex=D_%7BKL%7D%28A%7C%7CB%29+%3D+H%28A%2CB%29+"" alt=""D_{KL}(A||B) = H(A,B) "" eeimg=""1""> ，<b>也就是说KL散度和交叉熵在特定条件下等价。这个发现是这篇回答的重点。</b></p><p>同时补充交叉熵的一些性质：</p><ul><li>和KL散度相同，交叉熵也不具备对称性： <img src=""https://www.zhihu.com/equation?tex=+H%28A%2CB%29+%5Cne+H%28B%2CA%29"" alt="" H(A,B) \ne H(B,A)"" eeimg=""1""> ，此处不再赘述。</li><li>从名字上来看，Cross(交叉)主要是用于描述这是<b>两个事件</b>之间的相互关系，对自己求交叉熵等于熵。即 <img src=""https://www.zhihu.com/equation?tex=H%28A%2CA%29+%3D+S%28A%29"" alt=""H(A,A) = S(A)"" eeimg=""1""> ，注意只是非负而不一定等于0。</li></ul><h2><b>*4. 另一种理解KL散度、交叉熵、熵的角度（选读）-  可跳过</b></h2><p>那么问题来了，为什么有KL散度和交叉熵两种算法？为什么他们可以用来求分布的不同？什么时候可以等价使用？</p><p><b>一种信息论的解释是</b>：</p><ul><li>熵的意义是对A事件中的随机变量进行编码所需的最小字节数。</li><li>KL散度的意义是“额外所需的编码长度”如果我们用B的编码来表示A。</li><li>交叉熵指的是当你用B作为密码本来表示A时所需要的“平均的编码长度”。</li></ul><p>对于大部分读者，我觉得可以不用深入理解。感谢评论区@王瑞欣的指正，不知道为什么@不到他。</p><p><b>一些对比与观察：</b></p><ul><li>KL散度和交叉熵的不同处：交叉熵中不包括“熵”的部分</li><li>KL散度和交叉熵的相同处：a. 都不具备对称性 b. 都是非负的</li><li><b>等价条件（章节3）：当 <img src=""https://www.zhihu.com/equation?tex=A"" alt=""A"" eeimg=""1""> 固定不变时，那么最小化KL散度 <img src=""https://www.zhihu.com/equation?tex=D_%7BKL%7D%28A%7C%7CB%29"" alt=""D_{KL}(A||B)"" eeimg=""1""> 等价于最小化交叉熵<img src=""https://www.zhihu.com/equation?tex=+H%28A%2CB%29"" alt="" H(A,B)"" eeimg=""1""> 。</b> <img src=""https://www.zhihu.com/equation?tex=D_%7BKL%7D%28A%7C%7CB%29+%3D+H%28A%2CB%29+"" alt=""D_{KL}(A||B) = H(A,B) "" eeimg=""1""></li></ul><p><b>既然等价，那么我们优先选择更简单的公式，因此选择交叉熵。</b></p><h2><b>5. 机器如何“学习”？</b></h2><p>机器学习的过程就是希望在训练数据上<b>模型学到的分布</b> <img src=""https://www.zhihu.com/equation?tex=P%28model%29"" alt=""P(model)"" eeimg=""1""> 和<b>真实数据的分布</b> <img src=""https://www.zhihu.com/equation?tex=P%28real%29"" alt=""P(real)"" eeimg=""1""> 越接近越好，那么我们已经介绍过了....怎么最小化两个分布之间的不同呢？<b>用默认的方法，使其KL散度最小</b>！</p><p><b>但我们没有真实数据的分布，那么只能退而求其次，</b>希望<b>模型学到的分</b>布和<b>训练数据的分布</b> <img src=""https://www.zhihu.com/equation?tex=P%28training%29"" alt=""P(training)"" eeimg=""1""> 尽量相同，<b>也就是把训练数据当做模型和真实数据之间的代理人</b>。假设训练数据是从总体中独立同步分布采样(Independent and identically distributed sampled)而来，那么我们可以利用最小化训练数据的经验误差来降低模型的泛化误差。简单说：</p><ul><li>最终目的是希望学到的模型的分布和真实分布一致： <img src=""https://www.zhihu.com/equation?tex=P%28model%29+%5Csimeq+P%28real+%29"" alt=""P(model) \simeq P(real )"" eeimg=""1""></li><li>但真实分布是不可知的，我们只好假设 训练数据 是从真实数据中独立同分布采样而来： <img src=""https://www.zhihu.com/equation?tex=P%28training%29+%5Csimeq+P%28real+%29"" alt=""P(training) \simeq P(real )"" eeimg=""1""></li><li>退而求其次，我们希望学到的模型分布至少和训练数据的分布一致 <img src=""https://www.zhihu.com/equation?tex=P%28model%29+%5Csimeq+P%28training%29"" alt=""P(model) \simeq P(training)"" eeimg=""1""></li></ul><p>由此非常理想化的看法是如果<b>模型(左)</b>能够学到<b>训练数据(中)</b>的分布，那么应该近似的学到了<b>真实数据(右)</b>的分布： <img src=""https://www.zhihu.com/equation?tex=P%28model%29+%5Csimeq+P%28training%29+%5Csimeq+P%28real%29"" alt=""P(model) \simeq P(training) \simeq P(real)"" eeimg=""1""></p><h2><b>6. 为什么交叉熵可以用作代价？</b></h2><p>接着上一点说，最小化模型分布 <img src=""https://www.zhihu.com/equation?tex=P%28model%29"" alt=""P(model)"" eeimg=""1""> 与 训练数据上的分布 <img src=""https://www.zhihu.com/equation?tex=P%28training%29"" alt=""P(training)"" eeimg=""1""> 的差异 等价于 最小化这两个分布间的KL散度，也就是最小化 <img src=""https://www.zhihu.com/equation?tex=KL%28P%28training%29%7C%7CP%28model%29%29"" alt=""KL(P(training)||P(model))"" eeimg=""1""> 。</p><p>比照第四部分的公式：</p><ul><li>此处的A就是数据的真实分布： <img src=""https://www.zhihu.com/equation?tex=P%28training%29"" alt=""P(training)"" eeimg=""1""></li><li>此处的B就是模型从训练数据上学到的分布： <img src=""https://www.zhihu.com/equation?tex=P%28model%29"" alt=""P(model)"" eeimg=""1""></li></ul><p>巧的是，<b>训练数据的分布A是给定的</b>。那么根据<b>我们在第四部分说的，因为A固定不变，那么求 <img src=""https://www.zhihu.com/equation?tex=D_%7BKL%7D%28A%7C%7CB%29"" alt=""D_{KL}(A||B)"" eeimg=""1""> 等价于求 <img src=""https://www.zhihu.com/equation?tex=H%28A%2CB%29"" alt=""H(A,B)"" eeimg=""1""> ，也就是A与B的交叉熵</b>。<b>得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。</b></p><p><b>但是，完美的学到了训练数据分布往往意味着过拟合，因为训练数据不等于真实数据，我们只是假设它们是相似的，而一般还要假设存在一个高斯分布的误差，是模型的泛化误差下线。</b></p><h2><b>7. 总结</b></h2><p>因此在评价机器学习模型时，我们往往不能只看训练数据上的误分率和交叉熵，还是要关注测试数据上的表现。如果在测试集上的表现也不错，才能保证这不是一个过拟合或者欠拟合的模型。交叉熵比照误分率还有更多的优势，因为它可以和很多概率模型完美的结合。</p><p><b>所以逻辑思路是，为了让学到的模型分布更贴近真实数据分布，我们最小化 模型数据分布 与 训练数据之间的KL散度，而因为训练数据的分布是固定的，因此最小化KL散度等价于最小化交叉熵。</b></p><p><b>因为等价，而且交叉熵更简单更好计算，当然用它咯 ʕ•ᴥ•ʔ</b></p><hr><p>[1] 不同的领域都有不同解释，更传统的机器学习说法是似然函数的最大化就是交叉熵。<b>正所谓横看成岭侧成峰，大家没必要非说哪种思路是对的，有道理就好</b>。</p><a data-draft-node=""block"" data-draft-type=""mcn-link-card"" data-mcn-id=""1241908738887417856""></a><p></p>",1851,102,248,微调,https://api.zhihu.com/people/e3f5794fa10022aa07c05b0b9e6dc537,赞同了回答
1573369646722,当BERT遇上知识图谱,,,,,https://api.zhihu.com/articles/91052495,,,,1573369646,,"<h2><b>写在前面</b></h2><p>上篇博客理了一下一些知识表示学习模型，那今天我们来看目前最流行的BERT模型加上<b>外部知识</b>这个buff后到底会有怎么样的发展。其实这个思路在之前就有出现过比较有意思且高效的工作，像百度的<b>ERNIE</b>和<b>ERNIE2.0</b> 以及清华的<b>ERNIE</b>，这些工作的介绍可以参考<a href=""http://link.zhihu.com/?target=https%3A//blog.csdn.net/Kaiyuan_sjtu/article/details/90757442"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">站在BERT肩膀上的NLP新秀们（PART I）</a>。</p><p><i>√  </i>KG-BERT from NWU</p><p><i>√  </i>K-BERT from PKU</p><p><i>√  </i>KnowBERT from AI2</p><h2><b>1、<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.03193"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">KG-BERT: BERT for Knowledge Graph Completion(2019)</a></b></h2><p>这篇文章是介绍知识库补全方面的工作，结合预训练模型BERT可以将更丰富的上下文表示结合进模型中，在三元组分类、链接预测以及关系预测等任务中达到了SOTA效果。</p><p>具体的做法也非常简单易懂，就是修改了BERT模型的输入使其适用于知识库三元组的形式。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-dd8cf5259b8ff15120b7908824353e28_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""656"" data-rawheight=""374"" class=""origin_image zh-lightbox-thumb"" width=""656"" data-original=""https://pic3.zhimg.com/v2-dd8cf5259b8ff15120b7908824353e28_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='656'%20height='374'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""656"" data-rawheight=""374"" class=""origin_image zh-lightbox-thumb lazy"" width=""656"" data-original=""https://pic3.zhimg.com/v2-dd8cf5259b8ff15120b7908824353e28_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-dd8cf5259b8ff15120b7908824353e28_b.jpg""></figure><p>首先是<b>KG-BERT(a)</b>，输入为三元组 <img src=""https://www.zhihu.com/equation?tex=%28h%2C+r%2C+t%29"" alt=""(h, r, t)"" eeimg=""1"">的形式，当然还有BERT自带的special tokens。举个栗子，对于三元组 <img src=""https://www.zhihu.com/equation?tex=%28Steve+Jobs%2C+founded%2C+Apple+Inc%29"" alt=""(Steve Jobs, founded, Apple Inc)"" eeimg=""1""> ，上图中的<code>Head Entity</code>输入可以表示为<code>Steven Paul Jobs was an American business magnate, entrepreneur and investor</code>或者<code>Steve Jobs</code>，而<code>Tail Entity</code>可以表示为<code>Apple Inc. is an American multinational technology company headquartered in Cupertino, California</code>或<code>Apple Inc</code>。也就是说，头尾实体的输入可以是<b>实体描述</b>句子或者<b>实体名</b>本身。</p><p>模型训练是首先分别构建<b>positive triple set</b>和<b>negative triple set</b>，然后用BERT的[CLS]标签做一个sigmoid打分以及最后交叉熵损失 </p><p><img src=""https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D-%5Csum_%7B%5Ctau+%5Cin+%5Cmathbb%7BD%7D%2B%5Ccup+%5Cmathbb%7BD%7D%5E%7B-%7D%7D%5Cleft%28y_%7B%5Ctau%7D+%5Clog+%5Cleft%28s_%7B%5Ctau+0%7D%5Cright%29%2B%5Cleft%281-y_%7B%5Ctau%7D%5Cright%29+%5Clog+%5Cleft%28s_%7B%5Ctau+1%7D%5Cright%29%5Cright%29"" alt=""\mathcal{L}=-\sum_{\tau \in \mathbb{D}+\cup \mathbb{D}^{-}}\left(y_{\tau} \log \left(s_{\tau 0}\right)+\left(1-y_{\tau}\right) \log \left(s_{\tau 1}\right)\right)"" eeimg=""1""> </p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-fc24b1e3a98066bd914fe54af731f846_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""705"" data-rawheight=""531"" class=""origin_image zh-lightbox-thumb"" width=""705"" data-original=""https://pic3.zhimg.com/v2-fc24b1e3a98066bd914fe54af731f846_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='705'%20height='531'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""705"" data-rawheight=""531"" class=""origin_image zh-lightbox-thumb lazy"" width=""705"" data-original=""https://pic3.zhimg.com/v2-fc24b1e3a98066bd914fe54af731f846_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-fc24b1e3a98066bd914fe54af731f846_b.jpg""></figure><p>上述的<b>KG-BERT(a)</b>需要输入关系，对于关系分类任务不适用，于是作者又提出一种<b>KG-BERT(b)</b>，如上图。这里只是把sigmoid的二分类改成了softmax的关系多分类。</p><p><img src=""https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%5E%7B%5Cprime%7D%3D-%5Csum_%7B%5Ctau+%5Cin+%5Cmathbb%7BD%7D%5E%7B%2B%7D%7D+%5Csum_%7Bi%3D1%7D%5E%7BR%7D+y_%7B%5Ctau+i%7D%5E%7B%5Cprime%7D+%5Clog+%5Cleft%28s_%7B%5Ctau+i%7D%5E%7B%5Cprime%7D%5Cright%29"" alt=""\mathcal{L}^{\prime}=-\sum_{\tau \in \mathbb{D}^{+}} \sum_{i=1}^{R} y_{\tau i}^{\prime} \log \left(s_{\tau i}^{\prime}\right)"" eeimg=""1""> </p><p>有一点想法，既然已经训练了那么多的三元组信息，按理说模型应该是会有学到外部知识的一些信息，也算是一种知识融合，是不是可以把这个模型经过三元组训练后用来做一做其他的NLU任务看看效果？</p><a href=""http://link.zhihu.com/?target=https%3A//github.com/yao8839836/kg-bert"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic1.zhimg.com/v2-a1f1b0573b2c2e60ef8d336b8cdc18c8_ipico.jpg"" data-image-width=""180"" data-image-height=""180"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">yao8839836/kg-bert</a><h2><b>2、<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.07606"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">K-BERT: Enabling Language Representation with Knowledge Graph(2019)</a></b></h2><p>作者指出通过公开语料训练的BERT模型仅仅是获得了general knowledge，就像是一个普通人，当面对特定领域的情境时（如医疗、金融等），往往表现不如意，即<b>domain discrepancy between pre-training and fine-tuning</b>。而本文提出的<b>K-BERT</b>则像是领域专家，通过将知识库中的结构化信息（三元组）融入到预训练模型中，可以更好地解决领域相关任务。如何将外部知识整合到模型中成了一个关键点，这一步通常存在两个难点：</p><ul><li><b>Heterogeneous Embedding Space：</b> 即文本的单词embedding和知识库的实体实体embedding通常是通过不同方式获取的，使得他俩的向量空间不一致；</li><li><b>Knowledge Noise：</b> 即过多的知识融合可能会使原始句子偏离正确的本意，过犹不及。</li></ul><p>Okay，理解了大致思想之后我们来分析具体的实现方式。模型的整体框架如下图，主要包括了四个子模块： <b>knowledge layer</b>, <b>embedding layer</b>, <b>seeing layer</b> 和 <b>mask-transformer</b>。对于一个给定的输入 <img src=""https://www.zhihu.com/equation?tex=s%3D%5Cleft%5C%7Bw_%7B0%7D%2C+w_%7B1%7D%2C+w_%7B2%7D%2C+%5Cdots%2C+w_%7Bn%7D%5Cright%5C%7D"" alt=""s=\left\{w_{0}, w_{1}, w_{2}, \dots, w_{n}\right\}"" eeimg=""1""> ，首先 <b>knowledge layer</b>会从一个KG中注入相关的三元组，将原来的句子转换成一个knowledge-rich的句子树；接着句子树被同时送入<b>embedding layer</b>和<b>seeing layer</b>生成一个token级别的embedding表示和一个可见矩阵（visible matrix）；最后通过<b>mask-transformer</b>层编码后用于下游任务的输出。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-bf6449f45518cf3f88af7a62affb0247_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""680"" data-rawheight=""708"" class=""origin_image zh-lightbox-thumb"" width=""680"" data-original=""https://pic4.zhimg.com/v2-bf6449f45518cf3f88af7a62affb0247_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='680'%20height='708'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""680"" data-rawheight=""708"" class=""origin_image zh-lightbox-thumb lazy"" width=""680"" data-original=""https://pic4.zhimg.com/v2-bf6449f45518cf3f88af7a62affb0247_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-bf6449f45518cf3f88af7a62affb0247_b.jpg""></figure><h3><b>Knowledge Layer</b></h3><p>这一层的输入是原始句子 <img src=""https://www.zhihu.com/equation?tex=s%3D%5Cleft%5C%7Bw_%7B0%7D%2C+w_%7B1%7D%2C+w_%7B2%7D%2C+%5Cdots%2C+w_%7Bn%7D%5Cright%5C%7D"" alt=""s=\left\{w_{0}, w_{1}, w_{2}, \dots, w_{n}\right\}"" eeimg=""1""> ，输出是融入KG信息后的句子树 <img src=""https://www.zhihu.com/equation?tex=t+%3D+%5Cleft%5C%7Bw_%7B0%7D%2C+w_%7B1%7D%2C+%5Cldots%2C+w_%7Bi%7D%5Cleft%5C%7B%5Cleft%28r_%7Bi+0%7D%2C+w_%7Bi+0%7D%5Cright%29%2C+%5Cldots%2C%5Cleft%28r_%7Bi+k%7D%2C+w_%7Bi+k%7D%5Cright%29%5Cright%5C%7D%2C+%5Cldots%2C+w_%7Bn%7D%5Cright%5C%7D"" alt=""t = \left\{w_{0}, w_{1}, \ldots, w_{i}\left\{\left(r_{i 0}, w_{i 0}\right), \ldots,\left(r_{i k}, w_{i k}\right)\right\}, \ldots, w_{n}\right\}"" eeimg=""1""> 通过两步完成：</p><ul><li><b>K-Query</b>  输入句子中涉及的所有实体都被选中，并查询它们在KG中对应的三元组 <img src=""https://www.zhihu.com/equation?tex=E"" alt=""E"" eeimg=""1""> ；</li><li><b>K-Inject</b> 将查询到的三元组注入到句子 <img src=""https://www.zhihu.com/equation?tex=s"" alt=""s"" eeimg=""1""> 中，将 <img src=""https://www.zhihu.com/equation?tex=E"" alt=""E"" eeimg=""1""> 中的三元组插入到它们相应的位置，并生成一个句子树 <img src=""https://www.zhihu.com/equation?tex=t"" alt=""t"" eeimg=""1""> 。</li></ul><h3><b>Embedding Layer</b></h3><p>K-BERT的输入和原始BERT的输入形式是一样的，都需要token embedding, position embedding和segment embedding，不同的是，K-BERT的输入是一个句子树，因此问题就变成了句子树到序列化句子的转化，并同时保留结构化信息。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-d1a6c85faf6a3db43125f56e96ac672b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1544"" data-rawheight=""724"" class=""origin_image zh-lightbox-thumb"" width=""1544"" data-original=""https://pic1.zhimg.com/v2-d1a6c85faf6a3db43125f56e96ac672b_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1544'%20height='724'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1544"" data-rawheight=""724"" class=""origin_image zh-lightbox-thumb lazy"" width=""1544"" data-original=""https://pic1.zhimg.com/v2-d1a6c85faf6a3db43125f56e96ac672b_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-d1a6c85faf6a3db43125f56e96ac672b_b.jpg""></figure><h3><b>Token embedding</b></h3><p>句子树的序列化，作者提出一种简单的重排策略：<b>分支中的token被插入到相应节点之后，而后续的token被向后移动</b>。举个栗子，对于上图中的句子树，则重排后变成了<code>Tim Cook CEO Apple is visiting Beijing capital China is a City now</code>。没错，这的确看上去毫无逻辑，但是还好后面可以通过trick来解决。</p><h3><b>Soft-position embedding</b></h3><p>通过重排后的句子显然是毫无意义的，这里利用了position embedding来还原回结构信息。还是以上图为例，重排后，<code>CEO</code>和<code>Apple</code>被插入在了<code>Cook</code>和<code>is</code>之间，但是<code>is</code>应该是接在<code>Cook</code>之后一个位置的，那么我们直接把<code>is</code>的position number 设置为3即可。Segment embedding 部分同BERT一样。</p><h3><b>Seeing Layer</b></h3><p>作者认为Seeing layer的mask matrix是K-BERT有效的关键，主要解决了前面提到的<b>Knowledge Noise</b>问题。栗子中<code>China</code>仅仅修饰的是<code>Beijing</code>，和<code>Apple</code>半毛钱关系没有，因此像这种token之间就不应该有相互影响。为此定义一个可见矩阵，判断句子中的单词之间是否彼此影响 </p><p><img src=""https://www.zhihu.com/equation?tex=M_%7Bi+j%7D%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7B0%7D%26%7Bw_%7Bi%7D+%5Cominus+w_%7Bj%7D%7D+%5C%5C+%7B-%5Cinfty%7D+%26+%7Bw_%7Bi%7D+%5Coslash+w_%7Bj%7D%7D%5Cend%7Barray%7D%5Cright."" alt=""M_{i j}=\left\{\begin{array}{cc}{0}&{w_{i} \ominus w_{j}} \\ {-\infty} &amp; {w_{i} \oslash w_{j}}\end{array}\right."" eeimg=""1""> </p><h3><b>Mask-Transformer</b></h3><p>BERT中的Transformer Encoder不能接受上述可见矩阵作为输入，因此需要稍作改进。Mask-Transformer是一层层mask-self-attention的堆叠， </p><p><img src=""https://www.zhihu.com/equation?tex=+%5Cbegin%7Barray%7D+%7B+c+%7D+%7B+Q+%5E+%7B+i+%2B+1+%7D+%2C+K+%5E+%7B+i+%2B+1+%7D+%2C+V+%5E+%7B+i+%2B+1+%7D+%3D+h+%5E+%7B+i+%7D+W+_+%7B+q+%7D+%2C+h+%5E+%7B+i+%7D+W+_+%7B+k+%7D+%2C+h+%5E+%7B+i+%7D+W+_+%7B+v+%7D+%7D+%5C%5C+%7B+S+%5E+%7B+i+%2B+1+%7D+%3D+%5Coperatorname+%7B+softmax+%7D+%5Cleft%28+%5Cfrac+%7B+Q+%5E+%7B+i+%2B+1+%7D+K+%5E+%7B+i+%2B+1+%7D+%2B+M+%7D+%7B+%5Csqrt+%7B+d+_+%7B+k+%7D+%7D+%7D+%5Cright%29+%7D+%5C%5C+%7B+h+%5E+%7B+i+%2B+1+%7D+%3D+S+%5E+%7B+i+%2B+1+%7D+V+%5E+%7B+i+%2B+1+%7D+%7D+%5Cend%7Barray%7D+"" alt="" \begin{array} { c } { Q ^ { i + 1 } , K ^ { i + 1 } , V ^ { i + 1 } = h ^ { i } W _ { q } , h ^ { i } W _ { k } , h ^ { i } W _ { v } } \\ { S ^ { i + 1 } = \operatorname { softmax } \left( \frac { Q ^ { i + 1 } K ^ { i + 1 } + M } { \sqrt { d _ { k } } } \right) } \\ { h ^ { i + 1 } = S ^ { i + 1 } V ^ { i + 1 } } \end{array} "" eeimg=""1""></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-4689a5cd50175ec3a36e901b8bb6b733_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""791"" data-rawheight=""414"" class=""origin_image zh-lightbox-thumb"" width=""791"" data-original=""https://pic4.zhimg.com/v2-4689a5cd50175ec3a36e901b8bb6b733_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='791'%20height='414'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""791"" data-rawheight=""414"" class=""origin_image zh-lightbox-thumb lazy"" width=""791"" data-original=""https://pic4.zhimg.com/v2-4689a5cd50175ec3a36e901b8bb6b733_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-4689a5cd50175ec3a36e901b8bb6b733_b.jpg""></figure><a href=""http://link.zhihu.com/?target=https%3A//github.com/autoliuweijie/K-BERT"" data-draft-node=""block"" data-draft-type=""link-card"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Code Here</a><h2><b>3、<a href=""http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.10681"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations(2019)</a></b></h2><p>这篇是将外部常识知识用于特定的任务，在对话中的情绪识别问题，提出一种<b>Knowledge- Enriched Transformer (KET)</b> 框架，如下图。关于Transformer、情绪识别等背景知识不做复杂介绍，主要看看是如何将外部知识应用起来的。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-a61a834550095d15416cf830e7dc9bb5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1401"" data-rawheight=""816"" class=""origin_image zh-lightbox-thumb"" width=""1401"" data-original=""https://pic2.zhimg.com/v2-a61a834550095d15416cf830e7dc9bb5_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1401'%20height='816'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1401"" data-rawheight=""816"" class=""origin_image zh-lightbox-thumb lazy"" width=""1401"" data-original=""https://pic2.zhimg.com/v2-a61a834550095d15416cf830e7dc9bb5_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-a61a834550095d15416cf830e7dc9bb5_b.jpg""></figure><h3><b>任务概览</b></h3><p>首先来看一下任务，给定对话数据集，数据格式为 <img src=""https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+X+_+%7B+j+%7D+%5E+%7B+i+%7D+%2C+Y+_+%7B+j+%7D+%5E+%7B+i+%7D+%5Cright%5C%7D"" alt=""\left\{ X _ { j } ^ { i } , Y _ { j } ^ { i } \right\}"" eeimg=""1""> ， <img src=""https://www.zhihu.com/equation?tex=X+_+%7B+j+%7D+%5E+%7B+i+%7D"" alt=""X _ { j } ^ { i }"" eeimg=""1""> 表示第 <img src=""https://www.zhihu.com/equation?tex=i"" alt=""i"" eeimg=""1""> 组对话第 <img src=""https://www.zhihu.com/equation?tex=j+"" alt=""j "" eeimg=""1""> 句表述， <img src=""https://www.zhihu.com/equation?tex=Y+_+%7B+j+%7D+%5E+%7B+i+%7D"" alt=""Y _ { j } ^ { i }"" eeimg=""1""> 表示第 <img src=""https://www.zhihu.com/equation?tex=i"" alt=""i"" eeimg=""1""> 组对话第 <img src=""https://www.zhihu.com/equation?tex=j"" alt=""j"" eeimg=""1""> 句表述对应的标签，目标是最大化 <img src=""https://www.zhihu.com/equation?tex=%5CPhi+%3D+%5Cprod+_+%7B+i+%3D+1+%7D+%5E+%7B+N+%7D+%5Cprod+_+%7B+j+%3D+1+%7D+%5E+%7B+N+_+%7B+i+%7D+%7D+p+%5Cleft%28+Y+_+%7B+j+%7D+%5E+%7B+i+%7D+%7C+X+_+%7B+j+%7D+%5E+%7B+i+%7D+%2C+X+_+%7B+j+-+1+%7D+%5E+%7B+i+%7D+%2C+%5Cldots+%2C+X+_+%7B+1+%7D+%5E+%7B+i+%7D+%3B+%5Ctheta+%5Cright%29"" alt=""\Phi = \prod _ { i = 1 } ^ { N } \prod _ { j = 1 } ^ { N _ { i } } p \left( Y _ { j } ^ { i } | X _ { j } ^ { i } , X _ { j - 1 } ^ { i } , \ldots , X _ { 1 } ^ { i } ; \theta \right)"" eeimg=""1""> </p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-a34516c22d7bd62c5b40121c5ca37d4e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""675"" data-rawheight=""380"" class=""origin_image zh-lightbox-thumb"" width=""675"" data-original=""https://pic1.zhimg.com/v2-a34516c22d7bd62c5b40121c5ca37d4e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='675'%20height='380'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""675"" data-rawheight=""380"" class=""origin_image zh-lightbox-thumb lazy"" width=""675"" data-original=""https://pic1.zhimg.com/v2-a34516c22d7bd62c5b40121c5ca37d4e_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-a34516c22d7bd62c5b40121c5ca37d4e_b.jpg""></figure><h3><b>Knowledge Retrieval</b></h3><p>模型使用了常识知识库<b>ConceptNet</b>和情感词典<b>NEC_VAD</b>作为外部知识来源，对于对话中的每个非停用词token，都会从ConceptNet检索包含其直接邻居的连通知识图 <img src=""https://www.zhihu.com/equation?tex=g%28t%29"" alt=""g(t)"" eeimg=""1""> ，并且对对于每一个 <img src=""https://www.zhihu.com/equation?tex=g%28t%29"" alt=""g(t)"" eeimg=""1""> ，会移除停用词、词表外词以及置信分数低于1的concept。经过上述操作，最终得到的，对于每一个token，都会有 a list of tuples: </p><p><img src=""https://www.zhihu.com/equation?tex=%5Cleft%28c_%7B1%7D%2C+s_%7B1%7D%2C+V+A+D%5Cleft%28c_%7B1%7D%5Cright%29%5Cright%29%2C%5Cleft%28c_%7B2%7D%2C+s_%7B2%7D%2C+V+A+D%5Cleft%28c_%7B2%7D%5Cright%29%5Cright%29%2C+%5Cldots%2C%5Cleft%28c_%7B%7Cg%28t%29%7C%7D%2C+s_%7B%7Cg%28t%29%7C%7D%2C+V+A+D%5Cleft%28c_%7B%7Cg%28t%29%7C%7D%5Cright%29%5Cright%29"" alt=""\left(c_{1}, s_{1}, V A D\left(c_{1}\right)\right),\left(c_{2}, s_{2}, V A D\left(c_{2}\right)\right), \ldots,\left(c_{|g(t)|}, s_{|g(t)|}, V A D\left(c_{|g(t)|}\right)\right)"" eeimg=""1""> </p><h3><b>Embedding Layer</b></h3><p>就是常见的word embedding 加上 position embedding</p><h3><b>Dynamic Context-Aware Affective Graph Attention</b></h3><p>这个名字好拗口…动态上下文感知情感图注意力，目的就是计算每个token融入知识后的上下文表示 <img src=""https://www.zhihu.com/equation?tex=%5Cmathbf%7Bc%7D%28t%29%3D%5Csum_%7Bk%3D1%7D%5E%7B%7Cg%28t%29%7C%7D+%5Calpha_%7Bk%7D+%2A+%5Cmathbf%7Bc%7D_%7B%5Cmathbf%7Bk%7D%7D"" alt=""\mathbf{c}(t)=\sum_{k=1}^{|g(t)|} \alpha_{k} * \mathbf{c}_{\mathbf{k}}"" eeimg=""1""> <i>其中</i> <img src=""https://www.zhihu.com/equation?tex=c_%7Bk%7D"" alt=""c_{k}"" eeimg=""1"">表示concept embedding， <img src=""https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D"" alt=""\alpha_{k}"" eeimg=""1""> 代表其对应的attention weight  <img src=""https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D%3D%5Coperatorname%7Bsoftmax%7D%5Cleft%28w_%7Bk%7D%5Cright%29"" alt=""\alpha_{k}=\operatorname{softmax}\left(w_{k}\right)"" eeimg=""1""> </p><p>这里面最重要的就是 <img src=""https://www.zhihu.com/equation?tex=w_%7Bk%7D+"" alt=""w_{k} "" eeimg=""1""> 的计算，在这个模型中假设上下文越相关且情感强度越强的concept越重要，即具有更高的权重。那么如何衡量呢？提出了两个因素：<b>相关性因子</b>和<b>情感因子</b></p><ul><li><b>相关性因子</b>：衡量 <img src=""https://www.zhihu.com/equation?tex=c_%7Bk%7D"" alt=""c_{k}"" eeimg=""1""> 和会话上下文之间关系的关联程度。</li></ul><p><img src=""https://www.zhihu.com/equation?tex=r+e+l_%7Bk%7D%3D%5Cmin+-%5Cmax+%5Cleft%28s_%7Bk%7D%5Cright%29+%2A+%5Coperatorname%7Babs%7D%5Cleft%28%5Ccos+%5Cleft%28%5Cmathbf%7BC%7D+%5Cmathbf%7BR%7D%5Cleft%28X%5E%7Bi%7D%5Cright%29%2C+%5Cmathbf%7Bc%7D_%7B%5Cmathbf%7Bk%7D%7D%5Cright%29%5Cright%29"" alt=""r e l_{k}=\min -\max \left(s_{k}\right) * \operatorname{abs}\left(\cos \left(\mathbf{C} \mathbf{R}\left(X^{i}\right), \mathbf{c}_{\mathbf{k}}\right)\right)"" eeimg=""1""> </p><p>其中 <img src=""https://www.zhihu.com/equation?tex=cos%28%2A%29"" alt=""cos(*)"" eeimg=""1""> 表示余弦相似度， <img src=""https://www.zhihu.com/equation?tex=CR%28X%5E%7Bi%7D%29"" alt=""CR(X^{i})"" eeimg=""1""> 表示第 <img src=""https://www.zhihu.com/equation?tex=i"" alt=""i"" eeimg=""1""> 组对话的上下文表示，因为一组对话中可能存在多个句子，所以就表示为所有句子的向量平均 <img src=""https://www.zhihu.com/equation?tex=%5Cmathbf%7BC+R%7D%5Cleft%28X%5E%7Bi%7D%5Cright%29%3D%5Coperatorname%7Bavg%7D%5Cleft%28%5Cmathbf%7BS%7D+%5Cmathbf%7BR%7D%5Cleft%28X_%7Bj-M%7D%5E%7Bi%7D%5Cright%29%2C+%5Cldots%2C+%5Cmathbf%7BS+R%7D%5Cleft%28X_%7Bj%7D%5E%7Bi%7D%5Cright%29%5Cright%29"" alt=""\mathbf{C R}\left(X^{i}\right)=\operatorname{avg}\left(\mathbf{S} \mathbf{R}\left(X_{j-M}^{i}\right), \ldots, \mathbf{S R}\left(X_{j}^{i}\right)\right)"" eeimg=""1""> </p><ul><li><b>情感因子</b> ：衡量 <img src=""https://www.zhihu.com/equation?tex=c_%7Bk%7D"" alt=""c_{k}"" eeimg=""1""> 的情感强度 </li></ul><p><img src=""https://www.zhihu.com/equation?tex=a+f+f_%7Bk%7D%3D%5Cmin+-%5Cmax+%5Cleft%28%5Cleft%7C%5Cleft%5BV%5Cleft%28c_%7Bk%7D%5Cright%29-1+%2F+2%2C+A%5Cleft%28c_%7Bk%7D%5Cright%29+%2F+2%5Cright%5D%5Cright%7C_%7B2%7D%5Cright%29"" alt=""a f f_{k}=\min -\max \left(\left|\left[V\left(c_{k}\right)-1 / 2, A\left(c_{k}\right) / 2\right]\right|_{2}\right)"" eeimg=""1""> </p><p>将上述两个因素综合考虑可得到 <img src=""https://www.zhihu.com/equation?tex=w_%7Bk%7D"" alt=""w_{k}"" eeimg=""1""> 的表达式 <img src=""https://www.zhihu.com/equation?tex=w_%7Bk%7D%3D%5Clambda_%7Bk%7D+%2A+r+e+l_%7Bk%7D%2B%5Cleft%281-%5Clambda_%7Bk%7D%5Cright%29+%2A+a+f+f_%7Bk%7D"" alt=""w_{k}=\lambda_{k} * r e l_{k}+\left(1-\lambda_{k}\right) * a f f_{k}"" eeimg=""1""> <br>最后融入知识的上下文表示可以通过一个线性转变得到 <img src=""https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7Bt%7D%7D%3D%5Cmathbf%7BW%7D%5B%5Cmathbf%7Bt%7D+%3B+%5Cmathbf%7Bc%7D%28t%29%5D"" alt=""\hat{\mathbf{t}}=\mathbf{W}[\mathbf{t} ; \mathbf{c}(t)]"" eeimg=""1""> </p><h3><b>Hierarchical Self-Attention</b></h3><p>提出了一种层次化自注意力机制，以利用对话的结构表示形式并学习上下文话语的向量表示形式。</p><ul><li>第一步，对于utterance <img src=""https://www.zhihu.com/equation?tex=X_%7Bn%7D%5E%7Bi%7D"" alt=""X_{n}^{i}"" eeimg=""1"">，可以表示为： <img src=""https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7BX%7D%7D%7Bn%7D%5E%7Bi%7D%3DF+F%5Cleft%28L%5E%7B%5Cprime%7D%5Cleft%28M+H%5Cleft%28L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D%7Bn%7D%5E%7Bi%7D%5Cright%29%2C+L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D%7Bn%7D%5E%7Bi%7D%5Cright%29%2C+L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D%7Bn%7D%5E%7Bi%7D%5Cright%29%5Cright%29%5Cright%29%5Cright%29"" alt=""\hat{\mathbf{X}}{n}^{i}=F F\left(L^{\prime}\left(M H\left(L\left(\hat{\mathbf{X}}{n}^{i}\right), L\left(\hat{\mathbf{X}}{n}^{i}\right), L\left(\hat{\mathbf{X}}{n}^{i}\right)\right)\right)\right)"" eeimg=""1""> <br><img src=""https://www.zhihu.com/equation?tex=M+H%28Q%2C+K%2C+V%29%3D%5Coperatorname%7Bsoftmax%7D%5Cleft%28%5Cfrac%7BQ+K%5E%7BT%7D%7D%7B%5Csqrt%7Bd_%7Bs%7D%7D%7D%5Cright%29+V"" alt=""M H(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{s}}}\right) V"" eeimg=""1""> <img src=""https://www.zhihu.com/equation?tex=F+F%28x%29%3D%5Cmax+%5Cleft%280%2C+x+W_%7B1%7D%2Bb_%7B1%7D%5Cright%29+W_%7B2%7D%2Bb_%7B2%7D"" alt=""F F(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}"" eeimg=""1""> </li><li>第二步，利用所有对话来学习上下文表示 <img src=""https://www.zhihu.com/equation?tex=%5Cmathbf%7BC%7D%5E%7Bi%7D%3DF+F%5Cleft%28L%5E%7B%5Cprime%7D%5Cleft%28M+H%5Cleft%28L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D%5E%7Bi%7D%5Cright%29%2C+L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D%5E%7Bi%7D%5Cright%29%2C+L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D%5E%7Bi%7D%5Cright%29%5Cright%29%5Cright%29%5Cright%29"" alt=""\mathbf{C}^{i}=F F\left(L^{\prime}\left(M H\left(L\left(\hat{\mathbf{X}}^{i}\right), L\left(\hat{\mathbf{X}}^{i}\right), L\left(\hat{\mathbf{X}}^{i}\right)\right)\right)\right)"" eeimg=""1""> <img src=""https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7BX%7D%7D%5E%7Bi%7D+%5Ctext+%7B+denotes+%7D%5Cleft%5B%5Chat%7B%5Cmathbf%7BX%7D%7D_%7Bj-M%7D%5E%7B%5Cprime+i%7D+%3B+%5Cldots+%3B+%5Chat%7B%5Cmathbf%7BX%7D%7D_%7Bj-1%7D%5E%7Bi%7D%5Cright%5D"" alt=""\hat{\mathbf{X}}^{i} \text { denotes }\left[\hat{\mathbf{X}}_{j-M}^{\prime i} ; \ldots ; \hat{\mathbf{X}}_{j-1}^{i}\right]"" eeimg=""1""> </li></ul><h3><b>Context-Response Cross-Attention</b></h3><p>最后，通过前面得到的融入外部知识的上下文表示来得出预测标签。</p><p><img src=""https://www.zhihu.com/equation?tex=%5Cmathbf%7BR%7D%5E%7Bi%7D%3DF+F%5Cleft%28L%5E%7B%5Cprime%7D%5Cleft%28M+H%5Cleft%28L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D_%7Bj%7D%5E%7B%5Cprime+i%7D%5Cright%29%2C+L%5Cleft%28%5Cmathbf%7BC%7D%5E%7Bi%7D%5Cright%29%2C+L%5Cleft%28%5Cmathbf%7BC%7D%5E%7Bi%7D%5Cright%29%5Cright%29%5Cright%29%5Cright%29"" alt=""\mathbf{R}^{i}=F F\left(L^{\prime}\left(M H\left(L\left(\hat{\mathbf{X}}_{j}^{\prime i}\right), L\left(\mathbf{C}^{i}\right), L\left(\mathbf{C}^{i}\right)\right)\right)\right)"" eeimg=""1""> </p><p><img src=""https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7BX%7D%7D_%7Bj%7D%5E%7B%5Cprime+i%7D%3DL%5E%7B%5Cprime%7D%5Cleft%28M+H%5Cleft%28L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D_%7Bj%7D%5E%7Bi%7D%5Cright%29%2C+L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D_%7Bj%7D%5E%7Bi%7D%5Cright%29%2C+L%5Cleft%28%5Chat%7B%5Cmathbf%7BX%7D%7D_%7Bj%7D%5E%7Bi%7D%5Cright%29%5Cright%29%5Cright%29"" alt=""\hat{\mathbf{X}}_{j}^{\prime i}=L^{\prime}\left(M H\left(L\left(\hat{\mathbf{X}}_{j}^{i}\right), L\left(\hat{\mathbf{X}}_{j}^{i}\right), L\left(\hat{\mathbf{X}}_{j}^{i}\right)\right)\right)"" eeimg=""1""> </p><p><img src=""https://www.zhihu.com/equation?tex=%5Cmathbf%7BO%7D%3D%5Cmax+_%7B-%7D%5Coperatorname%7Bpool%7D%5Cleft%28%5Cmathbf%7BR%7D%5E%7Bi%7D%5Cright%29"" alt=""\mathbf{O}=\max _{-}\operatorname{pool}\left(\mathbf{R}^{i}\right)"" eeimg=""1""> </p><p><img src=""https://www.zhihu.com/equation?tex=p%3D%5Coperatorname%7Bsoftmax%7D%5Cleft%28%5Cmathbf%7BO%7D+W_%7B3%7D%2Bb_%7B3%7D%5Cright%29"" alt=""p=\operatorname{softmax}\left(\mathbf{O} W_{3}+b_{3}\right)"" eeimg=""1""> </p><a href=""http://link.zhihu.com/?target=https%3A//github.com/zhongpeixiang/KET"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic2.zhimg.com/v2-1004f7e81a8bf66bb8bb636a87dc7825_ipico.jpg"" data-image-width=""400"" data-image-height=""400"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">zhongpeixiang/KET</a><p>————————————————————————————————————————————————————————————————————————————</p><p>看完以后是不是发现大佬们向模型中整合知识的技巧五花八门，但是你再仔细一点会看到大家都有相同一个关键点:**ATTENTION**，哈哈感觉又回到了 attention is all you need。下一篇会也是打算介绍知识融入模型的内容，但重点是会在分析而不是介绍模型~</p><p><br></p><p>更多关于NLP的干货，欢迎关注<b>微信公众号NewBeeNLP</b>一起交流！</p>",301,7,,kaiyuan,https://api.zhihu.com/people/043cbfa23e392c322cdcae163b8673d5,赞同了文章
1572764900194,关于ELMo你不知道的一些细节,,,,,https://api.zhihu.com/articles/89894807,,,,1572764900,,"<p>        今年四月份做了关于elmo复现和微调方面的工作。近期在内部和凤巢做了两次关于elmo的分享，感觉大家对这个模型较为陌生，发现其中有些细节和经验值得拿出来说一说，希望对你会有所帮助。</p><p>         ELMo全称Embeddings from Language Models，是2018年allen nlp的发布的大规模语义模型，paper为《Deep contextualized word representations》。</p><p>        既然有Bert为什么要讲ELMo？个人认为，其实ELMO微调方式有一定创新性，给人以启发。</p><p>       以下是根据个人经验梳理出来的你值得关注的一些细节。                                       </p><p><b>细节一：elmo的网络结构是双向双层的lstm,如何实现双向的lstm的呢？</b></p><p><b>       与Bert在预训练目标中使用masked language model来实现双向不同，ELMo的双向概念实际是在网络结构中体现的。</b>输入的embedding通过lstm的hidden state作为正向输出，embedding做reverse后的结果再通过lstm的hidden state反向输出，正向输出与反向输出做concat。最后输出实际是个lauguage model，基于前面的词计算下一个词的概率。</p><p><b>细节二：与Bert的相比，ELMo微调是如何实现的？</b></p><p><b>       ELMo的微调从严格意义上来说，不是真正的微调，预训练网络结果是fix的。</b> 整体来说，是把句子输入到预训练网络的embedding，与下游任务word embedding做concat，concat的结果整体作为下游NLP任务的输出。图为上游ELMO网络迁移到下游阅读理解bidaf网络中。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-b98c4b157e1f8829da7203da227320ba_b.jpg"" data-size=""normal"" data-rawwidth=""2084"" data-rawheight=""850"" class=""origin_image zh-lightbox-thumb"" width=""2084"" data-original=""https://pic1.zhimg.com/v2-b98c4b157e1f8829da7203da227320ba_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2084'%20height='850'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""2084"" data-rawheight=""850"" class=""origin_image zh-lightbox-thumb lazy"" width=""2084"" data-original=""https://pic1.zhimg.com/v2-b98c4b157e1f8829da7203da227320ba_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-b98c4b157e1f8829da7203da227320ba_b.jpg""><figcaption>ELMo迁移到下游NLP任务</figcaption></figure><p><b> 细节三:ELMo的语义表示输出是如何处理的？</b></p><p><b>ELMo</b>的语义表示输出计算公式如下：</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-273444a6fbc4c5d7286153197bc0e33e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""580"" data-rawheight=""186"" class=""origin_image zh-lightbox-thumb"" width=""580"" data-original=""https://pic1.zhimg.com/v2-273444a6fbc4c5d7286153197bc0e33e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='580'%20height='186'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""580"" data-rawheight=""186"" class=""origin_image zh-lightbox-thumb lazy"" width=""580"" data-original=""https://pic1.zhimg.com/v2-273444a6fbc4c5d7286153197bc0e33e_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-273444a6fbc4c5d7286153197bc0e33e_b.jpg""></figure><p><br></p><p>其中，hk 代表三层的输出（分别是embedding层，第一层lstm，第二层lstm），与W相乘的权重参数， 通过 个softmax得到  , 权重参数W加入 L2 正则，防止拟合。</p><p>r是整体的缩放因 ，为的是与下游的任务 embedding概率分布统一 。</p><p>代码里还有个小技巧，word embeding层是256维，所以他把相同word embedding做个concat，与lstm输出的516维统一起来。</p><p><b>从这些细节处理可以看出ELMo在微调阶段的处理与Bert有挺大的不同。</b></p><p><b>细节四：elmo适合哪些下游NLP 任务？ </b></p><p><b> ELMo在短本任务上表现好。</b>ELMo迁移到下游网络中， 一个是答案较短的数据集，提升有3-4个点， 一个答案较长的数据集，提升只有0.5 左右。在实验中，我们对比过词法分析和阅读理解任务，其在词法分析效果好于阅读理解。</p><p><b>细节五：elmo还有哪些值得注意的参数细节？</b></p><p><b>1. </b>dropout的处理方式：训练时候设置为0.5，为防止过拟合。infer阶段dropout设置为0。</p><p>2.r值对微调阶段影响很大。r值对语义表示的输出调节与下游NLP任务的阈值有较大帮助。</p><p>3.No L2 和 L2效果差不多。elmo的语义表示输出公式中权重参数W中加入了L2正则，从实验结果来，没有L2正则对结果影响不大，猜想可能是dropout的设置过大，导致L2对结果不产生影响。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-f5877764d401a70afbe596b6b89f7995_b.jpg"" data-size=""normal"" data-rawwidth=""992"" data-rawheight=""618"" class=""origin_image zh-lightbox-thumb"" width=""992"" data-original=""https://pic3.zhimg.com/v2-f5877764d401a70afbe596b6b89f7995_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='992'%20height='618'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""992"" data-rawheight=""618"" class=""origin_image zh-lightbox-thumb lazy"" width=""992"" data-original=""https://pic3.zhimg.com/v2-f5877764d401a70afbe596b6b89f7995_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-f5877764d401a70afbe596b6b89f7995_b.jpg""><figcaption>图为ELMo迁移到下游网络的结果，红色的baseline为百度词法分析LAC。</figcaption></figure><p><b>细节六: 如何训练自己的elmo中文预训练模型？</b></p><p>      准备约3G的中文文档数据。GPU：8卡GPU，显存大于22GB，调节batch_size，适合于显存大小，最大限度利用显存资源。训练时间约为一周，7天。</p><p><b>细节七：ELMO微调训练周期长的下游任务，如何在较短时间看是否靠谱？</b></p><p>      有个技巧是把finetune阶段dropout往大了调，使其快速过拟合。看峰值是否符合预期。</p>",24,15,,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,发表了文章
1572525725021,语义表示模型新方向《DistillBert》,,,,,https://api.zhihu.com/articles/89522799,,,,1572525725,,"<p>        从应用落地的角度来说，bert虽然效果好，但有一个短板就是预训练模型太大，预测时间在平均在300ms以上（一条数据），无法满足业务需求。知识蒸馏是在较低成本下有效提升预测速度的方法。最近在看知识蒸馏方面的内容，对《DistillBert》做个简单的介绍。</p><h2> 提纲</h2><ol><li>Bert后演化的趋势</li><li>知识蒸馏基本原理</li><li>《DistillBert》详解</li><li> 后话</li></ol><h2>一、Bert后演化的趋势</h2><p>Bert后，语义表示的基本框架已确定，后续大多模型以提升精度、提升速度来做。基本以知识蒸馏、提升算力、多任务学习、网络结构优化四个方向来做。</p><p><i>如何提升速度?</i></p><ol><li>invida发布transformer op，底层算子做fuse。</li><li>知识蒸馏,以distillBert和tinyBert为代表。</li><li>神经网络优化技巧。prune来裁剪多余的网络节点，混合精度（fp32和fp16混合来降低计算精度从而实现速度的提升）</li></ol><p><i>如何提升精度？</i></p><ol><li>增强算力。roberta</li><li>改进网络。xlnet，利用transformer-xl。</li><li>多任务学习（ensemble）。微软发布的mk-dnn</li></ol><h2>二、知识蒸馏的基本原理</h2><p>    知识蒸馏是从算法层面提速的有效方式，是趋势之一。知识蒸馏从hinton大神14年《Distilling the Knowledge in a Neural Network》这篇paper而来。</p><p>    定义两个网络，一个teacher model，一个student model。teacher model是预训练出来的大模型，teacher model eval结果出来的softlabel作为student model学习的一部分。student model的学习目标由soft label和hard label组成。</p><p>    其中有个核心的问题，为什么要用soft label呢？因为作者认为softlabel中包含有hard label中没有信息，也就是样本的概率信息，可以达到泛化的效果。</p><p>    细节参考这篇博文：<a href=""http://link.zhihu.com/?target=https%3A//blog.csdn.net/nature553863/article/details/80568658"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">blog.csdn.net/nature553</span><span class=""invisible"">863/article/details/80568658</span><span class=""ellipsis""></span></a></p><h2>三、DistillBert</h2><ol><li><b>DistillBert的网络结构：</b></li></ol><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-e6e61c8b530ea617f52516333dca7a53_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1802"" data-rawheight=""1080"" class=""origin_image zh-lightbox-thumb"" width=""1802"" data-original=""https://pic1.zhimg.com/v2-e6e61c8b530ea617f52516333dca7a53_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1802'%20height='1080'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1802"" data-rawheight=""1080"" class=""origin_image zh-lightbox-thumb lazy"" width=""1802"" data-original=""https://pic1.zhimg.com/v2-e6e61c8b530ea617f52516333dca7a53_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-e6e61c8b530ea617f52516333dca7a53_b.jpg""></figure><p><br></p><p>student model的网络结果与teacher model也就是bert的网络结构基本一致。主要包含如下改动：</p><ol><li>每2层中去掉一层。。作者调研后结果是隐藏层维度的变化比层数的变化对计算性能的影响较小，所以只改变了层数，把计算层数减小到原来的一半。</li><li>去掉了token type embedding和pooler。</li><li>每一层加了初始化，每一层的初始化为teacher model的参数。</li></ol><p><b>2. 三个损失函数：</b></p><p>（1）Lce损失函数</p><p>      Lce损失函数为Teacher model的soft label的损失函数，Teacher model的logits ti/T(T 为温度),通过softmax计算输出得到teacher的概率分布，与student model logits si/T(T为温度)，通过softmax计算输出得到student的概率分布，最后计算两个概率分布的KL散度。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-6293fec83177e30113f23b00f5b4632f_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""608"" data-rawheight=""62"" class=""origin_image zh-lightbox-thumb"" width=""608"" data-original=""https://pic2.zhimg.com/v2-6293fec83177e30113f23b00f5b4632f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='608'%20height='62'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""608"" data-rawheight=""62"" class=""origin_image zh-lightbox-thumb lazy"" width=""608"" data-original=""https://pic2.zhimg.com/v2-6293fec83177e30113f23b00f5b4632f_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-6293fec83177e30113f23b00f5b4632f_b.jpg""></figure><p>（2）Lmlm损失函数</p><p>      Lmlm损失函数为hard label的损失函数，是bert 的masked language model的损失函数。</p><p>（3）Lcos损失函数</p><p>      计算teacher hidden state和student hidden state的余弦相似度。官方代码用的是：nn.CosineEmbeddingLoss。</p><p><i>整体计算公式为：    </i></p><p><b><i>Loss= 5.0*Lce+2.0* Lmlm+1.0* Lcos</i></b></p><p><b>3. 参数配置</b></p><p>training阶段:计算8个卡，16GB，V100的GPU机器，90个小时</p><p>性能： DistilBERT 比Bert快71%，训练参数为207 MB 。</p><figure data-size=""small""><noscript><img src=""https://pic1.zhimg.com/v2-3ee1f7b779e5af4cefcd146c05eb6fd2_b.jpg"" data-caption="""" data-size=""small"" data-rawwidth=""646"" data-rawheight=""308"" class=""origin_image zh-lightbox-thumb"" width=""646"" data-original=""https://pic1.zhimg.com/v2-3ee1f7b779e5af4cefcd146c05eb6fd2_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='646'%20height='308'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""small"" data-rawwidth=""646"" data-rawheight=""308"" class=""origin_image zh-lightbox-thumb lazy"" width=""646"" data-original=""https://pic1.zhimg.com/v2-3ee1f7b779e5af4cefcd146c05eb6fd2_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-3ee1f7b779e5af4cefcd146c05eb6fd2_b.jpg""></figure><h2><b>四、实验结果</b></h2><p><b>DistillBert在GLUE数据集上的表现</b></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-2bd9a4caae525d52d71d3f80938739b1_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1302"" data-rawheight=""204"" class=""origin_image zh-lightbox-thumb"" width=""1302"" data-original=""https://pic3.zhimg.com/v2-2bd9a4caae525d52d71d3f80938739b1_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1302'%20height='204'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1302"" data-rawheight=""204"" class=""origin_image zh-lightbox-thumb lazy"" width=""1302"" data-original=""https://pic3.zhimg.com/v2-2bd9a4caae525d52d71d3f80938739b1_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-2bd9a4caae525d52d71d3f80938739b1_b.jpg""></figure><p><b>下图为Ablation test的结果，可以看出Lce、Lcos、参数初始化为结果影响较大。</b></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-4df817bbf6d6341729a177171fcf6329_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1330"" data-rawheight=""306"" class=""origin_image zh-lightbox-thumb"" width=""1330"" data-original=""https://pic1.zhimg.com/v2-4df817bbf6d6341729a177171fcf6329_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1330'%20height='306'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1330"" data-rawheight=""306"" class=""origin_image zh-lightbox-thumb lazy"" width=""1330"" data-original=""https://pic1.zhimg.com/v2-4df817bbf6d6341729a177171fcf6329_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-4df817bbf6d6341729a177171fcf6329_b.jpg""></figure><p>五、后话</p><p>     知识蒸馏本质是什么？<b>  个人理解，其实知识蒸馏实际相当于引入先验概率（</b>prior knowledge)， soft label即是网络输入的先验概率，soft label与真实世界的事物类似，呈各种概率分布。</p><p></p>",31,3,,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,发表了文章
1568604459878,[论文笔记]Poincaré Embeddings & Hierarchical Represent,,,,,https://api.zhihu.com/articles/68104722,,,,1568604459,,"<h2>Poincare embedding</h2><h3><b>1. 本文目的</b></h3><p>本文要解决的问题是表示学习中表示方法的改进</p><h3>2. 先行研究及其问题</h3><h3>2.1 简述先行研究</h3><p>首先, 我们知道表示学习是用来学习大量离散符号数据表征的方法. 包括, 文本, 图像还有知识图谱. </p><p>一般的方法都是将符号编码到一个欧几里得空间中, 然后用embedding space中的相似度量(多是内积)等等去度量符号之间的语义. 但是这样的方法有一个缺点:</p><h3>2.2 一个普遍的缺点</h3><p>​   <i>此方法对于复杂模式的表示能力被embedding space的维度限制</i></p><p>尤其是对于图的表征学习而言尤为严重, 该文章给的一个结论是:</p><p>​   <i>到目前为止还没有一种方法可以在没有任何信息损失的情况下表示全部的图的信息.</i></p><h3>2.3 具体要解决的问题</h3><p>针对这个问题, 这个研究在几个具有大量复杂信息的符号数据上进行了测试, 这里针对的是含有 power-distribution 的几种数据:</p><ul><li>自然语言</li><li>scale-free network</li></ul><p>这里为什么要针对具有 power-distribution 的数据呢, 因为这些数据是具有层级性的, 这是被先行研究所证明的. </p><h3>3. 提出的方法</h3><h3>3.1 大概的想法</h3><p><b>在hyperbolic空间而非欧几里得空间去编码知识</b>. (双曲空间的介绍见最后)</p><p>hyperbolic空间具有天然的优势去编码树结构的数据，或者说，双曲空间本身就是一个 continous version of tree. 这个研究建立在了一个被广泛学习的双曲空间模型 - poincare ball model 上. 已经有针对这个模型的优化算法. </p><p>这个论文借用了对regular tree 的编码来对欧式空间以及双曲空间能力的异同进行了诠释. </p><p><b>hyperbolic space</b> :对于 hyperbolic space 而已, 一个有限的regular tree是可以通过二维双曲空间完全表示的, 即, 设 r 为点距中心的距离, 当 $r=l$  时, 认为该点对应着 l 所对应点等级. 而 $r&lt;l$ 时, 认为该点是更高等级的点. 反之更低. 双曲空间中的ball的周长与面积的计算分别是 arsin 和 arcos  都是呈指数上升的, 因此适合我们的模型. </p><p><b>Euclidean space</b> ：在这个空间中, ball的周长和面积分别随着 r 线性以及平方级的拓展, 速度太慢. 因此只有通过增加维度来提升表征能力. 但是这样会带来计算消耗大以及易过拟合的问题. </p><h3>3.2 具体做法 - poincare embedding</h3><h3>1). Poincaré Embeddings</h3><p>在讲解 这个 embedding 方法之前, 我们需要了解的是, 我们的目的是什么:</p><ul><li>上面提到了, 欧式空间中表达能力不足必须加维度的问题, 对应的, 我们希望学到一个能够介绍计算力以及尽量低维的表达所有信息的模型. </li><li>我们希望在新的模型中加入一个新的insight去可以固有的表达层级性结构. </li></ul><p>因此为了达到这两个目的, 论文采用了 poincare ball 模型. </p><h3>2). Poincare ball model</h3><p>本文中采用双曲几何中一个被广泛的模型 - poincare ball model 来进行表征学习. </p><p>下面是 poincare ball model 的示意图. </p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-4a32053dc02ea31c4f8d533295afebb3_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""362"" data-rawheight=""350"" class=""content_image"" width=""362""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='362'%20height='350'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""362"" data-rawheight=""350"" class=""content_image lazy"" width=""362"" data-actualsrc=""https://pic2.zhimg.com/v2-4a32053dc02ea31c4f8d533295afebb3_b.jpg""></figure><p>这个图是嵌在欧式空间下的poincare ball 图示, 需要注意的是, 这个图中的每两个点之间的线代表的长度都是固定的. 也就是说, 离中心越远, 单位欧几里得空间的线段所代表的长度越长. 即, 随着向圆周边际的拓展, 空间的表现能力越来越强, 但是其级别会越来越低. </p><p>这个模型是有很多现成的优化方法并且可以实现高速化和并行化. </p><h3>定义</h3><ul><li><b>黎曼张量</b></li></ul><p>该模型的严谨定义论文中并没有给出, 但是给出了几个关键的性质:</p><p>Poincare ball 的 黎曼张量是 :</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-38d38333d2361334624ab89d48be3753_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""300"" data-rawheight=""89"" class=""content_image"" width=""300""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='300'%20height='89'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""300"" data-rawheight=""89"" class=""content_image lazy"" width=""300"" data-actualsrc=""https://pic3.zhimg.com/v2-38d38333d2361334624ab89d48be3753_b.jpg""></figure><p>黎曼度量是什么呢? 简单来说, 就是距离的函数的导数. (实际上不是, 但是可以简单这么理解.) </p><p>这里我没有完全理解这个张量的具体公式, 但是大意是, 这个张量可以用来进行黎曼几何空间的微分和欧式空间中的微分值的转换, 换而言之, 就一种放缩函数. 和线性代数中的特征值有些像. 不过线性代数中的特征值代表的是轴之间的放缩, 而这里的黎曼张量对于每一个点都有一个不同的放缩值. 并且这个放缩值随着 x 光滑的变化. </p><ul><li><b>如何理解黎曼张量</b></li></ul><p>这个poincare ball的黎曼度量是怎么算的我不知道, 但是我们可以看看欧式二维空间中一维流行(曲线)的黎曼度量是如何得到的, 假设曲线如下:</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-c1d762e713ff8ddc089c3ea7a187775b_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""355"" data-rawheight=""142"" class=""content_image"" width=""355""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='355'%20height='142'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""355"" data-rawheight=""142"" class=""content_image lazy"" width=""355"" data-actualsrc=""https://pic1.zhimg.com/v2-c1d762e713ff8ddc089c3ea7a187775b_b.jpg""></figure><p>假设曲线在二维欧式空间的函数表示为: <img src=""https://www.zhihu.com/equation?tex=x%3DC_x%28t%29%2C+y%3DC_y%28t%29"" alt=""x=C_x(t), y=C_y(t)"" eeimg=""1""> . </p><p>那么曲线从 <img src=""https://www.zhihu.com/equation?tex=t%5Cin%5Ba%2Cb%5D+"" alt=""t\in[a,b] "" eeimg=""1""> 的距离就是: </p><p><img src=""https://www.zhihu.com/equation?tex=d%28a%2Cb%29+%3D+%5Cint_a%5Eb+%5Csqrt%7B%28%5Cfrac%7B%5Cdelta+C_x%28t%29%7D%7B%5Cdelta+t%7D%29%5E2+%2B%28%5Cfrac%7B%5Cdelta+C_x%28t%29%7D%7B%5Cdelta+t%7D%29%5E2%7D%5C%5C%3D%5Cint_a%5Eb+%5Csqrt%7B%28%5Cfrac%7B%5Cdelta+C_x%28t%29%7D%7B%5Cdelta+t%7D+%5C+%5C+%5C+%5C+%5Cfrac%7B%5Cdelta+C_x%28t%29%7D%7B%5Cdelta+t%7D%29++++++%5Cbegin%7Bpmatrix%7D+++++1+%26+0+%5C%5C+++++0+%26+1+%5C+++++%5Cend%7Bpmatrix%7D+%28%5Cfrac%7B%5Cdelta+C_x%28t%29%7D%7B%5Cdelta+t%7D+%5C+%5C+%5C+%5C+%5Cfrac%7B%5Cdelta+C_x%28t%29%7D%7B%5Cdelta+t%7D%29%5ET+%7D"" alt=""d(a,b) = \int_a^b \sqrt{(\frac{\delta C_x(t)}{\delta t})^2 +(\frac{\delta C_x(t)}{\delta t})^2}\\=\int_a^b \sqrt{(\frac{\delta C_x(t)}{\delta t} \ \ \ \ \frac{\delta C_x(t)}{\delta t})      \begin{pmatrix}     1 &amp; 0 \\     0 &amp; 1 \     \end{pmatrix} (\frac{\delta C_x(t)}{\delta t} \ \ \ \ \frac{\delta C_x(t)}{\delta t})^T }"" eeimg=""1""> </p><p>在这个里面我们可以看到有一个二维单位矩阵. 这个就是欧式空间的黎曼度量 $g^E$. </p><p>Poincare ball 的黎曼度量也就是将 $g^x$ 换成了 $g^E$. 最后的距离计算下来就是: </p><p><img src=""https://www.zhihu.com/equation?tex=d%28a%2Cb%29+%3D+%5Cint_a%5Eb+%5Csqrt%7B%5Cfrac%7B4%7D%7B%281-%7C%7Ct%7C%7C%5E2%29%5E2%7D%7Ddt%5C%5C%3D%5Cint_a%5Eb+%5Cfrac%7B2%7D%7B1-%7C%7Ct%7C%7C%5E2%7Ddt+"" alt=""d(a,b) = \int_a^b \sqrt{\frac{4}{(1-||t||^2)^2}}dt\\=\int_a^b \frac{2}{1-||t||^2}dt "" eeimg=""1""> </p><p>可以将下面的距离函数进行求导试一下, 其导数就是这个 <img src=""https://www.zhihu.com/equation?tex=%5Cfrac%7B2%7D%7B1-%7C%7Ct%7C%7C%5E2%7D"" alt=""\frac{2}{1-||t||^2}"" eeimg=""1""> . 这样整个就可以理解了. </p><ul><li><b>距离函数</b></li></ul><p>在poincare ball 中的距离函数定义如下(用欧式空间的坐标表示Poincare ball中的距离):</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-4291ef6ff40f044b2e3f580a2a97a9be_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""595"" data-rawheight=""96"" class=""origin_image zh-lightbox-thumb"" width=""595"" data-original=""https://pic1.zhimg.com/v2-4291ef6ff40f044b2e3f580a2a97a9be_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='595'%20height='96'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""595"" data-rawheight=""96"" class=""origin_image zh-lightbox-thumb lazy"" width=""595"" data-original=""https://pic1.zhimg.com/v2-4291ef6ff40f044b2e3f580a2a97a9be_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-4291ef6ff40f044b2e3f580a2a97a9be_b.jpg""></figure><p>从这个距离函数中我们还可以观察出一些性质. 即当 $||u||^2$或$||v||^2$ 趋近于1时, 距离会变得无限大. 因此我们就可以在一个直径为1的圆内, 拥有巨大的表现力. </p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-fb0dbb190388c74e01d17825dfdbb551_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""360"" data-rawheight=""358"" class=""content_image"" width=""360""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='360'%20height='358'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""360"" data-rawheight=""358"" class=""content_image lazy"" width=""360"" data-actualsrc=""https://pic2.zhimg.com/v2-fb0dbb190388c74e01d17825dfdbb551_b.jpg""></figure><p>这个是距离函数的图像, 从这个图中可以看出, 在两边在欧式空间中相距固定的情况下, <img src=""https://www.zhihu.com/equation?tex=%7C%7Cv%7C%7C%5E2"" alt=""||v||^2"" eeimg=""1""> 的值越大, 两点在Poincare space中距离越远.</p><ul><li><b>表现层级性</b></li></ul><p>为什么说这个可以表现出层级性呢? 下面进行解释:</p><p>通过将树的root 放在原点(自己学得的, 并不是有监督), 我们会发现, 相比于其他点, 在原点的点总的来说距离其他点更近一些. 为什么这么说呢,  我们可以在上面的圆图中以中心为一点画一个等边三角形. 按照前面介绍的性质, 我们可以知道, 连着原点的两条边的长度(poincare ball 度量)要比另外一条边长. 所以离root近的点天生比其他点占便宜; 相反地, 距离边界越近的点越是leaf node. 因此我们说poincare ball 具有固有的表示层级结构的能力. </p><p>而欧式空间不同的地方在于, 我们必须要通过词汇之间的相对位置才能得知词汇的层级性信息, 并且需要复杂的结构和很多数据才能在高位空间中学得这个能力. 因为增加维度之后, 欧式空间的embedding space具有更大的搜索空间. </p><p>而使用poincare ball 的原因就是利用这个固有的 层级性 特征去缩小的搜索空间, 并且保证了层级性的学习. </p><ul><li><b>Poincare disk -&gt; Poincare ball</b></li></ul><p>因为我一直在用 poincare ball, 可能读者没有注意. 实际上, 一直讨论的是对最理想的树 - regular tree 的建模. 然而实际的数据中, 更多的是交错复杂的树结构为了学习这种结构, 我们应该有高位的 poincare ball model 去表示. </p><p>另外的, 增加维度可以降低收敛的难度. </p><ul><li><b>最后的loss function</b></li></ul><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-915793f97f19cd2538f4e823520124b7_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""587"" data-rawheight=""65"" class=""origin_image zh-lightbox-thumb"" width=""587"" data-original=""https://pic4.zhimg.com/v2-915793f97f19cd2538f4e823520124b7_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='587'%20height='65'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""587"" data-rawheight=""65"" class=""origin_image zh-lightbox-thumb lazy"" width=""587"" data-original=""https://pic4.zhimg.com/v2-915793f97f19cd2538f4e823520124b7_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-915793f97f19cd2538f4e823520124b7_b.jpg""></figure><p>这里的 $\theta$ 是 d 维的欧式空间. </p><p>这里的 $\mathcal{L}(\Theta)$ 文中并没有给出具体的式子, 只说明其目的是使的语义相近的词汇的embedding在 Poincare ball space 中也有相近的距离. 因为这里是最抽象的损失函数, 所以没有给出具体的式子, 在后面的具体式子中会给出具体的公式. </p><h3>4 优化方法</h3><p>这里使用的方法是梯度下降法:</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-2e5af3c561d974fe397dbbc53ec1f9f3_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""338"" data-rawheight=""52"" class=""content_image"" width=""338""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='338'%20height='52'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""338"" data-rawheight=""52"" class=""content_image lazy"" width=""338"" data-actualsrc=""https://pic4.zhimg.com/v2-2e5af3c561d974fe397dbbc53ec1f9f3_b.jpg""></figure><p>在这个式子中, 我们需要两个东西:</p><ul><li> 损失函数在黎曼空间上的微分  <img src=""https://www.zhihu.com/equation?tex=%5Ctriangledown_R%5Cmathcal%7B%5Ctheta_t%7D"" alt=""\triangledown_R\mathcal{\theta_t}"" eeimg=""1""> <br> </li><li> 梯度下降函数 <img src=""https://www.zhihu.com/equation?tex=%5Cmathcal%7BR%7D_%7B%5Ctheta_t%7D"" alt=""\mathcal{R}_{\theta_t}"" eeimg=""1""> <br> </li></ul><h3>4.1 黎曼空间上的微分</h3><p>我们分两步去计算黎曼微分, 首先计算欧式空间微分, 再将欧式微分转换为黎曼积分:</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-5e09ff0924b72e99a348df5ba88970b3_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""257"" data-rawheight=""70"" class=""content_image"" width=""257""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='257'%20height='70'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""257"" data-rawheight=""70"" class=""content_image lazy"" width=""257"" data-actualsrc=""https://pic3.zhimg.com/v2-5e09ff0924b72e99a348df5ba88970b3_b.jpg""></figure><h3>1). 计算欧式空间微分</h3><p>右式的左式是根据具体的损失函数来求得, 我们假设这里是已知的. </p><p>右式的右式就是将距离函数进行求导:</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-9bc2b69e170f686a309a00a9de557dcd_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""481"" data-rawheight=""112"" class=""origin_image zh-lightbox-thumb"" width=""481"" data-original=""https://pic2.zhimg.com/v2-9bc2b69e170f686a309a00a9de557dcd_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='481'%20height='112'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""481"" data-rawheight=""112"" class=""origin_image zh-lightbox-thumb lazy"" width=""481"" data-original=""https://pic2.zhimg.com/v2-9bc2b69e170f686a309a00a9de557dcd_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-9bc2b69e170f686a309a00a9de557dcd_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-485150e3e81dccd1981634ac1ea1e62e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""660"" data-rawheight=""98"" class=""origin_image zh-lightbox-thumb"" width=""660"" data-original=""https://pic4.zhimg.com/v2-485150e3e81dccd1981634ac1ea1e62e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='660'%20height='98'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""660"" data-rawheight=""98"" class=""origin_image zh-lightbox-thumb lazy"" width=""660"" data-original=""https://pic4.zhimg.com/v2-485150e3e81dccd1981634ac1ea1e62e_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-485150e3e81dccd1981634ac1ea1e62e_b.jpg""></figure><h3>2. 将欧式微分转换为黎曼积分</h3><p>这里就稍微有些复杂. </p><p>要理解这里, 首先要知道两个知识点:</p><ul><li>Poincaré ball 是 hyperbolic space 的 共形模型(Conformal model)</li></ul><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-4e8a7d04c9e63d9ec10c776b23bcdd88_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""258"" data-rawheight=""80"" class=""content_image"" width=""258""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='258'%20height='80'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""258"" data-rawheight=""80"" class=""content_image lazy"" width=""258"" data-actualsrc=""https://pic3.zhimg.com/v2-4e8a7d04c9e63d9ec10c776b23bcdd88_b.jpg""></figure><p>也就是, 邻接向量的角度不会根据模型的转换而改变, 但是向量的长度会变. </p><ul><li>第二个是, metric tensor. 在上面提到的 Riemannian metric tensor 就是用来解决这个问题的, 通过这个 metric tensor 的倒数 <img src=""https://www.zhihu.com/equation?tex=g_%7B%5Ctheta%7D%5E%7B-1%7D"" alt=""g_{\theta}^{-1}"" eeimg=""1""> 可以对长度进行放缩:</li></ul><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-0179f4c29c8233cc34b74ef8a77768f5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""283"" data-rawheight=""87"" class=""content_image"" width=""283""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='283'%20height='87'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""283"" data-rawheight=""87"" class=""content_image lazy"" width=""283"" data-actualsrc=""https://pic3.zhimg.com/v2-0179f4c29c8233cc34b74ef8a77768f5_b.jpg""></figure><p>最后的结果黎曼微分就是:</p><h3>4.2 梯度下降函数</h3><p>这个函数要拥有将向量限制在ball内的能力, 因此采用如下函数:</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-b2cdc0c9286b54476f04c031e9f42cd9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""438"" data-rawheight=""82"" class=""origin_image zh-lightbox-thumb"" width=""438"" data-original=""https://pic3.zhimg.com/v2-b2cdc0c9286b54476f04c031e9f42cd9_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='438'%20height='82'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""438"" data-rawheight=""82"" class=""origin_image zh-lightbox-thumb lazy"" width=""438"" data-original=""https://pic3.zhimg.com/v2-b2cdc0c9286b54476f04c031e9f42cd9_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-b2cdc0c9286b54476f04c031e9f42cd9_b.jpg""></figure><h3>4.3 最后的优化函数</h3><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-74857c35c17174e34be0136512768fa6_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""499"" data-rawheight=""82"" class=""origin_image zh-lightbox-thumb"" width=""499"" data-original=""https://pic4.zhimg.com/v2-74857c35c17174e34be0136512768fa6_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='499'%20height='82'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""499"" data-rawheight=""82"" class=""origin_image zh-lightbox-thumb lazy"" width=""499"" data-original=""https://pic4.zhimg.com/v2-74857c35c17174e34be0136512768fa6_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-74857c35c17174e34be0136512768fa6_b.jpg""></figure><h3>4.4 Training Details</h3><p>研究表明, 初始化向量对结果有影响, 有两个方法可以用</p><ul><li>Uniform distribution</li><li>Angular layout: 是初始向量的角度具有一定的特征. </li></ul><h3>5. 个人看法</h3><p>这个论文是组上开会时讲的, 大家讨论后一致认为这个现在只是个玩具. </p><p>不仅是这篇论文, 这个系列方法的实用性都受到了怀疑. 看了几个类似方法的后续研究. 他们的实验都是在具有相当结构性的数据上进行实现的. Wordnet 或者 社交网络. </p><p>这些数据集都有极强的层次性假设, 而对于自然语言来说感觉实用性很低. </p><p>不过纯属个人看法, 有不同意见的还请告知. </p><h3>6. 补充</h3><h3>hyperbolic空间: 是指双曲空间, 相比较于欧几里得空间的不同是 其常曲率位负数. 而我们所常见的欧几里得空间的曲率为0. 下面我们从定理和图像两个方面来体会双曲空间和欧几里得空间的异同  定理  欧式空间有五大公理:  </h3><ol><li>从一点到另一点可以引一条直线</li><li>任何线段都能无限伸展为一条直线</li><li>给定任意线段，可以以其一个端点作为圆心，该线段作为半径作一个圆。</li><li>所有直角都相等。</li><li>两条直线都与第三条直线相交，并且在同一边的内角之和小于两个直角，则这两条直线在这一边必定相交。  (平行公设)</li></ol><p> 而双曲空间与欧几里得空间不一样的地方仅仅在第五个公理: </p><p>存在两条相交的直线使得, 两条直线与第三条直线相交，并且在同一边的内角之和等于两个直角</p><p>图像<br>这里是从三维空间欧几里得空间去看二维欧几里得和二维双曲空间. 最左边是欧几里得几何下的二维平面, 最右边是双曲几何. </p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-4180574e5c6beafaac4cdc14cd868bd0_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""378"" data-rawheight=""133"" class=""content_image"" width=""378""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='378'%20height='133'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""378"" data-rawheight=""133"" class=""content_image lazy"" width=""378"" data-actualsrc=""https://pic1.zhimg.com/v2-4180574e5c6beafaac4cdc14cd868bd0_b.jpg""></figure><p>另外文中的几个实验都很有意思, 可以看一下.  </p>",61,10,,赵来福,https://api.zhihu.com/people/f02b5833df7bfd190f6b68182e85884c,赞同了文章
1554960874803,为什么粉墨组合（BLACKPINK）自带高级感？,贴心提示：题主提问是出于是blackpink的好感路人，喜欢她们舞台的感觉和私服的感觉，好奇这种感觉与其他的女团不同，绝对不是希望引起大家的互撕，也完全不想大家贬低别的女团小仙女们。blink（是怎么写没错吧 ）带有粉丝滤镜很正常，安利吐槽都很正常，大家都可以发表自己的想法，但是某些匿名的盆友，希望可以专注自家，不要太戾气啊 大家都和谐一点呀～最后非常感谢各位大佬的回答，我觉得都很棒呀，谢谢大家～狗年大吉～ 原问题：blackpink的舞台服装总是走“丑贵风”，搭配起来有时候也很迷，但成员却撑出了不一样的感觉。身材相貌，其他女团也不乏很多有颜有身材的，为什么其他女团没有这种感觉呢？是服装的问题还是什么其他问题呢？,201,2,1517,https://api.zhihu.com/questions/267127124,匿名用户,,1518541862,1554960874,1527422832,"YG家的小师妹，家族公主，所有的装备都是顶级的，前面有很多人举例说服装大牌在这里我就先不说这一点了，举个其他例子：老杨专门为她们定制的耳机<br><br><br>首先是成员JISOO的耳机，使用了 钛合金的材料，干净的设计和JISOO的性格十分相符，价格约为199万韩元（约11510元人民币）。<br>接下来是JENNIE的耳机，简单的贝壳上装饰有粉色的名字首大写字母“J”，价格为1999美元，约255万韩元（约14762元人民币）。<br>Rosé的耳机又是怎么样的呢？耳机两边选择了水晶来装饰，如果舞台上的灯光打过来，就会闪闪发亮，和Rosé 亮丽的美貌十分般配。<br><br>LISA的耳机和本人也非常相似，华丽的金色外壳，并且LISA的耳机和Rosé的耳机是同个品牌，为1500美元，约169万韩元（约9793元人民币）左右<figure><noscript><img src=""https://pic4.zhimg.com/v2-b4b117a4f9946d948a4b91a2a91737aa_b.jpg"" data-rawheight=""600"" data-rawwidth=""650"" class=""origin_image zh-lightbox-thumb"" width=""650"" data-original=""https://pic4.zhimg.com/v2-b4b117a4f9946d948a4b91a2a91737aa_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='650'%20height='600'&gt;&lt;/svg&gt;"" data-rawheight=""600"" data-rawwidth=""650"" class=""origin_image zh-lightbox-thumb lazy"" width=""650"" data-original=""https://pic4.zhimg.com/v2-b4b117a4f9946d948a4b91a2a91737aa_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-b4b117a4f9946d948a4b91a2a91737aa_b.jpg""></figure><br><br><figure><noscript><img src=""https://pic4.zhimg.com/v2-c5abf8f8af75b08781bb7ccb7d4ec46e_b.jpg"" data-rawheight=""600"" data-rawwidth=""650"" class=""origin_image zh-lightbox-thumb"" width=""650"" data-original=""https://pic4.zhimg.com/v2-c5abf8f8af75b08781bb7ccb7d4ec46e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='650'%20height='600'&gt;&lt;/svg&gt;"" data-rawheight=""600"" data-rawwidth=""650"" class=""origin_image zh-lightbox-thumb lazy"" width=""650"" data-original=""https://pic4.zhimg.com/v2-c5abf8f8af75b08781bb7ccb7d4ec46e_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-c5abf8f8af75b08781bb7ccb7d4ec46e_b.jpg""></figure><br><figure><noscript><img src=""https://pic3.zhimg.com/v2-a173417397664cbdf7f316906cbb6ffd_b.jpg"" data-rawheight=""600"" data-rawwidth=""650"" class=""origin_image zh-lightbox-thumb"" width=""650"" data-original=""https://pic3.zhimg.com/v2-a173417397664cbdf7f316906cbb6ffd_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='650'%20height='600'&gt;&lt;/svg&gt;"" data-rawheight=""600"" data-rawwidth=""650"" class=""origin_image zh-lightbox-thumb lazy"" width=""650"" data-original=""https://pic3.zhimg.com/v2-a173417397664cbdf7f316906cbb6ffd_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-a173417397664cbdf7f316906cbb6ffd_b.jpg""></figure>这么高逼格的装备在其他公司是不存在的～<br><figure><noscript><img src=""https://pic4.zhimg.com/v2-9665458671a2a345759e2161fea69138_b.jpg"" data-rawheight=""641"" data-rawwidth=""640"" class=""origin_image zh-lightbox-thumb"" width=""640"" data-original=""https://pic4.zhimg.com/v2-9665458671a2a345759e2161fea69138_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='640'%20height='641'&gt;&lt;/svg&gt;"" data-rawheight=""641"" data-rawwidth=""640"" class=""origin_image zh-lightbox-thumb lazy"" width=""640"" data-original=""https://pic4.zhimg.com/v2-9665458671a2a345759e2161fea69138_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-9665458671a2a345759e2161fea69138_b.jpg""></figure><br>-----------------补充-----------------<br>谁说粉墨的一直走丑贵风的！<br>本宝宝不服！<br>举个例子，2017SBS歌谣大战的服装就可以用完美来形容<figure><noscript><img src=""https://pic4.zhimg.com/v2-9bc4dd60b47bd1f333b6ae3ba19698bc_b.jpg"" data-rawheight=""887"" data-rawwidth=""720"" class=""origin_image zh-lightbox-thumb"" width=""720"" data-original=""https://pic4.zhimg.com/v2-9bc4dd60b47bd1f333b6ae3ba19698bc_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='720'%20height='887'&gt;&lt;/svg&gt;"" data-rawheight=""887"" data-rawwidth=""720"" class=""origin_image zh-lightbox-thumb lazy"" width=""720"" data-original=""https://pic4.zhimg.com/v2-9bc4dd60b47bd1f333b6ae3ba19698bc_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-9bc4dd60b47bd1f333b6ae3ba19698bc_b.jpg""></figure><br><figure><noscript><img src=""https://pic4.zhimg.com/v2-81740a23c4ea2889c2bbc9eab53731bb_b.jpg"" data-rawheight=""1046"" data-rawwidth=""720"" class=""origin_image zh-lightbox-thumb"" width=""720"" data-original=""https://pic4.zhimg.com/v2-81740a23c4ea2889c2bbc9eab53731bb_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='720'%20height='1046'&gt;&lt;/svg&gt;"" data-rawheight=""1046"" data-rawwidth=""720"" class=""origin_image zh-lightbox-thumb lazy"" width=""720"" data-original=""https://pic4.zhimg.com/v2-81740a23c4ea2889c2bbc9eab53731bb_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-81740a23c4ea2889c2bbc9eab53731bb_b.jpg""></figure><figure><noscript><img src=""https://pic2.zhimg.com/v2-e292c9b7cd391679e05c5bf91787b61e_b.jpg"" data-rawheight=""1033"" data-rawwidth=""720"" class=""origin_image zh-lightbox-thumb"" width=""720"" data-original=""https://pic2.zhimg.com/v2-e292c9b7cd391679e05c5bf91787b61e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='720'%20height='1033'&gt;&lt;/svg&gt;"" data-rawheight=""1033"" data-rawwidth=""720"" class=""origin_image zh-lightbox-thumb lazy"" width=""720"" data-original=""https://pic2.zhimg.com/v2-e292c9b7cd391679e05c5bf91787b61e_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-e292c9b7cd391679e05c5bf91787b61e_b.jpg""></figure><br><br><br>而且当晚的化妆品用的都是Chanel<br>尤其是妮妮的香奈儿口红 <br><br>还有红毯服装，全部名牌<br>价钱分别是<br>jisoo：2895  RMB<br>rose：3595  RMB<br>Lisa：5067  RMB<br>jennie：65470 RMB<br><br><figure><noscript><img src=""https://pic2.zhimg.com/v2-45d9dbb612600d692817e56e5bee59db_b.jpg"" data-rawheight=""476"" data-rawwidth=""640"" class=""origin_image zh-lightbox-thumb"" width=""640"" data-original=""https://pic2.zhimg.com/v2-45d9dbb612600d692817e56e5bee59db_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='640'%20height='476'&gt;&lt;/svg&gt;"" data-rawheight=""476"" data-rawwidth=""640"" class=""origin_image zh-lightbox-thumb lazy"" width=""640"" data-original=""https://pic2.zhimg.com/v2-45d9dbb612600d692817e56e5bee59db_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-45d9dbb612600d692817e56e5bee59db_b.jpg""></figure><br>还有最最重要的一点，那就是Blackpink的舞台表现力非常强，Lisa在台上的每一个表情都堪称完美，智妮大写加粗的稳！智秀太吸睛，彩英酥死人。吊打一票四代女团",1416,258,165,南韩大大,https://api.zhihu.com/people/6145546604d4f82c0b7f2951d6a30943,赞同了回答
1554343565042,如何自己从零实现一个神经网络?,不用tensorflow、caffe之类的东西，用C++、python等语言及其标准库，从零开始写一个有具体功能的神经网络？最好是处理图片、文字、音频的。,38,1,2086,https://api.zhihu.com/questions/314879954,r7tc,https://api.zhihu.com/people/1204c550cfe3f3a51ef1f9d41414aed9,1551882763,1554343565,1554113502,"<p>“我在网上看到过很多神经网络的实现方法，但这一篇是最简单、最清晰的。”</p><p>一位来自普林斯顿的华人小哥Victor Zhou，写了篇神经网络入门教程，在线代码网站Repl.it联合创始人Amjad Masad看完以后，给予如是评价。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-99e054b2f05f86cad74ca594309d0257_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""916"" data-rawheight=""208"" class=""origin_image zh-lightbox-thumb"" width=""916"" data-original=""https://pic3.zhimg.com/v2-99e054b2f05f86cad74ca594309d0257_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='916'%20height='208'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""916"" data-rawheight=""208"" class=""origin_image zh-lightbox-thumb lazy"" width=""916"" data-original=""https://pic3.zhimg.com/v2-99e054b2f05f86cad74ca594309d0257_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-99e054b2f05f86cad74ca594309d0257_b.jpg""></figure><p><br></p><p>这篇教程发布仅天时间，就在Hacker News论坛上收获了574赞。程序员们纷纷夸赞这篇文章的代码写得很好，变量名很规范，让人一目了然。</p><p>下面就让我们一起从零开始学习神经网络吧。</p><h2><b>实现方法</b></h2><h3><b>搭建基本模块——神经元</b></h3><p>在说神经网络之前，我们讨论一下<b>神经元</b>（Neurons），它是神经网络的基本单元。神经元先获得输入，然后执行某些数学运算后，再产生一个输出。比如一个2输入神经元的例子：</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-5550feac69724dcbc145574525ae42f6_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""350"" class=""origin_image zh-lightbox-thumb"" width=""600"" data-original=""https://pic2.zhimg.com/v2-5550feac69724dcbc145574525ae42f6_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='600'%20height='350'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""350"" class=""origin_image zh-lightbox-thumb lazy"" width=""600"" data-original=""https://pic2.zhimg.com/v2-5550feac69724dcbc145574525ae42f6_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-5550feac69724dcbc145574525ae42f6_b.jpg""></figure><p><br></p><p>在这个神经元中，输入总共经历了3步数学运算，</p><p>先将两个输入乘以<b>权重</b>（weight）：</p><p>x1→x1 × w1<br>x2→x2 × w2</p><p>把两个结果想加，再加上一个<b>偏置</b>（bias）：</p><p>（x1 × w1）+（x2 × w2）+ b</p><p>最后将它们经过<b>激活函数</b>（activation function）处理得到输出：</p><p>y = f(x1 × w1 + x2 × w2 + b)</p><p>激活函数的作用是将无限制的输入转换为可预测形式的输出。一种常用的激活函数是sigmoid函数：</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-7ac6165a27e42e2642d399272db1396c_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""721"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-7ac6165a27e42e2642d399272db1396c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='721'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""721"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic2.zhimg.com/v2-7ac6165a27e42e2642d399272db1396c_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-7ac6165a27e42e2642d399272db1396c_b.jpg""></figure><p><br></p><p>sigmoid函数的输出介于0和1，我们可以理解为它把 (−∞,+∞) 范围内的数压缩到 (0, 1)以内。正值越大输出越接近1，负向数值越大输出越接近0。</p><p>举个例子，上面神经元里的权重和偏置取如下数值：</p><p>w=[0,1]<br>b = 4</p><p>w=[0,1]是w1=0、w2=1的向量形式写法。给神经元一个输入x=[2,3]，可以用向量点积的形式把神经元的输出计算出来：</p><p>w·x+b =（x1 × w1）+（x2 × w2）+ b = 0×2+1×3+4=7<br>y=f(w⋅X+b)=f(7)=0.999</p><p>以上步骤的Python代码是：</p><div class=""highlight""><pre><code class=""language-python""><span></span><span class=""kn"">import</span> <span class=""nn"">numpy</span> <span class=""kn"">as</span> <span class=""nn"">np</span>

<span class=""k"">def</span> <span class=""nf"">sigmoid</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">):</span>
  <span class=""c1""># Our activation function: f(x) = 1 / (1 + e^(-x))</span>
  <span class=""k"">return</span> <span class=""mi"">1</span> <span class=""o"">/</span> <span class=""p"">(</span><span class=""mi"">1</span> <span class=""o"">+</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">exp</span><span class=""p"">(</span><span class=""o"">-</span><span class=""n"">x</span><span class=""p"">))</span>

<span class=""k"">class</span> <span class=""nc"">Neuron</span><span class=""p"">:</span>
  <span class=""k"">def</span> <span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">weights</span><span class=""p"">,</span> <span class=""n"">bias</span><span class=""p"">):</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">weights</span> <span class=""o"">=</span> <span class=""n"">weights</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">bias</span> <span class=""o"">=</span> <span class=""n"">bias</span>

  <span class=""k"">def</span> <span class=""nf"">feedforward</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">inputs</span><span class=""p"">):</span>
    <span class=""c1""># Weight inputs, add bias, then use the activation function</span>
    <span class=""n"">total</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">dot</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">weights</span><span class=""p"">,</span> <span class=""n"">inputs</span><span class=""p"">)</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">bias</span>
    <span class=""k"">return</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""n"">total</span><span class=""p"">)</span>

<span class=""n"">weights</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">1</span><span class=""p"">])</span> <span class=""c1""># w1 = 0, w2 = 1</span>
<span class=""n"">bias</span> <span class=""o"">=</span> <span class=""mi"">4</span>                   <span class=""c1""># b = 4</span>
<span class=""n"">n</span> <span class=""o"">=</span> <span class=""n"">Neuron</span><span class=""p"">(</span><span class=""n"">weights</span><span class=""p"">,</span> <span class=""n"">bias</span><span class=""p"">)</span>

<span class=""n"">x</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""mi"">2</span><span class=""p"">,</span> <span class=""mi"">3</span><span class=""p"">])</span>       <span class=""c1""># x1 = 2, x2 = 3</span>
<span class=""k"">print</span><span class=""p"">(</span><span class=""n"">n</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">))</span>    <span class=""c1""># 0.9990889488055994</span>
</code></pre></div><p>我们在代码中调用了一个强大的Python数学函数库<b>NumPy</b>。<br></p><h3><b>搭建神经网络</b></h3><p>神经网络就是把一堆神经元连接在一起，下面是一个神经网络的简单举例：</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-6dd1407c23fe6950351fcbb47cb4ecdc_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""550"" data-rawheight=""250"" class=""origin_image zh-lightbox-thumb"" width=""550"" data-original=""https://pic3.zhimg.com/v2-6dd1407c23fe6950351fcbb47cb4ecdc_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='550'%20height='250'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""550"" data-rawheight=""250"" class=""origin_image zh-lightbox-thumb lazy"" width=""550"" data-original=""https://pic3.zhimg.com/v2-6dd1407c23fe6950351fcbb47cb4ecdc_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-6dd1407c23fe6950351fcbb47cb4ecdc_b.jpg""></figure><p><br></p><p>这个网络有2个输入、一个包含2个神经元的隐藏层（h1和h2）、包含1个神经元的输出层o1。<br></p><p>隐藏层是夹在输入输入层和输出层之间的部分，一个神经网络可以有多个隐藏层。</p><p>把神经元的输入向前传递获得输出的过程称为<b>前馈</b>（feedforward）。</p><p>我们假设上面的网络里所有神经元都具有相同的权重w=[0,1]和偏置b=0，激活函数都是sigmoid，那么我们会得到什么输出呢？</p><p>h1=h2=f(w⋅x+b)=f((0×2)+(1×3)+0)<br>=f(3)<br>=0.9526</p><p>o1=f(w⋅[h1,h2]+b)=f((0∗h1)+(1∗h2)+0)<br>=f(0.9526)<br>=0.7216</p><p>以下是实现代码：</p><div class=""highlight""><pre><code class=""language-python""><span></span><span class=""kn"">import</span> <span class=""nn"">numpy</span> <span class=""kn"">as</span> <span class=""nn"">np</span>

<span class=""c1""># ... code from previous section here</span>

<span class=""k"">class</span> <span class=""nc"">OurNeuralNetwork</span><span class=""p"">:</span>
  <span class=""sd"">'''</span>
<span class=""sd"">  A neural network with:</span>
<span class=""sd"">    - 2 inputs</span>
<span class=""sd"">    - a hidden layer with 2 neurons (h1, h2)</span>
<span class=""sd"">    - an output layer with 1 neuron (o1)</span>
<span class=""sd"">  Each neuron has the same weights and bias:</span>
<span class=""sd"">    - w = [0, 1]</span>
<span class=""sd"">    - b = 0</span>
<span class=""sd"">  '''</span>
  <span class=""k"">def</span> <span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>
    <span class=""n"">weights</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">1</span><span class=""p"">])</span>
    <span class=""n"">bias</span> <span class=""o"">=</span> <span class=""mi"">0</span>

    <span class=""c1""># The Neuron class here is from the previous section</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">h1</span> <span class=""o"">=</span> <span class=""n"">Neuron</span><span class=""p"">(</span><span class=""n"">weights</span><span class=""p"">,</span> <span class=""n"">bias</span><span class=""p"">)</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">h2</span> <span class=""o"">=</span> <span class=""n"">Neuron</span><span class=""p"">(</span><span class=""n"">weights</span><span class=""p"">,</span> <span class=""n"">bias</span><span class=""p"">)</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">o1</span> <span class=""o"">=</span> <span class=""n"">Neuron</span><span class=""p"">(</span><span class=""n"">weights</span><span class=""p"">,</span> <span class=""n"">bias</span><span class=""p"">)</span>

  <span class=""k"">def</span> <span class=""nf"">feedforward</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">x</span><span class=""p"">):</span>
    <span class=""n"">out_h1</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">h1</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">)</span>
    <span class=""n"">out_h2</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">h2</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">)</span>

    <span class=""c1""># The inputs for o1 are the outputs from h1 and h2</span>
    <span class=""n"">out_o1</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">o1</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">(</span><span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""n"">out_h1</span><span class=""p"">,</span> <span class=""n"">out_h2</span><span class=""p"">]))</span>

    <span class=""k"">return</span> <span class=""n"">out_o1</span>

<span class=""n"">network</span> <span class=""o"">=</span> <span class=""n"">OurNeuralNetwork</span><span class=""p"">()</span>
<span class=""n"">x</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""mi"">2</span><span class=""p"">,</span> <span class=""mi"">3</span><span class=""p"">])</span>
<span class=""k"">print</span><span class=""p"">(</span><span class=""n"">network</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">))</span> <span class=""c1""># 0.7216325609518421</span>
</code></pre></div><h3><b>训练神经网络</b></h3><p>现在我们已经学会了如何搭建神经网络，现在我们来学习如何训练它，其实这就是一个优化的过程。</p><p>假设有一个数据集，包含4个人的身高、体重和性别：</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-f15a751a4c95d116aff1cbe66e85e2df_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""280"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-f15a751a4c95d116aff1cbe66e85e2df_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='280'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""280"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-f15a751a4c95d116aff1cbe66e85e2df_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-f15a751a4c95d116aff1cbe66e85e2df_b.jpg""></figure><p><br></p><p>现在我们的目标是训练一个网络，根据体重和身高来推测某人的性别。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-b9ee7f111417a4c4e4dcc5e814667e09_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""550"" data-rawheight=""250"" class=""origin_image zh-lightbox-thumb"" width=""550"" data-original=""https://pic1.zhimg.com/v2-b9ee7f111417a4c4e4dcc5e814667e09_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='550'%20height='250'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""550"" data-rawheight=""250"" class=""origin_image zh-lightbox-thumb lazy"" width=""550"" data-original=""https://pic1.zhimg.com/v2-b9ee7f111417a4c4e4dcc5e814667e09_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-b9ee7f111417a4c4e4dcc5e814667e09_b.jpg""></figure><p><br></p><p>为了简便起见，我们将每个人的身高、体重减去一个固定数值，把性别男定义为1、性别女定义为0。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-463fc37c108236a0f571b09c3184dbf6_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""280"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-463fc37c108236a0f571b09c3184dbf6_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='280'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""280"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-463fc37c108236a0f571b09c3184dbf6_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-463fc37c108236a0f571b09c3184dbf6_b.jpg""></figure><p><br></p><p>在训练神经网络之前，我们需要有一个标准定义它到底好不好，以便我们进行改进，这就是<b>损失</b>（loss）。</p><p>比如用<b>均方误差</b>（MSE）来定义损失：<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-1471c8a0328fd909137892a88dfec354_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""153"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-1471c8a0328fd909137892a88dfec354_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='153'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""153"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-1471c8a0328fd909137892a88dfec354_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-1471c8a0328fd909137892a88dfec354_b.jpg""></figure><p><br></p><p>n是样本的数量，在上面的数据集中是4；<br>y代表人的性别，男性是1，女性是0；<br>ytrue是变量的真实值，ypred是变量的预测值。</p><p>顾名思义，均方误差就是所有数据方差的平均值，我们不妨就把它定义为损失函数。预测结果越好，损失就越低，<b>训练神经网络就是将损失最小化。</b></p><p>如果上面网络的输出一直是0，也就是预测所有人都是男性，那么损失是：<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-b80d6824424193c22835b3814a82ebf5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""281"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-b80d6824424193c22835b3814a82ebf5_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='281'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""281"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-b80d6824424193c22835b3814a82ebf5_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-b80d6824424193c22835b3814a82ebf5_b.jpg""></figure><p><br></p><p><b>MSE= 1/4 (1+0+0+1)= 0.5</b></p><p>计算损失函数的代码如下：<br></p><div class=""highlight""><pre><code class=""language-python""><span></span><span class=""kn"">import</span> <span class=""nn"">numpy</span> <span class=""kn"">as</span> <span class=""nn"">np</span>

<span class=""k"">def</span> <span class=""nf"">mse_loss</span><span class=""p"">(</span><span class=""n"">y_true</span><span class=""p"">,</span> <span class=""n"">y_pred</span><span class=""p"">):</span>
  <span class=""c1""># y_true and y_pred are numpy arrays of the same length.</span>
  <span class=""k"">return</span> <span class=""p"">((</span><span class=""n"">y_true</span> <span class=""o"">-</span> <span class=""n"">y_pred</span><span class=""p"">)</span> <span class=""o"">**</span> <span class=""mi"">2</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">mean</span><span class=""p"">()</span>

<span class=""n"">y_true</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""mi"">1</span><span class=""p"">,</span> <span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">1</span><span class=""p"">])</span>
<span class=""n"">y_pred</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">0</span><span class=""p"">])</span>

<span class=""k"">print</span><span class=""p"">(</span><span class=""n"">mse_loss</span><span class=""p"">(</span><span class=""n"">y_true</span><span class=""p"">,</span> <span class=""n"">y_pred</span><span class=""p"">))</span> <span class=""c1""># 0.5</span>
</code></pre></div><h3><b>减少神经网络损失</b><br></h3><p>这个神经网络不够好，还要不断优化，尽量减少损失。我们知道，改变网络的权重和偏置可以影响预测值，但我们应该怎么做呢？</p><p>为了简单起见，我们把数据集缩减到只包含Alice一个人的数据。于是损失函数就剩下Alice一个人的方差：<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-607474125bb8f8fd26606eaa6b823d9d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""297"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-607474125bb8f8fd26606eaa6b823d9d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='297'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""297"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-607474125bb8f8fd26606eaa6b823d9d_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-607474125bb8f8fd26606eaa6b823d9d_b.jpg""></figure><p><br></p><p>预测值是由一系列网络权重和偏置计算出来的：<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-1bc3b1c2be12eec72acfc953a7bac37d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""550"" data-rawheight=""250"" class=""origin_image zh-lightbox-thumb"" width=""550"" data-original=""https://pic1.zhimg.com/v2-1bc3b1c2be12eec72acfc953a7bac37d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='550'%20height='250'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""550"" data-rawheight=""250"" class=""origin_image zh-lightbox-thumb lazy"" width=""550"" data-original=""https://pic1.zhimg.com/v2-1bc3b1c2be12eec72acfc953a7bac37d_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-1bc3b1c2be12eec72acfc953a7bac37d_b.jpg""></figure><p><br></p><p>所以损失函数实际上是包含多个权重、偏置的多元函数：</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-db41b32acd686d37c28c769259e0ad91_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""85"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-db41b32acd686d37c28c769259e0ad91_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='85'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""85"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-db41b32acd686d37c28c769259e0ad91_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-db41b32acd686d37c28c769259e0ad91_b.jpg""></figure><p><br></p><p>（注意！前方高能！需要你有一些基本的多元函数微分知识，比如偏导数、链式求导法则。）<br></p><p>如果调整一下w1，损失函数是会变大还是变小？我们需要知道偏导数∂L/∂w1是正是负才能回答这个问题。</p><p>根据链式求导法则：</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-e66542108055988b3a1df8d556e2d11d_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""141"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-e66542108055988b3a1df8d556e2d11d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='141'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""141"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-e66542108055988b3a1df8d556e2d11d_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-e66542108055988b3a1df8d556e2d11d_b.jpg""></figure><p><br></p><p>而L=(1-ypred)2，可以求得第一项偏导数：</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-252af9e26b84b8ac4777e84a2799f57c_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""125"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-252af9e26b84b8ac4777e84a2799f57c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='125'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""125"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-252af9e26b84b8ac4777e84a2799f57c_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-252af9e26b84b8ac4777e84a2799f57c_b.jpg""></figure><p><br></p><p>接下来我们要想办法获得ypred和w1的关系，我们已经知道神经元h1、h2和o1的数学运算规则：<br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-fce5cec5e9477239784768d75fc97530_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""105"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-fce5cec5e9477239784768d75fc97530_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='105'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""105"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-fce5cec5e9477239784768d75fc97530_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-fce5cec5e9477239784768d75fc97530_b.jpg""></figure><p><br></p><p>实际上只有神经元h1中包含权重w1，所以我们再次运用链式求导法则：<br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-1bee04eaf0dea89d1708998ac81dc378_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""245"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-1bee04eaf0dea89d1708998ac81dc378_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='245'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""245"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-1bee04eaf0dea89d1708998ac81dc378_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-1bee04eaf0dea89d1708998ac81dc378_b.jpg""></figure><p><br></p><p>然后求∂h1/∂w1</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-70aa49b8d152d7984a0d993d2291a0bb_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""221"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-70aa49b8d152d7984a0d993d2291a0bb_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='221'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""221"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic4.zhimg.com/v2-70aa49b8d152d7984a0d993d2291a0bb_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-70aa49b8d152d7984a0d993d2291a0bb_b.jpg""></figure><p><br></p><p>我们在上面的计算中遇到了2次激活函数sigmoid的导数f′(x)，sigmoid函数的导数很容易求得：</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-d41dc8afcf6fb553d66dbad24758fe9a_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""274"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-d41dc8afcf6fb553d66dbad24758fe9a_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='274'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""274"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-d41dc8afcf6fb553d66dbad24758fe9a_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-d41dc8afcf6fb553d66dbad24758fe9a_b.jpg""></figure><p><br></p><p>总的链式求导公式：<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-ee978cf040e5582b799ac3bcfcfc0cb1_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1068"" data-rawheight=""172"" class=""origin_image zh-lightbox-thumb"" width=""1068"" data-original=""https://pic1.zhimg.com/v2-ee978cf040e5582b799ac3bcfcfc0cb1_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1068'%20height='172'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1068"" data-rawheight=""172"" class=""origin_image zh-lightbox-thumb lazy"" width=""1068"" data-original=""https://pic1.zhimg.com/v2-ee978cf040e5582b799ac3bcfcfc0cb1_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-ee978cf040e5582b799ac3bcfcfc0cb1_b.jpg""></figure><p><br></p><p>这种向后计算偏导数的系统称为<b>反向传播</b>（backpropagation）。</p><p>上面的数学符号太多，下面我们带入实际数值来计算一下。h1、h2和o1</p><p>h1=f(x1⋅w1+x2⋅w2+b1)=0.0474</p><p>h2=f(w3⋅x3+w4⋅x4+b2)=0.0474</p><p>o1=f(w5⋅h1+w6⋅h2+b3)=f(0.0474+0.0474+0)=f(0.0948)=0.524</p><p>神经网络的输出y=0.524，没有显示出强烈的是男（1）是女（0）的证据。现在的预测效果还很不好。</p><p>我们再计算一下当前网络的偏导数∂L/∂w1：</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-4ab9796eea7123ad708e268c4e7b9448_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""992"" data-rawheight=""1242"" class=""origin_image zh-lightbox-thumb"" width=""992"" data-original=""https://pic4.zhimg.com/v2-4ab9796eea7123ad708e268c4e7b9448_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='992'%20height='1242'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""992"" data-rawheight=""1242"" class=""origin_image zh-lightbox-thumb lazy"" width=""992"" data-original=""https://pic4.zhimg.com/v2-4ab9796eea7123ad708e268c4e7b9448_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-4ab9796eea7123ad708e268c4e7b9448_b.jpg""></figure><p><br></p><p>这个结果告诉我们：如果增大w1，损失函数L会有一个非常小的增长。</p><h3><b>随机梯度下降</b></h3><p>下面将使用一种称为<b>随机梯度下降</b>（SGD）的优化算法，来训练网络。</p><p>经过前面的运算，我们已经有了训练神经网络所有数据。但是该如何操作？SGD定义了改变权重和偏置的方法：</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-22118be9602e08f7320ee1a3b7aee998_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""744"" data-rawheight=""178"" class=""origin_image zh-lightbox-thumb"" width=""744"" data-original=""https://pic2.zhimg.com/v2-22118be9602e08f7320ee1a3b7aee998_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='744'%20height='178'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""744"" data-rawheight=""178"" class=""origin_image zh-lightbox-thumb lazy"" width=""744"" data-original=""https://pic2.zhimg.com/v2-22118be9602e08f7320ee1a3b7aee998_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-22118be9602e08f7320ee1a3b7aee998_b.jpg""></figure><p><br></p><p>η是一个常数，称为<b>学习率</b>（learning rate），它决定了我们训练网络速率的快慢。将w1减去η·∂L/∂w1，就等到了新的权重w1。<br></p><p>当∂L/∂w1是正数时，w1会变小；当∂L/∂w1是负数 时，w1会变大。</p><p>如果我们用这种方法去逐步改变网络的权重w和偏置b，损失函数会缓慢地降低，从而改进我们的神经网络。</p><p>训练流程如下：</p><p>1、从数据集中选择一个样本；<br>2、计算损失函数对所有权重和偏置的偏导数；<br>3、使用更新公式更新每个权重和偏置；<br>4、回到第1步。</p><p>我们用Python代码实现这个过程：</p><div class=""highlight""><pre><code class=""language-python""><span></span><span class=""kn"">import</span> <span class=""nn"">numpy</span> <span class=""kn"">as</span> <span class=""nn"">np</span>

<span class=""k"">def</span> <span class=""nf"">sigmoid</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">):</span>
  <span class=""c1""># Sigmoid activation function: f(x) = 1 / (1 + e^(-x))</span>
  <span class=""k"">return</span> <span class=""mi"">1</span> <span class=""o"">/</span> <span class=""p"">(</span><span class=""mi"">1</span> <span class=""o"">+</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">exp</span><span class=""p"">(</span><span class=""o"">-</span><span class=""n"">x</span><span class=""p"">))</span>

<span class=""k"">def</span> <span class=""nf"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">):</span>
  <span class=""c1""># Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))</span>
  <span class=""n"">fx</span> <span class=""o"">=</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">)</span>
  <span class=""k"">return</span> <span class=""n"">fx</span> <span class=""o"">*</span> <span class=""p"">(</span><span class=""mi"">1</span> <span class=""o"">-</span> <span class=""n"">fx</span><span class=""p"">)</span>

<span class=""k"">def</span> <span class=""nf"">mse_loss</span><span class=""p"">(</span><span class=""n"">y_true</span><span class=""p"">,</span> <span class=""n"">y_pred</span><span class=""p"">):</span>
  <span class=""c1""># y_true and y_pred are numpy arrays of the same length.</span>
  <span class=""k"">return</span> <span class=""p"">((</span><span class=""n"">y_true</span> <span class=""o"">-</span> <span class=""n"">y_pred</span><span class=""p"">)</span> <span class=""o"">**</span> <span class=""mi"">2</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">mean</span><span class=""p"">()</span>

<span class=""k"">class</span> <span class=""nc"">OurNeuralNetwork</span><span class=""p"">:</span>
  <span class=""sd"">'''</span>
<span class=""sd"">  A neural network with:</span>
<span class=""sd"">    - 2 inputs</span>
<span class=""sd"">    - a hidden layer with 2 neurons (h1, h2)</span>
<span class=""sd"">    - an output layer with 1 neuron (o1)</span>

<span class=""sd"">  *** DISCLAIMER ***:</span>
<span class=""sd"">  The code below is intended to be simple and educational, NOT optimal.</span>
<span class=""sd"">  Real neural net code looks nothing like this. DO NOT use this code.</span>
<span class=""sd"">  Instead, read/run it to understand how this specific network works.</span>
<span class=""sd"">  '''</span>
  <span class=""k"">def</span> <span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>
    <span class=""c1""># Weights</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w1</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w2</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w3</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w4</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w5</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w6</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>

    <span class=""c1""># Biases</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b1</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b2</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b3</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">random</span><span class=""o"">.</span><span class=""n"">normal</span><span class=""p"">()</span>

  <span class=""k"">def</span> <span class=""nf"">feedforward</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">x</span><span class=""p"">):</span>
    <span class=""c1""># x is a numpy array with 2 elements.</span>
    <span class=""n"">h1</span> <span class=""o"">=</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w1</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w2</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">1</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b1</span><span class=""p"">)</span>
    <span class=""n"">h2</span> <span class=""o"">=</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w3</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w4</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">1</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b2</span><span class=""p"">)</span>
    <span class=""n"">o1</span> <span class=""o"">=</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w5</span> <span class=""o"">*</span> <span class=""n"">h1</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w6</span> <span class=""o"">*</span> <span class=""n"">h2</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b3</span><span class=""p"">)</span>
    <span class=""k"">return</span> <span class=""n"">o1</span>

  <span class=""k"">def</span> <span class=""nf"">train</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">data</span><span class=""p"">,</span> <span class=""n"">all_y_trues</span><span class=""p"">):</span>
    <span class=""sd"">'''</span>
<span class=""sd"">    - data is a (n x 2) numpy array, n = # of samples in the dataset.</span>
<span class=""sd"">    - all_y_trues is a numpy array with n elements.</span>
<span class=""sd"">      Elements in all_y_trues correspond to those in data.</span>
<span class=""sd"">    '''</span>
    <span class=""n"">learn_rate</span> <span class=""o"">=</span> <span class=""mf"">0.1</span>
    <span class=""n"">epochs</span> <span class=""o"">=</span> <span class=""mi"">1000</span> <span class=""c1""># number of times to loop through the entire dataset</span>

    <span class=""k"">for</span> <span class=""n"">epoch</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""n"">epochs</span><span class=""p"">):</span>
      <span class=""k"">for</span> <span class=""n"">x</span><span class=""p"">,</span> <span class=""n"">y_true</span> <span class=""ow"">in</span> <span class=""nb"">zip</span><span class=""p"">(</span><span class=""n"">data</span><span class=""p"">,</span> <span class=""n"">all_y_trues</span><span class=""p"">):</span>
        <span class=""c1""># --- Do a feedforward (we'll need these values later)</span>
        <span class=""n"">sum_h1</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w1</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w2</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">1</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b1</span>
        <span class=""n"">h1</span> <span class=""o"">=</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h1</span><span class=""p"">)</span>

        <span class=""n"">sum_h2</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w3</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w4</span> <span class=""o"">*</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">1</span><span class=""p"">]</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b2</span>
        <span class=""n"">h2</span> <span class=""o"">=</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h2</span><span class=""p"">)</span>

        <span class=""n"">sum_o1</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w5</span> <span class=""o"">*</span> <span class=""n"">h1</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w6</span> <span class=""o"">*</span> <span class=""n"">h2</span> <span class=""o"">+</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b3</span>
        <span class=""n"">o1</span> <span class=""o"">=</span> <span class=""n"">sigmoid</span><span class=""p"">(</span><span class=""n"">sum_o1</span><span class=""p"">)</span>
        <span class=""n"">y_pred</span> <span class=""o"">=</span> <span class=""n"">o1</span>

        <span class=""c1""># --- Calculate partial derivatives.</span>
        <span class=""c1""># --- Naming: d_L_d_w1 represents ""partial L / partial w1""</span>
        <span class=""n"">d_L_d_ypred</span> <span class=""o"">=</span> <span class=""o"">-</span><span class=""mi"">2</span> <span class=""o"">*</span> <span class=""p"">(</span><span class=""n"">y_true</span> <span class=""o"">-</span> <span class=""n"">y_pred</span><span class=""p"">)</span>

        <span class=""c1""># Neuron o1</span>
        <span class=""n"">d_ypred_d_w5</span> <span class=""o"">=</span> <span class=""n"">h1</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_o1</span><span class=""p"">)</span>
        <span class=""n"">d_ypred_d_w6</span> <span class=""o"">=</span> <span class=""n"">h2</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_o1</span><span class=""p"">)</span>
        <span class=""n"">d_ypred_d_b3</span> <span class=""o"">=</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_o1</span><span class=""p"">)</span>

        <span class=""n"">d_ypred_d_h1</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w5</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_o1</span><span class=""p"">)</span>
        <span class=""n"">d_ypred_d_h2</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w6</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_o1</span><span class=""p"">)</span>

        <span class=""c1""># Neuron h1</span>
        <span class=""n"">d_h1_d_w1</span> <span class=""o"">=</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h1</span><span class=""p"">)</span>
        <span class=""n"">d_h1_d_w2</span> <span class=""o"">=</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">1</span><span class=""p"">]</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h1</span><span class=""p"">)</span>
        <span class=""n"">d_h1_d_b1</span> <span class=""o"">=</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h1</span><span class=""p"">)</span>

        <span class=""c1""># Neuron h2</span>
        <span class=""n"">d_h2_d_w3</span> <span class=""o"">=</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h2</span><span class=""p"">)</span>
        <span class=""n"">d_h2_d_w4</span> <span class=""o"">=</span> <span class=""n"">x</span><span class=""p"">[</span><span class=""mi"">1</span><span class=""p"">]</span> <span class=""o"">*</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h2</span><span class=""p"">)</span>
        <span class=""n"">d_h2_d_b2</span> <span class=""o"">=</span> <span class=""n"">deriv_sigmoid</span><span class=""p"">(</span><span class=""n"">sum_h2</span><span class=""p"">)</span>

        <span class=""c1""># --- Update weights and biases</span>
        <span class=""c1""># Neuron h1</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w1</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_h1</span> <span class=""o"">*</span> <span class=""n"">d_h1_d_w1</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w2</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_h1</span> <span class=""o"">*</span> <span class=""n"">d_h1_d_w2</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b1</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_h1</span> <span class=""o"">*</span> <span class=""n"">d_h1_d_b1</span>

        <span class=""c1""># Neuron h2</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w3</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_h2</span> <span class=""o"">*</span> <span class=""n"">d_h2_d_w3</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w4</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_h2</span> <span class=""o"">*</span> <span class=""n"">d_h2_d_w4</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b2</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_h2</span> <span class=""o"">*</span> <span class=""n"">d_h2_d_b2</span>

        <span class=""c1""># Neuron o1</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w5</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_w5</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">w6</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_w6</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">b3</span> <span class=""o"">-=</span> <span class=""n"">learn_rate</span> <span class=""o"">*</span> <span class=""n"">d_L_d_ypred</span> <span class=""o"">*</span> <span class=""n"">d_ypred_d_b3</span>

      <span class=""c1""># --- Calculate total loss at the end of each epoch</span>
      <span class=""k"">if</span> <span class=""n"">epoch</span> <span class=""o"">%</span> <span class=""mi"">10</span> <span class=""o"">==</span> <span class=""mi"">0</span><span class=""p"">:</span>
        <span class=""n"">y_preds</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">apply_along_axis</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">,</span> <span class=""mi"">1</span><span class=""p"">,</span> <span class=""n"">data</span><span class=""p"">)</span>
        <span class=""n"">loss</span> <span class=""o"">=</span> <span class=""n"">mse_loss</span><span class=""p"">(</span><span class=""n"">all_y_trues</span><span class=""p"">,</span> <span class=""n"">y_preds</span><span class=""p"">)</span>
        <span class=""k"">print</span><span class=""p"">(</span><span class=""s2"">""Epoch </span><span class=""si"">%d</span><span class=""s2""> loss: </span><span class=""si"">%.3f</span><span class=""s2"">""</span> <span class=""o"">%</span> <span class=""p"">(</span><span class=""n"">epoch</span><span class=""p"">,</span> <span class=""n"">loss</span><span class=""p"">))</span>

<span class=""c1""># Define dataset</span>
<span class=""n"">data</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span>
  <span class=""p"">[</span><span class=""o"">-</span><span class=""mi"">2</span><span class=""p"">,</span> <span class=""o"">-</span><span class=""mi"">1</span><span class=""p"">],</span>  <span class=""c1""># Alice</span>
  <span class=""p"">[</span><span class=""mi"">25</span><span class=""p"">,</span> <span class=""mi"">6</span><span class=""p"">],</span>   <span class=""c1""># Bob</span>
  <span class=""p"">[</span><span class=""mi"">17</span><span class=""p"">,</span> <span class=""mi"">4</span><span class=""p"">],</span>   <span class=""c1""># Charlie</span>
  <span class=""p"">[</span><span class=""o"">-</span><span class=""mi"">15</span><span class=""p"">,</span> <span class=""o"">-</span><span class=""mi"">6</span><span class=""p"">],</span> <span class=""c1""># Diana</span>
<span class=""p"">])</span>
<span class=""n"">all_y_trues</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span>
  <span class=""mi"">1</span><span class=""p"">,</span> <span class=""c1""># Alice</span>
  <span class=""mi"">0</span><span class=""p"">,</span> <span class=""c1""># Bob</span>
  <span class=""mi"">0</span><span class=""p"">,</span> <span class=""c1""># Charlie</span>
  <span class=""mi"">1</span><span class=""p"">,</span> <span class=""c1""># Diana</span>
<span class=""p"">])</span>

<span class=""c1""># Train our neural network!</span>
<span class=""n"">network</span> <span class=""o"">=</span> <span class=""n"">OurNeuralNetwork</span><span class=""p"">()</span>
<span class=""n"">network</span><span class=""o"">.</span><span class=""n"">train</span><span class=""p"">(</span><span class=""n"">data</span><span class=""p"">,</span> <span class=""n"">all_y_trues</span><span class=""p"">)</span>
</code></pre></div><p>随着学习过程的进行，损失函数逐渐减小。<br></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-a0435513c747f70eb38929117ab0cf47_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""507"" data-rawheight=""387"" class=""origin_image zh-lightbox-thumb"" width=""507"" data-original=""https://pic1.zhimg.com/v2-a0435513c747f70eb38929117ab0cf47_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='507'%20height='387'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""507"" data-rawheight=""387"" class=""origin_image zh-lightbox-thumb lazy"" width=""507"" data-original=""https://pic1.zhimg.com/v2-a0435513c747f70eb38929117ab0cf47_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-a0435513c747f70eb38929117ab0cf47_b.jpg""></figure><p><br></p><p>现在我们可以用它来推测出每个人的性别了：</p><div class=""highlight""><pre><code class=""language-python""><span></span><span class=""c1""># Make some predictions</span>
<span class=""n"">emily</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""o"">-</span><span class=""mi"">7</span><span class=""p"">,</span> <span class=""o"">-</span><span class=""mi"">3</span><span class=""p"">])</span> <span class=""c1""># 128 pounds, 63 inches</span>
<span class=""n"">frank</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">([</span><span class=""mi"">20</span><span class=""p"">,</span> <span class=""mi"">2</span><span class=""p"">])</span>  <span class=""c1""># 155 pounds, 68 inches</span>
<span class=""k"">print</span><span class=""p"">(</span><span class=""s2"">""Emily: </span><span class=""si"">%.3f</span><span class=""s2"">""</span> <span class=""o"">%</span> <span class=""n"">network</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">(</span><span class=""n"">emily</span><span class=""p"">))</span> <span class=""c1""># 0.951 - F</span>
<span class=""k"">print</span><span class=""p"">(</span><span class=""s2"">""Frank: </span><span class=""si"">%.3f</span><span class=""s2"">""</span> <span class=""o"">%</span> <span class=""n"">network</span><span class=""o"">.</span><span class=""n"">feedforward</span><span class=""p"">(</span><span class=""n"">frank</span><span class=""p"">))</span> <span class=""c1""># 0.039 - M</span>
</code></pre></div><h2><b>更多</b><br></h2><p>这篇教程只是万里长征第一步，后面还有很多知识需要学习：</p><p>1、用更大更好的机器学习库搭建神经网络，如Tensorflow、Keras、PyTorch<br>2、在浏览器中的直观理解神经网络：<a href=""http://link.zhihu.com/?target=https%3A//playground.tensorflow.org/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">playground.tensorflow.org</span><span class=""invisible"">/</span><span class=""ellipsis""></span></a><br>3、学习sigmoid以外的其他激活函数：<a href=""http://link.zhihu.com/?target=https%3A//keras.io/activations/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">keras.io/activations/</span><span class=""invisible""></span></a><br>4、学习SGD以外的其他优化器：<a href=""http://link.zhihu.com/?target=https%3A//keras.io/optimizers/"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">keras.io/optimizers/</span><span class=""invisible""></span></a><br>5、学习卷积神经网络（CNN）<br>6、学习递归神经网络（RNN）</p><p>这些都是Victor给自己挖的“坑”。他表示自己未来“可能”会写这些主题内容，希望他能陆续把这些坑填完。如果你想入门神经网络，不妨去订阅他的博客。</p><h2><b>关于这位小哥</b></h2><p>Victor Zhou是普林斯顿2019级CS毕业生，已经拿到Facebook软件工程师的offer，今年8月入职。他曾经做过JS编译器，还做过两款页游，一个仇恨攻击言论的识别库。</p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-15dcba5bdf7b3fbabe1801673bdf8b34_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""150"" data-rawheight=""150"" class=""content_image"" width=""150""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""150"" data-rawheight=""150"" class=""content_image lazy"" width=""150"" data-actualsrc=""https://pic2.zhimg.com/v2-15dcba5bdf7b3fbabe1801673bdf8b34_b.jpg""></figure><p><br></p><p>最后附上小哥的博客链接：<br></p><a href=""http://link.zhihu.com/?target=https%3A//victorzhou.com/"" data-draft-node=""block"" data-draft-type=""link-card"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://</span><span class=""visible"">victorzhou.com/</span><span class=""invisible""></span></a><p>— <b>完</b> —</p><p>量子位 · QbitAI<br>վ'ᴗ' ի 追踪AI技术和产品新动态</p><a href=""https://www.zhihu.com/org/liang-zi-wei-48"" data-draft-node=""block"" data-draft-type=""link-card"" data-image=""https://pic4.zhimg.com/v2-ca6e7ffc10a0d10edbae635cee82d007_ipico.jpg"" data-image-width=""250"" data-image-height=""250"" class=""internal"">量子位</a><p>欢迎大家关注我们，以及订阅<a href=""https://zhuanlan.zhihu.com/qbitai"" class=""internal"">我们的知乎专栏</a></p>",3895,0,1401,量子位,https://api.zhihu.com/people/36f69162230003d316d0b8a6d8da20ba,赞同了回答
1552803887433,极数深流,,,,,https://api.zhihu.com/columns/ExtremeDeep,,,,1552803887,,,,,,匿名用户,,关注了专栏
1552310522547,"从QANet看自然语言处理如何""炫技""",,,,,https://api.zhihu.com/articles/58961139,,,,1552310522,,"<p>        这篇文章讲讲谷歌大脑团队和 CMU 联合推出的 QANet模型，QAnet是SQUAD排行榜2018年3月份排名第一的模型。<b>既然有Bert，为什么还要讲QAnet？因为QAnet融合了2017年~2018年NLP领域一些重要的突破（各种炫酷技巧），通过学习它，你可以对NLP近两年的发展有较为全面的了解。</b><a href=""http://link.zhihu.com/?target=https%3A//openreview.net/pdf%3Fid%3DB14TlG-RW"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">QAnet链接</a></p><p>         这篇文章从以下几部分讲：</p><p><b>         第一部分：QANet模型整体网络结构介绍</b></p><p><b>         第二部分：</b> <b>分别介绍各个炫酷的技术点的原理</b></p><p><b>         第三部分： 总结（与Bert对比）</b></p><p>  QANet融合的NLP和深度学习领域近两年内主要成果包括：</p><p>     （1）Fackbook 2017.5  提出CNN seq2seq,替代LSTM ，position encoding。</p><p>     （2）Google 2017.6 提出《Attention is all you need》 muti-head attention。 </p><p>     （3）Depthwise Separable Conv 深度可分离卷积         </p><p>     （4）Highway network 高速神经网络    </p><p>         .........</p><ul><li><b>一、QANet模型网络结构介绍</b> </li></ul><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-dba82ef8a618f2253a848d9118089c33_b.jpg"" data-size=""normal"" data-rawwidth=""596"" data-rawheight=""454"" class=""origin_image zh-lightbox-thumb"" width=""596"" data-original=""https://pic4.zhimg.com/v2-dba82ef8a618f2253a848d9118089c33_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='596'%20height='454'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""596"" data-rawheight=""454"" class=""origin_image zh-lightbox-thumb lazy"" width=""596"" data-original=""https://pic4.zhimg.com/v2-dba82ef8a618f2253a848d9118089c33_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-dba82ef8a618f2253a848d9118089c33_b.jpg""><figcaption>QAnet整体网络结构</figcaption></figure><p>（1）输入的数据有Context和Question两部分组成。</p><p>（2）通过Embedding层转化为Embedding向量，分为word embedding和char embedding。Highway network包含在embedding层中。</p><p>（3）通过Encoder Block层，Encoder Block是QANet重要组成部分，如右图所示。Encoder Block层分别通过四个部分，Position Encoding、 Conv卷积层、Self attention和Feedword层。每个部分开头做layernorm处理，结尾做残差连接。</p><p>（4）Context和Question的encoder向量，通过Context Query Attention计算相关性。</p><p>（5）接着通过三层Encoder Block层，第一层和第二层输出连接后，通过一层全链接层作为起始位置的概率。</p><p>（6）第一层和第三层输出连接后，通过一层全链接层作为起始位置的概率。</p><p><br></p><ul><li><b>二、</b> <b>梳理各个炫酷的技术点</b></li></ul><p><b>1.技能点一：</b>Encoder Block层<b>里的卷积</b></p><p>      2017年Facebook发表了NLP领域重要论文《Convolutional Sequence to Sequence Learning》，提到它们用卷积神经网络实现seq2seq，实现机器翻译，效果上达到LSTM网络的水平，效率明显优于LSTM。</p><p>       这块有很多文章可以参考，我就不细讲了。</p><p><b>2.技能点二：Depthwise Separable Conv</b></p><p>      值得注意的是，QANet用的不是简单的CNN，而是采用Depthwise Separable Conv（深度可分离卷积）。</p><p>       这个卷积的的大致意思是对每一个深度图分别进行卷积再融合，步骤是先 Depthwise Conv 再 Pointwise Conv，大大减少了参数量。目的是加速整个卷积计算过程的速度。Depthwise Separable Conv来源于Inception和Xception。</p><p>        Inception 最初提出的版本，其核心思想就是使用多尺寸卷积核去观察输入数据。也就是把一个尺寸较大的卷积核替换为多个小的卷积核串联或者并联，以降低计算量。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-4045ff96b607620b61c0dedcec7d6c5a_b.jpg"" data-size=""normal"" data-rawwidth=""645"" data-rawheight=""387"" class=""origin_image zh-lightbox-thumb"" width=""645"" data-original=""https://pic4.zhimg.com/v2-4045ff96b607620b61c0dedcec7d6c5a_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='645'%20height='387'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""645"" data-rawheight=""387"" class=""origin_image zh-lightbox-thumb lazy"" width=""645"" data-original=""https://pic4.zhimg.com/v2-4045ff96b607620b61c0dedcec7d6c5a_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-4045ff96b607620b61c0dedcec7d6c5a_b.jpg""><figcaption>Inception</figcaption></figure><p>          基于Inception发展而来的 Xception ，作者称其为 Extreme Inception。首先探讨的是Inception 的 多尺寸卷积核 和 卷积核替换，然后到 Bottleneck，最后到 Xception 的 Depthwise Separable Conv 。从Inception 到Xception大大减少参数量，从而加快训练速度。N=1024，Dk=3,参数降为0.112。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-631ca2bc8ef414b32400c5b49701e2d2_b.jpg"" data-size=""normal"" data-rawwidth=""731"" data-rawheight=""406"" class=""origin_image zh-lightbox-thumb"" width=""731"" data-original=""https://pic2.zhimg.com/v2-631ca2bc8ef414b32400c5b49701e2d2_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='731'%20height='406'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""731"" data-rawheight=""406"" class=""origin_image zh-lightbox-thumb lazy"" width=""731"" data-original=""https://pic2.zhimg.com/v2-631ca2bc8ef414b32400c5b49701e2d2_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-631ca2bc8ef414b32400c5b49701e2d2_b.jpg""><figcaption>Depthwise Separable Conv</figcaption></figure><p><b>3.技能点三：Self attention</b></p><p>        Self attention 来源于Google 2017年发表的一篇重要论文《Attention is all you need》提出的Multi-head attention。如果你关注自然语言处理领域，估计对这个很熟悉了。多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。 比如Multi-Head Attention就是Attention做多次然后拼接，这跟CNN中的多个卷积核的思想是一致的。Self attention即两个输入是一样的，自己和自己做attention计算。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-33cb2d9c878a7ffb55cb0d87de381105_b.jpg"" data-size=""normal"" data-rawwidth=""847"" data-rawheight=""380"" class=""origin_image zh-lightbox-thumb"" width=""847"" data-original=""https://pic3.zhimg.com/v2-33cb2d9c878a7ffb55cb0d87de381105_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='847'%20height='380'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""847"" data-rawheight=""380"" class=""origin_image zh-lightbox-thumb lazy"" width=""847"" data-original=""https://pic3.zhimg.com/v2-33cb2d9c878a7ffb55cb0d87de381105_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-33cb2d9c878a7ffb55cb0d87de381105_b.jpg""><figcaption>multihead attention原理图</figcaption></figure><p><b>4.技能点四：Position Encoding</b></p><p>       这里的Position Encoding方式可参考Facebook的《Convolutional Sequence to Sequence Learning》，用法是一致的。计算公式如下：</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-24cd4f252a3ec0ab120d0dc51481e83f_b.jpg"" data-size=""normal"" data-rawwidth=""491"" data-rawheight=""172"" class=""origin_image zh-lightbox-thumb"" width=""491"" data-original=""https://pic4.zhimg.com/v2-24cd4f252a3ec0ab120d0dc51481e83f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='491'%20height='172'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""491"" data-rawheight=""172"" class=""origin_image zh-lightbox-thumb lazy"" width=""491"" data-original=""https://pic4.zhimg.com/v2-24cd4f252a3ec0ab120d0dc51481e83f_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-24cd4f252a3ec0ab120d0dc51481e83f_b.jpg""><figcaption>position encoding计算公式</figcaption></figure><p>       对于position encoding，多说一句，个人观点，原理上是很有道理的，但是效果并不明显，从论文可以看出，对模型的效果贡献不大，属于可有可无。也就是比较接近我的说法：”炫技”。</p><p><b> 5.技能点五：Highway Network</b></p><p>        如果你仔细看过QAnet详细资料，可以看到QAnet的Embedding层不是简单的把word embedding和char embedding做了一层连接作为输出，而是通过highway network再做输出。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-45f7f6d4791be7869458260a7897f636_b.jpg"" data-size=""normal"" data-rawwidth=""525"" data-rawheight=""360"" class=""origin_image zh-lightbox-thumb"" width=""525"" data-original=""https://pic2.zhimg.com/v2-45f7f6d4791be7869458260a7897f636_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='525'%20height='360'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""525"" data-rawheight=""360"" class=""origin_image zh-lightbox-thumb lazy"" width=""525"" data-original=""https://pic2.zhimg.com/v2-45f7f6d4791be7869458260a7897f636_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-45f7f6d4791be7869458260a7897f636_b.jpg""><figcaption>QAnet embedding层</figcaption></figure><p><b>Highway network本质上与残差网络作用类似，降低信息在传输过程中的损失</b>。这是通过一种控制穿过神经网络的信息流的闸门机制所实现的。通过这种机制，神经网络可以提供通路，让信息穿过后却没有损失。我们将这种通路称为information highways。</p><p><b>6.技能点六：Batch normalization</b></p><p>       Batch normalization每个batch里面的很接近的样本，都归一成0均值1方差。</p><p>       为什么要归一成0均值1方差呢？随着在神经网络训练深度加深，其整体分布逐渐发生偏移或变动，导致后向传播时神经网络梯度消失，深度网络收敛越来越慢。Batch normalization把越来越偏的分布强行正态分布，梯度变大，减缓梯度消失问题。<b>因此，加Batch normalization是为了使深度神经网络训练速度加快。</b></p><p><b>7.技能点七：LayerNorm</b></p><p>       Encoder层每个部分的输入都用到了LayerNorm，实现近似独立同分布。</p><p>       为什么要独立同分布？随着神经网络的加深，每一层的输出到下一层的输入，会出现分布偏差越来越大。最后输出的target层的分布和输入的分布差距很大。而完全的白华操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。</p><p><b>三、 个人总结</b></p><p>       对比下Bert和QAnet，Bert整体网络结构相对于QAnet更加简单。QANet强调技巧，Bert强调算力。从对比中，可以看出NLP发展的趋势。</p>",79,2,,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,发表了文章
1552101318466,职级“T5”在腾讯是怎样的存在？,,,,,https://api.zhihu.com/articles/58436622,,,,1552101318,,"<p>这个问题，如果给两万名腾讯技术人员来回答，大部分的答案估计只有一个字 —— <b>神</b></p><p>要知道，在腾讯职级能力体系里，大多数人达到<b>T3</b>已殊为不易，已是人才市场上的重要参照。</p><figure data-size=""small""><noscript><img src=""https://pic3.zhimg.com/v2-dae72d2cb1fa769c7c3c8f23ce80da9d_b.jpg"" data-caption="""" data-size=""small"" data-rawwidth=""1080"" data-rawheight=""872"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-dae72d2cb1fa769c7c3c8f23ce80da9d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='872'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""small"" data-rawwidth=""1080"" data-rawheight=""872"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-dae72d2cb1fa769c7c3c8f23ce80da9d_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-dae72d2cb1fa769c7c3c8f23ce80da9d_b.jpg""></figure><p>腾讯对T5科学家的评定标准极其严苛：<b>他们不仅要是各自领域公认的资深专家，还需要有足够的战略眼光参与公司重大领域和项目。</b></p><p>这让创立20年的腾讯T5科学家极为稀缺，此前他们像七龙珠一般星散在各个事业群。腾讯围绕他们的能力所长，设置了种种名目神秘的实验室：<b>科恩、玄武、优图、量子、音视频</b>……你也许很少听到他们的消息，但你很可能每一天都受惠于他们的技术。</p><p>而在去年腾讯在9月30日宣布战略升级后，这些科学家被收归于「云与智慧产业事业群」（CSIG）和「技术工程事业群」（TEG），其中肩负产业互联网重任的<b>CSIG，是腾讯拥有最多T5科学家的事业群。</b></p><hr><h2><b>▶吴石</b> with<b>科恩实验室：</b>哥不在江湖，但江湖上都是哥的传说</h2><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-171bc562ff1ad904a221081d7d2a2ba2_b.jpg"" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""360"" class=""origin_image zh-lightbox-thumb"" width=""600"" data-original=""https://pic3.zhimg.com/v2-171bc562ff1ad904a221081d7d2a2ba2_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='600'%20height='360'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""360"" class=""origin_image zh-lightbox-thumb lazy"" width=""600"" data-original=""https://pic3.zhimg.com/v2-171bc562ff1ad904a221081d7d2a2ba2_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-171bc562ff1ad904a221081d7d2a2ba2_b.jpg""><figcaption>吴石</figcaption></figure><p>有人的地方就有江湖，黑客世界也一样。有黑帽子通过技术攻击制造麻烦和牟利，就有白帽子从事漏洞发掘与防御。</p><p>而进入腾讯之前，吴石是世界安全业界著名的白帽黑客，也是全球提交漏洞数量最多的个人。</p><p>传说他是个货真价实的宅男，曾有一段时间他加入了微软，工作状态是这样的：从来不去微软中国，也不去微软美国，但部门领导每年都会从美国飞过来家里看他。每年20多个可用漏洞的KPI他只用了一个月就能完成，还顺便买了套房。</p><p>之所以能有这样的待遇，是因为早在14年前，吴石一个人就发掘超过100个 Safari 的 CVE（“Common Vulnerabilities &amp; Exposures”，公共漏洞和暴露）漏洞。他曾创造过<b>单月申报微软漏洞数量全球占比10%、独自发掘15个iOS漏洞</b>的惊人成绩——这个数字甚至比同期苹果自家研究人员发现漏洞（6个）还要多。</p><p>2013年，吴石加入KEEN Team，这个由业内顶尖白帽黑客组成的信息安全研究队伍，在 Pwn2Own Mobile大会上两度破解 Safari 。吴石等团队骨干也在回国后不久加入腾讯，成立了科恩实验室，正式成为腾讯T5科学家。</p><p>科恩实验室把使命定义为<b>“守护全网用户的信息安全”</b>，不仅研究桌面端安全、移动端安全，还向物联网，在智能网联汽车、IoT 安全、云计算和虚拟化技术安全各个领域发力。</p><p>最让大众印象深刻的，是科恩实验室<b>实现了全球首次“远程无物理接触方式”入侵特斯拉汽车。</b>他们无需走进汽车，就可以将特斯拉解锁、打开天窗、启动雨刷、远程刹车，创造力过剩的他们甚至让特斯拉跟随音乐的节拍进行车灯秀。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-76ffdbf5139a0809ce06333583db14c0_b.gif"" data-size=""normal"" data-rawwidth=""640"" data-rawheight=""333"" data-thumbnail=""https://pic4.zhimg.com/v2-76ffdbf5139a0809ce06333583db14c0_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""640"" data-original=""https://pic4.zhimg.com/v2-76ffdbf5139a0809ce06333583db14c0_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='640'%20height='333'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""640"" data-rawheight=""333"" data-thumbnail=""https://pic4.zhimg.com/v2-76ffdbf5139a0809ce06333583db14c0_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""640"" data-original=""https://pic4.zhimg.com/v2-76ffdbf5139a0809ce06333583db14c0_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-76ffdbf5139a0809ce06333583db14c0_b.gif""><figcaption>特斯拉被远程控制开始灯光秀</figcaption></figure><blockquote>视频详情请<a href=""http://link.zhihu.com/?target=https%3A//v.qq.com/x/page/r0024jcxaai.html%3Fstart%3D192"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">点击&gt;&gt;&gt;</a></blockquote><p>“特斯拉过去的安全负责人Chris Evans，原来是谷歌互联网安全团队‘Project Zero’的负责人。我们团队从2014年底开始和谷歌在安全研究上就有合作，所以我们和Chris关系很好，相互欣赏对方的技术研究能力。这次我们和特斯拉的全程沟通过程中，坚持了一个观点，就是<b>推动特斯拉尽快修复这些高危问题，确保特斯拉用户的安全。</b>”吴石曾经就破解特斯拉对媒体这样说过。</p><p>为了表彰科恩实验室的贡献，吴石的团队获得了特斯拉官方在2016年12月公开致谢以及4万美金奖励，并成为年度唯一一支获得“特斯拉安全研究员名人堂”的团队，这一消息甚至惊动了马斯克，它们收到了硅谷“钢铁侠”的亲笔致谢信。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-3f2713f5224659ee4fce30f56801bc12_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""798"" class=""origin_image zh-lightbox-thumb"" width=""600"" data-original=""https://pic2.zhimg.com/v2-3f2713f5224659ee4fce30f56801bc12_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='600'%20height='798'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""600"" data-rawheight=""798"" class=""origin_image zh-lightbox-thumb lazy"" width=""600"" data-original=""https://pic2.zhimg.com/v2-3f2713f5224659ee4fce30f56801bc12_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-3f2713f5224659ee4fce30f56801bc12_b.jpg""></figure><p>科恩实验室从中完成的技术积累，也在腾讯云上作为云安全产品和安全解决方案实现对外开放。2017年腾讯云发布的车联网安全解决方案，便是从安全管理、概念风险验证、产品研发到技术支持等阶段，为车联安全提供防护。</p><p><br></p><h2>▶<b>贾佳亚</b> with<b>优图实验室</b>：要做落地价值的科学家</h2><p><br></p><p>就像2018年万众讨论区块链的日子似乎已是遥远的记忆，人们很难再回想起2016年AI创业的火爆场景：钱找人，却找不到几个人，但凡是个初创产品，想办法也要往AI上面靠靠，说不定能拿到钱。</p><p>风口过后，真正应用到AI技术的场景开始进入沉淀的发展阶段。于是有一种声音感慨：“AI也就这样了。”</p><p>但正是在这时候，贾佳亚决定走出学校，他的观点恰恰与前一种声音相反：AI能做的太多，却像基础设施一般，需要长期的投入。</p><p>作为香港中文大学终身教授，贾佳亚是业内著名的计算机视觉专家，在大学任职期间创立的视觉实验室，很多研究成果被高校教科书、课件和开源视觉代码库（包括OpenCV）收录，落地到各大厂商，还培养出了还包括商汤科技的CEO 徐立博士在内等众多人才。</p><figure data-size=""small""><noscript><img src=""https://pic4.zhimg.com/v2-d90427f0b9b77db9a5b3bb90c33c16eb_b.jpg"" data-size=""small"" data-rawwidth=""600"" data-rawheight=""900"" class=""origin_image zh-lightbox-thumb"" width=""600"" data-original=""https://pic4.zhimg.com/v2-d90427f0b9b77db9a5b3bb90c33c16eb_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='600'%20height='900'&gt;&lt;/svg&gt;"" data-size=""small"" data-rawwidth=""600"" data-rawheight=""900"" class=""origin_image zh-lightbox-thumb lazy"" width=""600"" data-original=""https://pic4.zhimg.com/v2-d90427f0b9b77db9a5b3bb90c33c16eb_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-d90427f0b9b77db9a5b3bb90c33c16eb_b.jpg""><figcaption>贾佳亚</figcaption></figure><p>众多公司向他伸来橄榄枝，几经权衡，他加入了腾讯优图实验室。在此之前，优图已经在国际最权威、难度最高的海量人脸识别数据库MegaFace中击败Google等团队，<b>以83.290%的最新成绩在100万级别人脸识别测试（Challenge1/FaceScrub identification）中刷新了世界纪录。</b></p><p>在这样的研究基础上，他准备大干一场。超级大牛带来的向心力开始显现：随着贾佳亚的到来，许多高端的研发人才也纷纷将目光投向互联网公司，不少博士生在面试时就答得很干脆——想跟着贾佳亚做事情、学东西。</p><p>很快贾佳亚就为优图建立起了几十人的博士团队，此外在全球高校还不乏在读的硕士、博士生希望加入到优图的中短期的科研项目中。</p><p>万事俱备，贾佳亚开始了他的腾讯生涯：他要把优图的AI能力带到C端和B端产业，带到腾讯内部的社交和内容平台和外部的传统工业、医疗、交通等领域去。</p><p>在B端产业里面，现阶段工业生产目前面临很多困难：人力成本升高且培训周期长，许多工作又会因为过于琐碎，导致人员不稳定，最智能的“人”反而成为生产链条中薄弱的一环，贾佳亚正是从中看出了AI施展拳脚的机会。</p><p>他很快找到了那颗合适的“钉子”：国内面板龙头企业华星光电。优图提供的可复制性工业自动化排检系统，辅助华星光电完成100多道工序检测，从而让质检人数减少了60%。更重要的是这套系统还可以帮工厂为缺损面板分析，从而回溯源头查漏补缺，改进生产工艺，这是AI带来的新能力。</p><p>除此之外，贾佳亚还带着团队深入到<b>高危检测、癌症早筛、图像视频内容理解、企业办公、文化保护、自动驾驶</b>这些领域，多以优图提供底层技术、搭建框架和技术体系、腾讯云对外提供整体方案的形式进行配合。</p><p>“CDN、网络服务储存这些硬件技术很成熟，成本和造价都比较透明，大多数的竞争只能比谁降价多，在同等计算能力下比谁便宜；但算法和技术是上层建筑，现在还是有独门秘方和容易比较的高低好坏。<b>比别人做的早、效果更好、结果更准、更受欢迎，那它就能凭借价值而不是价格去吸引客户。</b>我们在做的就是这样的高附加值的技术。”贾佳亚如是说。</p><p>这样的价值，也正在成为腾讯云的核心优势。</p><p><br></p><h2>▶<b>刘杉</b> with<b>音视频实验室</b>：让业界标准也有腾讯的一份功劳</h2><p>想象一下：一部两小时、60帧每秒、1980×1080像素的电影，如果没有经过压缩，会有多大？答案是<b>2.7TB</b>，注意，是TB（1TB=1000GB），不是我们日常见到的GB。</p><p>从“不可用”到“可用”，这就需要视频压缩技术不断突破并形成行业标准。</p><p>“行业标准”这个词，刘杉并不陌生，加入腾讯之前，她曾在多家全球500强公司担任高级技术和管理职务，参与制定了视频压缩、点云压缩、多媒体系统和传输协议、IoT、无线网络在内的多个领域的行业国际标准。以她作为第一和主要贡献人的50余篇技术提案被ITU-T 和ISO/IEC 标准采纳。她还曾多次担任国际标准组织专家小组主席和联席主席，是超过200个美国和全球专利申请的发明人，也是ITU-T H.265 | ISO/IEC HEVC V4七位主编之一。</p><figure data-size=""small""><noscript><img src=""https://pic3.zhimg.com/v2-0b5cfa14584cdd4d8712ebc49eab3092_b.jpg"" data-size=""small"" data-rawwidth=""675"" data-rawheight=""900"" class=""origin_image zh-lightbox-thumb"" width=""675"" data-original=""https://pic3.zhimg.com/v2-0b5cfa14584cdd4d8712ebc49eab3092_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='675'%20height='900'&gt;&lt;/svg&gt;"" data-size=""small"" data-rawwidth=""675"" data-rawheight=""900"" class=""origin_image zh-lightbox-thumb lazy"" width=""675"" data-original=""https://pic3.zhimg.com/v2-0b5cfa14584cdd4d8712ebc49eab3092_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-0b5cfa14584cdd4d8712ebc49eab3092_b.jpg""><figcaption>刘杉</figcaption></figure><p>头衔和荣誉早已对刘杉失去吸引力，当时间来到2017年，一个机会摆在刘杉面前：加入腾讯，带领中国企业参与制定行业标准。</p><p>她的第一反应和普通人一样：为何是腾讯来找她？</p><p>“腾讯的目标是出海和国际化，音视频领域一定要做成国际标准。”汤道生这样对她说。</p><p>为什么音视频的国际标准，值得日理万机的高管亲力亲为去招人？因为真的很——重——要。</p><p>音视频标准的进化，正在潜移默化地改变我们的生活方式。<b>就像十年前的我们，根本不能想象自己还能拿着手机随意直播，而不是像电视台那样扛着长枪大炮，不远处还得跟一辆转播车。</b>同样，语音通话、长短视频、直播、即时游戏等这些用户习以为常的服务，也离不开这样一步步摸爬滚打的改进。</p><p>这也是腾讯音视频技术中心在2016年底升级为腾讯音视频实验室的原因，随着移动化浪潮的到来，他们需要为腾讯在未来准备就绪。</p><p>在海外工作生活二十年之后，有机会带领中国互联网巨头参与制定国际行业标准，影响十亿量级的用户，这是这份工作最吸引人的地方，刘杉答应了。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-06ae7867a245553b7eca7a2965420e80_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""810"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-06ae7867a245553b7eca7a2965420e80_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='810'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""810"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic1.zhimg.com/v2-06ae7867a245553b7eca7a2965420e80_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-06ae7867a245553b7eca7a2965420e80_b.jpg""></figure><p>她在短短两个月时间内，与音视频实验室在硅谷和深圳两个团队日夜接力钻研，硬生生从一众老牌国标团队中挤出一条路，向 MPEG 122 会议提交了十个高质量的提案。</p><p>此后腾讯在音视频标准界一发不可收拾——在2018 年 7 月的卢布尔雅纳标准会议、10月的澳门会议上，腾讯音视频实验室的多项技术连续被 VVC 标准采纳，其中王者荣耀视频片段更是被纳入 VVC标准制定测试集，确保腾讯重要应用场景受益于新一代视频压缩标准。<b>“除了VVC，音视频实验室也在积极参与制定PCC、DASH、OMAF、AVS、IETF等行业标准。”</b>刘杉介绍道，她本人也再一次被推举为下一代视频压缩标准的联合主编。</p><p>在行业标准陆续取得突破的同时，音视频实验室也在酝酿自己的黑科技。2018年，音视频实验室开发了基于深度学习的视频处理平台“丽影”并上线微视。2019年，音视频实验室正式上线了TSE（包含面向屏幕内容编码优化技术的视频编解码内核），可<b>在视频压缩上节省带宽约30%，同时保证通讯实时性，</b>这将是众多会议达人的福音。</p><p>在2018年9月30日腾讯战略升级后，<b>辅助腾讯云“出海”</b>也成为音视频实验室的一项重任。其最先为QQ 开发的音视频通话能力、面向全民K歌的直播能力、面向游戏的GME游戏多媒体引擎，均已作为云上解决方案对外开放。视频云转码内核和平台、智能会议、在线教育和自动驾驶等领域，将会是刘杉和她的团队下一个挑战的目标。</p><p><br></p><h2>▶<b>张胜誉</b> with<b>量子实验室</b>：做我们这一行，耐心和聪明同样重要</h2><p><br></p><figure data-size=""small""><noscript><img src=""https://pic1.zhimg.com/v2-e81e90a4d6f34bf04c29ca4bb58702ce_b.jpg"" data-caption="""" data-size=""small"" data-rawwidth=""640"" data-rawheight=""962"" class=""origin_image zh-lightbox-thumb"" width=""640"" data-original=""https://pic1.zhimg.com/v2-e81e90a4d6f34bf04c29ca4bb58702ce_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='640'%20height='962'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""small"" data-rawwidth=""640"" data-rawheight=""962"" class=""origin_image zh-lightbox-thumb lazy"" width=""640"" data-original=""https://pic1.zhimg.com/v2-e81e90a4d6f34bf04c29ca4bb58702ce_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-e81e90a4d6f34bf04c29ca4bb58702ce_b.jpg""></figure><p>2001年，张胜誉还未去普林斯顿大学跟着姚期智读博士，已经开始接触量子计算，但彼时几乎所有人都觉得量子计算实现可能会是“下辈子”的事情。</p><p>尽管量子力学在过去的100多年里得到长足的发展，但人类控制和测量量子系统的能力依然薄弱。</p><p>“那时候我们很少有人拍胸脯说自己研究量子计算。”因为这会让人敬而远之，他笑道，早些年他不确定有生之年能够看到量子计算的应用。</p><p>然而后来的事情，超过了所有人的想象：</p><blockquote>2007年，神秘的加拿大公司D-Wave System 突然宣布造出量子计算机原型机 Orion<br>2015年，谷歌联合其他研究机构宣布实现9个超导量子比特的高精度操纵<br>2017年，我国科学家首次实现10个超导量子比特的纠缠<br>2018年，谷歌发布全球首个72量子比特通用量子计算机芯片<br>2019年，IBM推出全球首款据称可「商用」的量子计算机</blockquote><p>短短十几年间，谷歌、微软、IBM、Intel、Honeywell、Amazon乃至于通用汽车、波音公司等科技巨头，包括世界各国的科研机构在内，纷纷加码量子计算。尽管现阶段人类距离量子计算的商业化尚远，行业内甚至没有人能够精确预言量子计算未来能够在哪里一定会改变世界，反而是外行商业炒作愈发火热，“量子工业时代”、“量子霸权”等浮夸的新词甚嚣尘上，各类创业项目纷纷拿钱上马。</p><p><b>“好比一群17世纪的人在想象未来手机的功能。”</b>有业内人士这样评价。</p><p>“这是一个可能持续十年、二十年的事业。”在还是香港中文大学计算机系副教授的时候，张胜誉就明确告诉腾讯，量子计算的未来不可估量，但需要充分的耐心和投入，希望商业公司审慎对待。此时他已经担任Theoretical Computer Science及International Journal of Quantum Information杂志的编委，对量子计算有了十几年的研究。</p><p><b>在「长远福祉」和「短期利益」面前，腾讯选择了前者。</b>这份诚意打动了张胜誉，他在2018年1月加入腾讯并组建「量子实验室」，开始深港两地漂的日子。</p><p>顶着腾讯最神秘实验室的光环，量子实验室从一开始就不打算太在意外界的喧嚣。他先招人，招那些能够坐得住的人。“做我们这一行，耐心和聪明同样重要。”他笑道——当然他没有说的是，能研究量子计算的人本身已经智商超群。</p><p>其次是定方向。这个行业从理论、算法到硬件都是一道道山，IBM、谷歌、微软等企业各自选择了不同的路径去攀登。腾讯要想找到最合适的道路，就得先全方位了解这个行业正在发生的方方面面。</p><p><b>“我们不相信任何一个人给我们的直接结论，不管这个人过去的成就和地位。”</b>张胜誉说道。“量子实验室一直将许多精力放在行业最新文献和当面的技术交流上，从细节出发，与各方面专家认认真真探索，帮助腾讯在量子领域始终掌握着第一手资讯，并逐渐摸索出自己的方向。”</p><p>“但——”他又神秘地笑了笑，表示量子实验室目前在量子计算化学领域开始了不错的布局，这可能会对传统药企、化工企业研发起到很大的帮助，这一能力他们正在想办法在腾讯云上对企业进行开放，也许会以云产品进行发布。</p><p>量子计算+云生态，将会是什么样的能力？</p><h2>▶<b>于旸</b> with<b>玄武实验室</b>：宝藏男孩的“挖宝”之路</h2><p><br></p><p>在知乎上有这样一个问题：当你的能力处在你所在行业的顶端或前端时，是一种什么样的体验？</p><p>有一个答案获得了244个赞，位居第一：</p><p><br></p><b><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-10d6286816975eeb0059bb7824bf32f7_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""713"" class=""origin_image zh-lightbox-thumb"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-10d6286816975eeb0059bb7824bf32f7_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='713'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1080"" data-rawheight=""713"" class=""origin_image zh-lightbox-thumb lazy"" width=""1080"" data-original=""https://pic3.zhimg.com/v2-10d6286816975eeb0059bb7824bf32f7_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-10d6286816975eeb0059bb7824bf32f7_b.jpg""></figure></b><p><br></p><p>这个网名 <a class=""member_mention"" href=""http://www.zhihu.com/people/7baaf3c00c2c7daeddf8e9ba813af141"" data-hash=""7baaf3c00c2c7daeddf8e9ba813af141"" data-hovercard=""p$b$7baaf3c00c2c7daeddf8e9ba813af141"">@tombkeeper</a> 的答主，也就是如今玄武实验室的掌门人于旸，人称“TK教主”。了解这个宝藏男孩的人都知道这句话绝非抖机灵，因为你永远猜不出见多识广的教主会从哪里给你带来惊喜。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-b74782c48b85de2fe3a115f3b7fb8bba_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1034"" data-rawheight=""706"" class=""origin_image zh-lightbox-thumb"" width=""1034"" data-original=""https://pic1.zhimg.com/v2-b74782c48b85de2fe3a115f3b7fb8bba_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1034'%20height='706'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1034"" data-rawheight=""706"" class=""origin_image zh-lightbox-thumb lazy"" width=""1034"" data-original=""https://pic1.zhimg.com/v2-b74782c48b85de2fe3a115f3b7fb8bba_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-b74782c48b85de2fe3a115f3b7fb8bba_b.jpg""></figure><p>医科出身、半路出家的他，因为大学期间偶然在路边报摊读到一篇介绍网络安全的文章，从此走上了网络安全的道路。凭着在校期间<b>业余研究</b>的成果，他从医学院毕业后直接获得了一份国内著名安全公司的工作。2008年还担任了<b>奥运会信息网络安全指挥部技术专家。</b></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-aad7477bf814a7eff187a9cd46d389d4_b.jpg"" data-size=""normal"" data-rawwidth=""371"" data-rawheight=""497"" class=""content_image"" width=""371""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='371'%20height='497'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""371"" data-rawheight=""497"" class=""content_image lazy"" width=""371"" data-actualsrc=""https://pic2.zhimg.com/v2-aad7477bf814a7eff187a9cd46d389d4_b.jpg""><figcaption>于旸</figcaption></figure><p>2013年，微软因他的一项研究改变了从不给安全研究者发放奖金的规则，并设立全球安全挑战赛。他则迅速凭借另一项研究成为了<b>全球仅有的三名</b>获得微软安全挑战悬赏奖10万美元奖金的白帽子黑客之一。</p><p>2014年TK加入腾讯成立了玄武实验室。</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-d46fdf77df974c07a3751293a40d96fa_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""300"" data-rawheight=""300"" class=""content_image"" width=""300""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='300'%20height='300'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""300"" data-rawheight=""300"" class=""content_image lazy"" width=""300"" data-actualsrc=""https://pic2.zhimg.com/v2-d46fdf77df974c07a3751293a40d96fa_b.jpg""></figure><p>“玄武”是中华传统文化的符号之一。TK认为“玄武”形象中厚重的龟和灵动的蛇恰好代表了网络安全的防和攻两个方面。从此TK开启了自己的“腾讯时期”。原以为赢得微软技术挑战大奖是自己“一辈子所能做出最重要的发现”的TK，在创立玄武实验室后则收获了一个又一个新的成果：</p><blockquote>● 2015年玄武实验室发现的扫码器“BadBarcode”漏洞，神奇到甚至可以通过发射一束激光来控制扫码器入侵电脑。而且这一安全隐患在全世界大部分扫码器中都存在。此后玄武实验室和微信支付合作，一起推动了整个国内扫码器行业在安全上的大幅提升。<br><br>● 2015年玄武实验室的生态安全小组发现：全世界一半的杀毒软件都存在破坏操作系统安全机制的问题，以至于黑客可以“踩”着杀毒软件冲破系统。在帮助众多厂商修复这些漏洞后，这项研究也成为了温哥华 CanSecWest安全会议的议题。<br><br>● 2016年发现的“BadTunnel”漏洞影响了从Windows 95到Windows 10之间的所有系统，无论影响范围之广还是利用方式之灵活都堪称史无前例。微软破格为此发了5万美元奖金。<b>而且据说这个漏洞不是找出来的，是TK有一次乘飞机百无聊赖时凭空想出来的。</b><br><br>● 2017年玄武实验室提出 “应用克隆”模型，彻底刷新了人们对移动安全的认知。在这个模型的视角下，很多之前被认为影响不大的安全问题实际上都非常危险。玄武实验室研究发现：对大部分安卓用户来说，只要发送一条短信，就可以控制用户手机上的应用。<br><br>● 2018年玄武实验室发现了手机行业新出现的 “屏下指纹技术”中存在的“残迹重用”漏洞，并帮助国内各大手机厂商修复了问题。这个漏洞原理非常复杂而表现却很简单。在GeekPwn大会现场，甚至主持人黄健翔也很快学会了如何一秒钟解锁存在漏洞的手机。由于这个漏洞巨大的影响，各手机公司都对玄武实验室给予了很高评价。华为手机安全部门的负责人甚至专程带队赶到北京召开了答谢会。<br><br>● 对于2019年初出现的WinRAR漏洞，很多研究团队都意识到这个漏洞不止影响WinRAR，但很难给出较为全面的影响列表。而玄武实验室利用多年积累研发出的 <b><a href=""http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI1NzM0MTMzMg%3D%3D%26mid%3D2247486526%26idx%3D1%26sn%3Dc30b07b1d23f7810827193d9a13a357f%26scene%3D21%23wechat_redirect"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">“阿图因”系统</a></b>，迅速找到了39款受影响的软件。这个结果除了帮助很多企业用户应对这一严重漏洞，也为国家主管机构对该漏洞的应急处置提供了有力支持，获得了国家信息安全漏洞共享平台的感谢。</blockquote><p><br></p><p>这样吸睛的新闻，只是玄武实验室众多研究成果和项目的冰山一角。事实上，除了玄武实验室、科恩实验室，腾讯还有云鼎实验室、湛泸实验室、反病毒实验室、反诈骗实验室和移动安全实验室，这七大专业实验室一共组成了<b>腾讯安全联合实验室，</b>汇聚了国际最顶尖的白帽黑客，专注于安全技术研究和安全攻防体系的搭建，腾讯云业内领先的安全能力，也多来源于此。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-5b208f8c41c540db9cbaea64a920f905_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""900"" data-rawheight=""592"" class=""origin_image zh-lightbox-thumb"" width=""900"" data-original=""https://pic3.zhimg.com/v2-5b208f8c41c540db9cbaea64a920f905_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='900'%20height='592'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""900"" data-rawheight=""592"" class=""origin_image zh-lightbox-thumb lazy"" width=""900"" data-original=""https://pic3.zhimg.com/v2-5b208f8c41c540db9cbaea64a920f905_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-5b208f8c41c540db9cbaea64a920f905_b.jpg""></figure><p><br></p><p>从这些让人眼花缭乱的成就，可以想见每一次关于T5的任命消息出炉，在腾讯内外会引发怎样的关注。</p><p>T5的队伍依旧在不断壮大中，标准也愈发严苛，下一位科学家将会是谁？尚未得知，也许还远，也许很近，也许就将磨砺于产业互联网的回声中。</p><p><br></p><p>封面图：<a href=""http://link.zhihu.com/?target=https%3A//pixels.com/featured/giant-angel-carlene-smith.html"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Giant Angel by Carlene Smith</a></p><p>文章来源：腾讯云（公众号）</p>",4640,550,,腾讯,https://api.zhihu.com/people/c1e3881713aff51f577fdce931b9f525,赞同了文章
1551862764612,BERT实战（源码分析+踩坑）,,,,,https://api.zhihu.com/articles/58471554,,,,1551862764,,"<p>        最近在内部技术分享会上发现大家对Bert实践中的问题比较感兴趣，疑问点主要集中在，Bert机器资源代价昂贵，<b>如何用较小成本（金钱和时间）把Bert跑起来？</b>因此，希望这篇文章能帮助你在实践Bert的过程中少走一些弯路。</p><p>        整个文章结构分成四部分。</p><p><b>          第一部分Bert代码速读，提示Bert代码中容易忽略的关键点，目的是让你快速的熟悉代码并且跑起来</b></p><p><b>          第二部分总结下我在服务化部署Bert中趟过的一些坑</b></p><p><b>          第三部分参考资料（同样有干货）</b></p><p><b>          第四部分总结性能和效果，给出实践Bert最低成本路径。</b></p><ul><li><b>一、Bert代码速读</b></li></ul><p>        这一部分代码来源是google research 在github上发布的官网链接：<a href=""http://link.zhihu.com/?target=https%3A//github.com/google-research/bert"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">google-research/bert</a>讲讲代码中容易忽略但是很重要的点，帮助你在较短时间内实践Bert，所要掌握的必要代码。解析Google research官方发布的Bert源码(给出连接)的主要结构，重点讲run_<i>classifier.py，run_squad.py，modeling.py中模型构建的核心代码</i>。</p><p>      1.Bert代码结构</p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-a77b534846c87fc99816abc42481539e_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1244"" data-rawheight=""588"" class=""origin_image zh-lightbox-thumb"" width=""1244"" data-original=""https://pic2.zhimg.com/v2-a77b534846c87fc99816abc42481539e_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1244'%20height='588'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1244"" data-rawheight=""588"" class=""origin_image zh-lightbox-thumb lazy"" width=""1244"" data-original=""https://pic2.zhimg.com/v2-a77b534846c87fc99816abc42481539e_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-a77b534846c87fc99816abc42481539e_b.jpg""></figure><p>   2.两个微调模型run_<i>classifier.py和run_squad.py的create _model 部分核心代码。</i></p><figure data-size=""normal""><noscript><img src=""https://pic2.zhimg.com/v2-925c42991bd202c937d167c1319937ee_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1958"" data-rawheight=""1398"" class=""origin_image zh-lightbox-thumb"" width=""1958"" data-original=""https://pic2.zhimg.com/v2-925c42991bd202c937d167c1319937ee_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1958'%20height='1398'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1958"" data-rawheight=""1398"" class=""origin_image zh-lightbox-thumb lazy"" width=""1958"" data-original=""https://pic2.zhimg.com/v2-925c42991bd202c937d167c1319937ee_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-925c42991bd202c937d167c1319937ee_b.jpg""></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-1bb58f9981cade50f27b5ed91bcd7f5c_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1976"" data-rawheight=""1398"" class=""origin_image zh-lightbox-thumb"" width=""1976"" data-original=""https://pic1.zhimg.com/v2-1bb58f9981cade50f27b5ed91bcd7f5c_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1976'%20height='1398'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1976"" data-rawheight=""1398"" class=""origin_image zh-lightbox-thumb lazy"" width=""1976"" data-original=""https://pic1.zhimg.com/v2-1bb58f9981cade50f27b5ed91bcd7f5c_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-1bb58f9981cade50f27b5ed91bcd7f5c_b.jpg""></figure><p>3.预训练模型层modeling.py 中的attention_layer代码，包含原理图和代码解析。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-d1f7b001240bf94933df9c6d871b0ac3_b.jpg"" data-size=""normal"" data-rawwidth=""461"" data-rawheight=""557"" class=""origin_image zh-lightbox-thumb"" width=""461"" data-original=""https://pic4.zhimg.com/v2-d1f7b001240bf94933df9c6d871b0ac3_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='461'%20height='557'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""461"" data-rawheight=""557"" class=""origin_image zh-lightbox-thumb lazy"" width=""461"" data-original=""https://pic4.zhimg.com/v2-d1f7b001240bf94933df9c6d871b0ac3_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-d1f7b001240bf94933df9c6d871b0ac3_b.jpg""><figcaption>attention layer原理图</figcaption></figure><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-7bdc38c2ffa7dfa146b91547d52b64a9_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1958"" data-rawheight=""1412"" class=""origin_image zh-lightbox-thumb"" width=""1958"" data-original=""https://pic4.zhimg.com/v2-7bdc38c2ffa7dfa146b91547d52b64a9_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1958'%20height='1412'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1958"" data-rawheight=""1412"" class=""origin_image zh-lightbox-thumb lazy"" width=""1958"" data-original=""https://pic4.zhimg.com/v2-7bdc38c2ffa7dfa146b91547d52b64a9_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-7bdc38c2ffa7dfa146b91547d52b64a9_b.jpg""></figure><p>        预训练阶段，对机器和数据量要求高，所幸，作者提供了主要语言英语和中文的预训练模 型，直接下载即可，<a href=""http://link.zhihu.com/?target=https%3A//storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">中文预训练模型</a>。因此，我们重点关注的是，如何构建和利用微调模型实现我们的目标。重点讲下作者在源码中给出的两个微调模型。看完之后，你可以会惊呼微调模型竟然这么简单。<b>用run_classifier.py，整个50万样本量，微调阶段训练时间约为半个小时。</b></p><p>        从代码中可以看到，run_<i>squad.py和run_classifier.py微调模型是一层简单的全链接层，</i>以此类推，如果你要实现命名实体识别等其他目标任务，可在预训练的模型基础上，加入少了全链接层。</p><ul><li><b>二、我趟过的一些坑</b></li></ul><p><b>1.tensorflow服务化部署的坑。</b></p><p>       Tensorflow服务化部署有好几套接口，有版本历史原因，导致相互之间不兼容，对用户来说可谓非常不友好。我这边提供一个可用的接口方法供参考。 <a href=""http://link.zhihu.com/?target=http%3A//shzhangji.com/cnblogs/2018/05/14/serve-tensorflow-estimator-with-savedmodel/"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">TensorFlow 模型如何对外提供服务</a>。 微调模型跑出结果模型是checkpoint的文件格式，需转化为.pt格式提供出来。</p><p><b>2.TPU改成GPU estimator</b></p><p>       官网源码中给出的是TPU estimator接口，改成普通estimator接口方案就能跑起来了。</p><a href=""http://link.zhihu.com/?target=https%3A//www.tensorflow.org/guide/estimators"" data-draft-node=""block"" data-draft-type=""link-card"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">https://www.</span><span class=""visible"">tensorflow.org/guide/es</span><span class=""invisible"">timators</span><span class=""ellipsis""></span></a><p>给个示例，<b>run_classifier.py</b>中关于TPU estimator的修改，直接上代码吧</p><p>（1）main()函数中estimator定义部分的修改 </p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-30c06fa43e38c5dec2bfa8ecfda48917_b.jpg"" data-size=""normal"" data-rawwidth=""1518"" data-rawheight=""450"" class=""origin_image zh-lightbox-thumb"" width=""1518"" data-original=""https://pic4.zhimg.com/v2-30c06fa43e38c5dec2bfa8ecfda48917_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1518'%20height='450'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""1518"" data-rawheight=""450"" class=""origin_image zh-lightbox-thumb lazy"" width=""1518"" data-original=""https://pic4.zhimg.com/v2-30c06fa43e38c5dec2bfa8ecfda48917_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-30c06fa43e38c5dec2bfa8ecfda48917_b.jpg""><figcaption>源代码中的定义</figcaption></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-3785193420edbf91ce7b255eba2fd4c6_b.jpg"" data-size=""normal"" data-rawwidth=""1478"" data-rawheight=""144"" class=""origin_image zh-lightbox-thumb"" width=""1478"" data-original=""https://pic1.zhimg.com/v2-3785193420edbf91ce7b255eba2fd4c6_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1478'%20height='144'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""1478"" data-rawheight=""144"" class=""origin_image zh-lightbox-thumb lazy"" width=""1478"" data-original=""https://pic1.zhimg.com/v2-3785193420edbf91ce7b255eba2fd4c6_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-3785193420edbf91ce7b255eba2fd4c6_b.jpg""><figcaption>修改后的定义</figcaption></figure><p>(2) model_fn()部分代码修改，给个train部分的示例，eval部分同理可得。                 </p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-2554cef7ec8b1e00fc4d24a50a698e94_b.jpg"" data-size=""normal"" data-rawwidth=""2328"" data-rawheight=""674"" class=""origin_image zh-lightbox-thumb"" width=""2328"" data-original=""https://pic4.zhimg.com/v2-2554cef7ec8b1e00fc4d24a50a698e94_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2328'%20height='674'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""2328"" data-rawheight=""674"" class=""origin_image zh-lightbox-thumb lazy"" width=""2328"" data-original=""https://pic4.zhimg.com/v2-2554cef7ec8b1e00fc4d24a50a698e94_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-2554cef7ec8b1e00fc4d24a50a698e94_b.jpg""><figcaption>源码</figcaption></figure><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-1955b02ec6df93626de1caabfdf3471d_b.jpg"" data-size=""normal"" data-rawwidth=""2400"" data-rawheight=""836"" class=""origin_image zh-lightbox-thumb"" width=""2400"" data-original=""https://pic1.zhimg.com/v2-1955b02ec6df93626de1caabfdf3471d_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2400'%20height='836'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""2400"" data-rawheight=""836"" class=""origin_image zh-lightbox-thumb lazy"" width=""2400"" data-original=""https://pic1.zhimg.com/v2-1955b02ec6df93626de1caabfdf3471d_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-1955b02ec6df93626de1caabfdf3471d_b.jpg""><figcaption>修改后</figcaption></figure><p><b>3.Out of Memory问题</b></p><p>        官网源码中Readme.md中有关于Out of Memory解决的方法，如果你遇到类似问题，一定要先看这部分文档。文中意思大概是调节两个参数<b><code>max_seq_length</code>和<code>train_batch_size</code></b>，观察你GPU显存的占用情况   。我使用的GPU显存28GB，如果你想把微调模型跑起来，这个显存基本够了（train_batch_size=64,--max_seq_length=128 显存占用22GB）</p><ul><li><b>三、参考资料</b></li></ul><p><b>1.Bert as Service  </b><a href=""http://link.zhihu.com/?target=https%3A//github.com/hanxiao/bert-as-service"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">hanxiao/bert-as-service</a></p><p><br></p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-5d9a8cd35f83311c2841dd2f268efb54_b.jpg"" data-size=""normal"" data-rawwidth=""2422"" data-rawheight=""694"" class=""origin_image zh-lightbox-thumb"" width=""2422"" data-original=""https://pic3.zhimg.com/v2-5d9a8cd35f83311c2841dd2f268efb54_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2422'%20height='694'&gt;&lt;/svg&gt;"" data-size=""normal"" data-rawwidth=""2422"" data-rawheight=""694"" class=""origin_image zh-lightbox-thumb lazy"" width=""2422"" data-original=""https://pic3.zhimg.com/v2-5d9a8cd35f83311c2841dd2f268efb54_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-5d9a8cd35f83311c2841dd2f268efb54_b.jpg""><figcaption>图优化方法</figcaption></figure><p>       Bert预训练模型较为完整的服务化部署方法，预训练模型可作为NLP基础服务。<b>源码中两个亮点：一是提供了图优化的方法，</b>提升效率和降低显存消耗。Freezed图冻结把tf.Variable变为tf.Constant，Pruned去掉训练时多余的节点，Quantized降低浮点数维度，比如把int64改为int32。<b>二是zeromq实现异步并发请求</b>，设计了一套Bert服务化部署的软件架构。</p><p><b>2.参考我的另一篇博文，关于Bert的原理</b><a href=""https://zhuanlan.zhihu.com/p/46997268"" class=""internal"">章鱼小丸子：NLP突破性成果 BERT 模型详细解读</a></p><ul><li><b>四、总结</b></li></ul><p>关于Bert的效果，我未做定量分析，但从个人评估结果来看，其在公开数据集的泛化能力，明显优于利用词向量预训练的QAnet等问答其他模型。</p><p>       关于Bert的性能，对服务进行压力测试，根据应用场景，我调节的max length=30，耗时均值在400ms左右，能满足一般应用qps要求。如果对运算速度要求更高的产品，需改为GPU分布式计算。</p><p><b>      较低成本实践Bert的路径：</b></p><p><b>      第一步：找一台满足GPU显存要求的机器（一般是28GB左右，不同情况略有不同）</b></p><p><b>      第二步：设置一个你能拿到数据集的微调任务，如分类、问答、实体标注等。</b></p><p><b>       第三步：修改微调代码跑起来，验证效果。</b></p><p></p><p></p><p></p><p></p>",390,39,,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,发表了文章
1546068710300,人工智能（AI）与教育有哪些结合点？,,,,,https://api.zhihu.com/questions/63008649,,,,1546068710,,,,1,,姜鑫,https://api.zhihu.com/people/a074d205ff44c4267e5be58e05bc2fe7,关注了问题
1545647838392,知识图谱是否是NLP的未来？,最近在空想，知识图谱是否是nlp的未来，如果神经网络自己能通过训练存储所有知识信息，也就没必要用知识图谱了？ 比如图像检测，就可以认为神经网络存储了各个物体长相的知识,27,1,459,https://api.zhihu.com/questions/267242467,TechOnly,https://api.zhihu.com/people/dfa09089a0d521de903a0f0d81bf97d3,1518823488,1545647838,1545647838,<p>我认为正好相反，nlp是知识图谱未来的方向。</p>,10,0,1,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1543760723468,机器学习该怎么入门？,本人大学本科，对机器学习很感兴趣，想从事这方面的研究。在网上看到机器学习有一些经典书如Bishop的PRML， Tom Mitchell的machine learning，还有pattern classification，不知该如何入门？那本书比较容易理解？,334,15,28841,https://api.zhihu.com/questions/20691338,匿名用户,,1357304890,1543760723,1543760723,"<p>我觉得除了知识层面的学习还有能力的培养。</p><p><b> 如何系统性的学习一个领域呢？一个很有效的方法，把这个领域的知识点分为初中高级，以及潜在所需具备的能力，组成一个领域的""知识图谱""，逐一攻克。</b>当然，结合你平常的一些项目，可能不是直线式学习，而是跳跃式学习。比如，我现在要做ocr，要求我具备的阶段，可能直接从初级跳到高级，但你需要会过头来补充一些初级和中级所必须掌握的知识点。下面是我大概画出的一个成长roadmap，你的可以跟我的不一样，并且在学习实践中不断丰富它。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""2058"" data-rawheight=""848"" data-default-watermark-src=""https://pic4.zhimg.com/v2-df2615eeb665badb6e06b4071992c0fc_b.jpg"" class=""origin_image zh-lightbox-thumb"" width=""2058"" data-original=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2058'%20height='848'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""2058"" data-rawheight=""848"" data-default-watermark-src=""https://pic4.zhimg.com/v2-df2615eeb665badb6e06b4071992c0fc_b.jpg"" class=""origin_image zh-lightbox-thumb lazy"" width=""2058"" data-original=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_b.jpg""></figure><p>   在这里，我认为对算法工程师来说，最基础的软实力是数据敏感度。数据敏感度作用是什么？不知道你工作生活中，是否遇到过这样的伙伴，你觉得他技术一般，但每次模型调出来的效果准确率都挺高的，这种人就是我们一般对数据比较敏感。数据比较敏感的人，能洞见异样数据。发现数据之间的内在规律，能找到关键的特征数据，对模型的选用以及模型调参大有帮助。产品或运营岗位，同样也要学习这样的能力，从大量统计的PV、UV等数据指标中，推测其背后主要因素。<b>如何培养数据敏感度呢？把你自己大脑想像成一个含有快照功能的照相机，snapshot输入输出大量数据，试图从里面找到数据的联系及内在规律，尝试通过改动模型、调节参数、改变特征等手段，判断预想结果是否朝你想要的方向发展。在这条路上一直走的还算顺利，跟入门一年大量的""snapshot""训练有很大关系。</b></p><p> 学习机器学习为什么需要跨领域思考能力呢。深度学习中包含大量抽象概念，比如卷积为什么能做特征提取，可能需要你用物理、信息论等基础学科的理论来解释。而物理这样的基础学科为什么能解释这样复杂抽象的数学模型呢？机器学习中涉及的所有数据，几乎来自互联网、人类语言、图像。而这些数据其实是物理世界的映射，这样的大数据的存在能有效的还原物理世界。<b>机器学习做的事情，就是从大量能映射物理世界的数据中，提取其能真实反应物理世界的本质规律。</b></p><p><br></p><p><b>具体参考：</b><a href=""https://zhuanlan.zhihu.com/p/51392903"" class=""internal"">机器学习中的第一性原理（一）</a></p><p></p>",0,0,1,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1543759813681,机器学习中的第一性原理（一）,,,,,https://api.zhihu.com/articles/51392903,,,,1543759813,,"<p>        最两年随着人工智能的大火，越来越多的朋友想往往这个领域发展。人工智能这波热潮不同于早几年云计算和移动互联网不同，人工智能技术起点较高需要精深的专业知识，学习曲线过于陡峭，快速学习比较困难。同时，随着这波热潮，很多非本专业的人才也想深入了解这个领域，一探究竟它到底在讲什么。百度出了一门关于机器学习的课程，用户画像数据显示关注量最高的岗位竟然是HR。但如果要要想真的了解机器学习的精髓，其实需要在这个领域通过：理论学习-&gt;实践-&gt;思考-&gt;理论学习-&gt;实践-&gt;思考，这个循环中，至少做两到三年。</p><p>       从我自己的个人经历来说，我研究生方向是软件架构，非机器学习科班出身。我觉得可以给转型想从事这个方向的非专业人士，一些学习方面的建议，以便快速的入门。同时也想了解这个领域深入原理的人士，谈一谈我在从事这个领域中的一些思考。个人浅见，欢迎指正（拍砖）。</p><p>       由于时间仓促，这篇博文，我会分两部分来完成。第一部分我根据自己的经验，总结的机器学习的方法论。第二部分，将机器学习中运用到的基础学科的基本原理，及其应用场景，在延伸在其他领域的应用（也就是我这篇博文为什么叫《机器学习中的第一性原理》的原因）</p><p><b>第一部分：算法人才需要什么样的素质？算法人才从初级到高级成长的路线。</b></p><p><b>       我这里的算法是指机器学习的算法，具体细分领域如知识图谱、nlp、ocr等。 主要对比服务端程序员，据我了解，大部分想转型的是前后端出身的朋友。这一部分内容是给想入门的朋友，提供一个系统性学习路径的建议，以便学习起来事半功倍。</b></p><p><b>      第二部分：机器学习中的一些基础性原理，应用场景，与基础科学数学、物理学、信息论的联系。并从基础原理中衍生出与其他领域的联系。</b></p><p><b>      这一部分内容主要是为了非该领域的朋友，为你想更深入的了解其中原理，提供思考的方向。</b></p><p>一、算法人才需要什么样的素质？</p><p>          个人总结服务端技术人才和算法人才的核心能力如下图所示。</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-6b251f43a7357a62e84451b4854c6658_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1218"" data-rawheight=""700"" class=""origin_image zh-lightbox-thumb"" width=""1218"" data-original=""https://pic1.zhimg.com/v2-6b251f43a7357a62e84451b4854c6658_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1218'%20height='700'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1218"" data-rawheight=""700"" class=""origin_image zh-lightbox-thumb lazy"" width=""1218"" data-original=""https://pic1.zhimg.com/v2-6b251f43a7357a62e84451b4854c6658_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-6b251f43a7357a62e84451b4854c6658_b.jpg""></figure><p>        服务端人才的核心能力我总结为以下三点。在这三个能力里面，我觉得系统性思维是最底层的能力，我们谈到以工程师视角来看待世界，就是系统性思维。把一个大的系统分解成一个个模块，每个模型保证其可移植、解耦、可扩展。类似一个组织系统中，分解成各个模块，让其高效运转，各司其职，局部最优化达到集体最优化。另外，体现系统性思维的是软件架构里讲的分层，分层的目的是在大用户量产品比如上亿用户数产品中，如何实现分流、负载均衡等。严密的逻辑思维和较强编码能力是后端工程师的法宝，代码不出bug，靠的就是这个。</p><p>        而算法技术人才，最基础的思维方法，是大数据思维，就是我说的纵向思维（逻辑思维是横向思维）。所有机器学习的算法不过是纵向统计计算概率值。而深度学习为什么需要大语料，因为少量语料统计出来的概率分布与实际情况有所偏差。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-abd898c57ebb36492688f3d2704c1d51_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1106"" data-rawheight=""888"" class=""origin_image zh-lightbox-thumb"" width=""1106"" data-original=""https://pic4.zhimg.com/v2-abd898c57ebb36492688f3d2704c1d51_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1106'%20height='888'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1106"" data-rawheight=""888"" class=""origin_image zh-lightbox-thumb lazy"" width=""1106"" data-original=""https://pic4.zhimg.com/v2-abd898c57ebb36492688f3d2704c1d51_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-abd898c57ebb36492688f3d2704c1d51_b.jpg""></figure><p>         对算法人才来说，纵向思维是基础，而深度思考能力、抽象能力、数学基础是决定能走多远的能力。深度学习大部分模型较为复杂且抽象，至于他为什么这样好，那么不好，在于你思考的角度。比如transformer中加入attention，为什么要加入attention，很多时候不是靠灵光一动，而是你层层剖析，一步步在深度思考中往下探求其本质。</p><p><b>        关于数学能力，我写的是""对数学的理解以及数学建模的能力""</b>。不是我们应试教育所学的数学，应试教育数学强调的是计算。证明、推导能力。因此，我不建议重温大学数学教材。对数学的理解以及数学建模的能力具体讲，对于做算法的人才，并不要求你成为一名数学家，写出黎曼猜想的证明，<b>更多情况，你的工作可能面对的是需要给城市交通公交车路线规划一套算法，更实际的，产品的推荐系统中设计一套新的算法，实现PV，UV的增长，与组织一起承担商业目标</b>。这时候，需要你基于现实情况和应用场景，把问题抽象为数学模型的能力。</p><p><b>那如何培养这样的数学能力呢？一方面看看国外教材和教学视频。</b>优秀的国外教材更强调一个原理它是如何发明的，这个过程非常重要，其目的是培养一种思考问题的方法。<b>另一方面，结合现实情况多思考，建立起这样的心智模式和思维惯性。</b>比如你听网易云音乐的时候，想想如何生成一个歌单推荐的算法；打滴滴的时候，想想它的溢价算法有什么变化，在不同情况下，如何定价。这样思考的目的，不是为了得到所谓正确答案，而是你如何在你的大脑里，把现实情况与理论模型联系起来，培养这样的思维惯性。</p><p>       最近看了一篇博文《如何像爱因斯坦那样加倍提升自己的脑力》，听到研究员罗伯特在他的著作《天才的火花》中提到：""年轻的爱因斯坦全面接受了现代科学家所说的“思想实验”的教育：观察和感受一个物理环境，操纵其中的各个元素，观察它们的变化，所有这些都是在头脑中想象出来的。""任何思维方式的培养，离不开平常有意识的训练。</p><p>       大部分人会以为工程（后端、前端）转算法岗会比较容易，<b>从我个人经历的角度来说，其实两者岗位要求的核心能力相差较大</b>。可以说如果你铁了心想深入的做算法，并不太容易。话又说回来，如果你认识到两者能力要求相差较大，并愿意以归零的心态来对待，也不是那么难。看到很多热门博客提到，较强的编码能力也是算法人才必须掌握的。因为我之前写过几年代码，它不会成为困扰我的问题。但从发展潜力来考虑，我个人认为：数学&gt;编码。试想数学能学的好的话，学习基础的编码能力根本不成问题。</p><p>二、算法人才从初级到高级成长的路线</p><p><b>       如何系统性的学习一个领域呢？一个很有效的方法，把这个领域的知识点分为初中高级，以及潜在所需具备的能力，组成一个领域的""知识图谱""，逐一攻克。</b>当然，结合你平常的一些项目，可能不是直线式学习，而是跳跃式学习。比如，我现在要做ocr，要求我具备的阶段，可能直接从初级跳到高级，但你需要会过头来补充一些初级和中级所必须掌握的知识点。下面是我大概画出的一个成长roadmap，你的可以跟我的不一样，并且在学习实践中不断丰富它。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""2058"" data-rawheight=""848"" class=""origin_image zh-lightbox-thumb"" width=""2058"" data-original=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2058'%20height='848'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""2058"" data-rawheight=""848"" class=""origin_image zh-lightbox-thumb lazy"" width=""2058"" data-original=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-9804ea3bcef9a944bbc1c3ce0f806bc5_b.jpg""></figure><p>       我上面灰色文字标明的一些知识点，这里不累述了。下面的软实力，我重点说数据敏感度和跨领域思考能力，其他的我觉得是通用能力，并且很多博文都涉及到过。</p><p>       在这里，我认为对算法工程师来说，最基础的软实力是数据敏感度。数据敏感度作用是什么？不知道你工作生活中，是否遇到过这样的伙伴，你觉得他技术一般，但每次模型调出来的效果准确率都挺高的，这种人就是我们一般对数据比较敏感。数据比较敏感的人，能洞见异样数据。发现数据之间的内在规律，能找到关键的特征数据，对模型的选用以及模型调参大有帮助。产品或运营岗位，同样也要学习这样的能力，从大量统计的PV、UV等数据指标中，推测其背后主要因素。<b>如何培养数据敏感度呢？把你自己大脑想像成一个含有快照功能的照相机，snapshot输入输出大量数据，试图从里面找到数据的联系及内在规律，尝试通过改动模型、调节参数、改变特征等手段，判断预想结果是否朝你想要的方向发展。在这条路上一直走的还算顺利，跟入门一年大量的""snapshot""训练有很大关系。</b></p><p>        学习机器学习为什么需要跨领域思考能力呢。深度学习中包含大量抽象概念，比如卷积为什么能做特征提取，可能需要你用物理、信息论等基础学科的理论来解释。而物理这样的基础学科为什么能解释这样复杂抽象的数学模型呢？机器学习中涉及的所有数据，几乎来自互联网、人类语言、图像。而这些数据其实是物理世界的映射，这样的大数据的存在能有效的还原物理世界。<b>机器学习做的事情，就是从大量能映射物理世界的数据中，提取其能真实反应物理世界的本质规律。</b></p>",3,1,,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,发表了文章
1543550880861,知识图谱是否是NLP的未来？,,,,,https://api.zhihu.com/questions/267242467,,,,1543550880,,,,1,,TechOnly,https://api.zhihu.com/people/dfa09089a0d521de903a0f0d81bf97d3,关注了问题
1542290574503,如何通俗地理解人工神经网络？,题主是基础医学方向的研究生，最近在阅读一篇生物信息学方面的文献。因为不是生信专业的，所以查阅资料后对人工神经网络的概念仍然有些不理解。故此求助，感谢。,3,1,10,https://api.zhihu.com/questions/301642650,林克,https://api.zhihu.com/people/4b059c860b19c6c3252ca098f29ec23d,1541590004,1542290574,1542290574,<p>一层神经网络你可以理解为线性函数，多层神经网络可以理解为<b>非线性函数。</b></p><p><b>这个非线性函数究竟长什么样，由几个方面共同决定：</b></p><p><b>1.你定义它大概长什么样（用CNN、RNN还是多个全连接）</b></p><p><b>2.你觉得它不应该什么样（加入L1范式，L2范式）</b></p><p><b>3.训练集长什么样。</b></p><p><br></p><p><b>神经网络和其他机器学习算法没什么本质上的区别。</b></p>,0,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1539911785016,机器之心,,,,,https://api.zhihu.com/columns/jiqizhixin,,,,1539911785,,,,,,匿名用户,,关注了专栏
1539909756062,"点击率预测 线上测试都有那些环节, 需要注意什么？",关于广告点击率预测， 在完成离线测试之后，准备线上测试并推广的时候， 具体还有哪些步骤？线上部分是怎样实现的？有哪些需要注意的地方？,1,0,4,https://api.zhihu.com/questions/299105113,Soju,https://api.zhihu.com/people/fe7c55a1bd1dca74a567bc1b912f613c,1539900820,1539909756,1539909756,<p>上线前完成：</p><p>1.压力测试</p><p>2.需要准备离线备份数据（服务挂掉的时候有兜底方案）</p><p>3.打点，后续需要评估点击率，为优化迭代做准备。</p><p>4.准备率评估规范制定，完成准确率评估，为后续优化迭代做准备。</p>,0,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1539765321208,如何评价 BERT 模型？,据说目前最强的 NLP 模型 BERT 的论文被谷歌AI组放出来了https://arxiv.org/pdf/1810.04805.pdf 其在 GLUE 上的效果排名第一（https://gluebenchmark.com/leaderboard）,50,5,4376,https://api.zhihu.com/questions/298203515,秦洛,https://api.zhihu.com/people/99b5278c105301815f94cbd5390508d5,1539318186,1539765321,1539765321,"<p>      个人觉得如果你大概了解近两年NLP的发展的话，Bert模型的突破在情理之中，大多思想是借用前人的突破，比如双向self attention的想法是借助这篇论文《Semi-supervised sequence tagging with bidirectional language models》。并且，他提出的一些新的思想，是我们自然而然就会想到的。（十一在家的时候，在做问答模型的时候，我就在想，为什么不能把前一个句子和后一个句子作为标注数据，组成一个二分类模型来训练呢。）</p><p>       整片论文最有价值的部分，我认为是预训练的两种方法，不需要大量标注数据，在工程实践和一些NLP基础训练中具有很大借鉴意义。</p><p>       自然语言处理领域2017年和2018年的两个大趋势：一方面，模型从复杂回归到简单。另一方面，迁移学习和半监督学习大热。这两个趋势是NLP从学术界向产业界过渡的苗头，因为现实情况往往是，拿不到大量高质量标注数据，资源设备昂贵解决不了效率问题。</p><p>关于《BERT》模型的详细解读可以参考这里：</p><p><a href=""https://zhuanlan.zhihu.com/p/46997268"" class=""internal"">NLP突破性成果《BERT》模型详细解读</a></p>",16,4,1,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1539763629598,自适应学习中，为什么会同时用到教育数据挖掘和贝叶斯知识跟踪？,教育数据挖掘和贝叶斯知识跟踪在自适应学习中，都是用来预测学生未来的学习表现，它们的区别在哪？,2,0,7,https://api.zhihu.com/questions/298800742,自己流浪,https://api.zhihu.com/people/b8f89396e6cc6e7f0ac8679552fb69ec,1539697748,1539763629,1539763629,<p>问法有问题：教育数据挖掘是一个大的领域，贝叶斯知识跟踪是一种算法。</p>,2,1,1,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1539575687803,2019秋招nlp算法岗形势如何？,,3,0,23,https://api.zhihu.com/questions/298458035,印拓,https://api.zhihu.com/people/a01c55fcdaccd3a9c41747d67273109e,1539491309,1539575687,1539575687,<p>形势应该不错。云龙混杂的太多，缺的是真正有实力的。</p>,0,2,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1539575610810,如何对特征向量赋予权重？,"用word2vec训练出词向量之后，如何对重要程度不同的词向量赋予权重，类似于自注意力那样的机制，比如说""我 今天 很 忙"" 这句话里面可能就是""忙""最重要，因此要对""忙""这个词向量赋予更大的权重，怎么得到这个权重呢？得到之后权重直接数乘词向量可以吗？类似于a*word_vector(""忙"")，其中a是权重",3,0,5,https://api.zhihu.com/questions/298527208,「已注销」,https://api.zhihu.com/people/4b04a14464cd589f2904b3e7b89642fc,1539524420,1539575610,1539575610,<p>方法如下：</p><p>1.对每个词算出一个TF-IDF值，再与词向量相乘。</p><p>2.得到每个词的词性，选出具有重要意义的词，乘以一个权重。</p><p>3.利用命名实体识别、专名识别的方法识别出实体，对实体词乘以一个权重。</p>,4,0,1,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1538980933543,小白想问一下NLP中如何判断一句话表述的是否合理，除了n-gram，HMM、CRF可以用吗，怎么用了？,,3,0,10,https://api.zhihu.com/questions/297631204,Insist,https://api.zhihu.com/people/1bdc42e0ed7827013b29019fe139fc33,1538969037,1538980933,1538980933,<p>n-gram可以判断一句话是否通顺。基本原理就是一句话中根据前几个词，预测下一个词出现的概率，训练出一个语言模型。</p><p></p>,0,3,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1538823556530,有哪些人工智能上的事实，没有一定人工智能知识的人不会相信？,相关问题： 有哪些计算机的事实，没有一定计算机知识的人不会相信？ 有哪些历史上的事实，没有一定历史知识的人不会相信？ 有哪些地理上的事实，没有一定地理知识的人不会相信？ 有哪些物理学上的事实，没有一定物理学知识的人不会相信？ 有哪些材料科学上的事实，没有一定材料学知识的人不会相信？ 有哪些天文学上的事实，没有一定天文学知识的人不会相信？ 有哪些生物学上的事实，没有一定生物学知识的人不会相信？ 有哪些化学上的事实，没有一定化学知识的人不会相信？ 有哪些数学上的事实，没有一定数学知识的人不会相信？ 有哪些地质学上的事实，没有一定地质学知识的人不会相信？ 有哪些医学上的事实，没有一定医学知识的人不会相信？ 有哪些农业上的事实，没有一定农学知识的人不会相信？ 有哪些语言学上的事实，没有一定语言学知识的人不会相信？,110,7,1724,https://api.zhihu.com/questions/290014218,赵光香,https://api.zhihu.com/people/1add95dd29a0931a88d74bb4ebcd78b8,1534125934,1538823556,1538092864,<p>数学远比代码本身重要</p>,16,0,2,Eternity-Myth,https://api.zhihu.com/people/54d8a334306418daa052325675c93619,赞同了回答
1525163358469,如何评价作业帮、猿辅导、企鹅辅导2018都涨价？,,11,0,16,https://api.zhihu.com/questions/275256258,在线,https://api.zhihu.com/people/802de7436b565d234fe13d38d297c685,1524983559,1525163358,1525163358,我理解k12教育行业本身不是对价格很敏感的行业。打价格战在这个行业没有太大意义。家长不会把自己孩子交给一个很便宜但教学质量不高的老师。,1,0,1,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1525163266941,如何评价作业帮、猿辅导、企鹅辅导2018都涨价？,,,,,https://api.zhihu.com/questions/275256258,,,,1525163266,,,,0,,在线,https://api.zhihu.com/people/802de7436b565d234fe13d38d297c685,关注了问题
1523519230832,为什么交叉熵（cross-entropy）可以用于计算代价？,两个数的差值可以表示距离，这个很好理解，但是为什么交叉熵也可以用于计算“距离”？,31,1,1509,https://api.zhihu.com/questions/65288314,刘博,https://api.zhihu.com/people/4751c3974182d4ef9e5ba3f7555118c5,1505283486,1523519230,1505421860,"<p>如果咱不想去深度纠结里头的数学，那就记住这几个结论，一样可以理解交叉熵的意义。我尽量回溯理解思路，如果说到题主已然理解的知识点，跳过就好:)</p><p>第一，使用交叉熵的模型，<b>输出一定是概率</b>！那什么叫输出的是概率呢？先不考虑任意维度的输出，就假设咱的输出是 <img src=""https://www.zhihu.com/equation?tex=y+%3D+%28y_1%2C+y_2%2C+y_3%29"" alt=""y = (y_1, y_2, y_3)"" eeimg=""1""> ，也就是说，一个三维向量。那么对合法的 <img src=""https://www.zhihu.com/equation?tex=y"" alt=""y"" eeimg=""1""> 而言，必须保证</p><ol><ol><li><img src=""https://www.zhihu.com/equation?tex=y_1%2By_2%2By_3%3D1"" alt=""y_1+y_2+y_3=1"" eeimg=""1""> ，总概率必须为 <img src=""https://www.zhihu.com/equation?tex=1"" alt=""1"" eeimg=""1""> ，这个不抽象吧。</li><li><img src=""https://www.zhihu.com/equation?tex=y_1%2C+y_2%2C+y_3+%5Cgeq+0"" alt=""y_1, y_2, y_3 \geq 0"" eeimg=""1""> ，概率不能为负，这个也很合理吧。</li></ol></ol><p>上述两个条件粗略来看其实就是概率测度三条公理中的前两条，不过这是题外话。</p><p>那么，如何保证输出一定符合这俩条件呢，因为我们的模型一定是千奇百怪，如果是神经网络，那输出层是一个仿射变换的结果，比如说，长这样 <img src=""https://www.zhihu.com/equation?tex=%280.8%2C-1.2%2C0.0%29"" alt=""(0.8,-1.2,0.0)"" eeimg=""1""> ，有正有负，和还不为 <img src=""https://www.zhihu.com/equation?tex=1"" alt=""1"" eeimg=""1""> 。所以，为了将之变成概率，我们又要引入另一个函数叫 softmax。公式我就不给了，网上一大把。若把 <img src=""https://www.zhihu.com/equation?tex=%280.8%2C-1.2%2C0.0%29"" alt=""(0.8,-1.2,0.0)"" eeimg=""1""> 丢到 softmax 里头去，结果会得到 <img src=""https://www.zhihu.com/equation?tex=%280.63%2C0.09%2C0.28%29"" alt=""(0.63,0.09,0.28)"" eeimg=""1""> ，是不是和为 <img src=""https://www.zhihu.com/equation?tex=1"" alt=""1"" eeimg=""1""> ？而且各项都大于等于 <img src=""https://www.zhihu.com/equation?tex=0"" alt=""0"" eeimg=""1""> ？</p><p>对三个参数是这样，对任意 <img src=""https://www.zhihu.com/equation?tex=n"" alt=""n"" eeimg=""1""> 个参数也是这样。很多模型中，softmax 和 cross-entropy 是不分家的，在 tensorflow 中，有一个方法就叫 softmax_cross_entropy_with_logits，意思就是对 logits 先 softmax，后 cross-entropy。所谓 logits 就是指还没有被概率化的输出。</p><p>第二，<b>label 也必须是概率</b>。这个不难理解，毕竟模型的输出和学习的目标得用同一种“语言”，或者用数学的话说，必须在同一个空间中，才容易操作。以分类为例，一个数据集，每一个输入，数据集都会提供你一个对应的 label。以烂大街的 MNIST 数据集为例，输入是一张张固定大小的正方形图片，上面有手写的数字。每张图片对应一个 label，取值范围在 <img src=""https://www.zhihu.com/equation?tex=0"" alt=""0"" eeimg=""1""> 到 <img src=""https://www.zhihu.com/equation?tex=9"" alt=""9"" eeimg=""1""> ，分别代表张图片上写的是啥数字。如果题主有亲自操作过一些 MNIST 数据集学习的教程的话，记不记有一步操作，要将 lable 转为<b> one-hot 向量。</b>比方说，你的 label 是 <img src=""https://www.zhihu.com/equation?tex=5"" alt=""5"" eeimg=""1""> ，那么对应的 one-hot 形式就是</p><p><img src=""https://www.zhihu.com/equation?tex=%5Cqquad+%280%2C0%2C0%2C0%2C0%2C1%2C0%2C0%2C0%2C0%29"" alt=""\qquad (0,0,0,0,0,1,0,0,0,0)"" eeimg=""1""></p><p>有没有发觉这是符合概率定义的？加和为 <img src=""https://www.zhihu.com/equation?tex=1"" alt=""1"" eeimg=""1""> ，且都大于等于 <img src=""https://www.zhihu.com/equation?tex=0"" alt=""0"" eeimg=""1""> 。one-hot 操作从本质来说，就是把 lable 空间变换到一个概率测度空间。注意，这是一个<b>条件概率</b>，而且大白话版定义如下</p><p><img src=""https://www.zhihu.com/equation?tex=%5Cquad+l_%7B%5Ctext%7Bone-hot%7D%7D+%3D+%28p_0%2C+%5Ccdots%2C+p_k%2C+%5Ccdots+%2C+p_9%29"" alt=""\quad l_{\text{one-hot}} = (p_0, \cdots, p_k, \cdots , p_9)"" eeimg=""1""></p><p><img src=""https://www.zhihu.com/equation?tex=%5Cquad+p_k+%3D+%5Ctext%7BPr%7D%28%5Ctext%7Bthe+given+picture+is+digit+%7D+k+%5Cmid+%5Ctext%7Bgiven+picture%7D%29"" alt=""\quad p_k = \text{Pr}(\text{the given picture is digit } k \mid \text{given picture})"" eeimg=""1""></p><p>且</p><p><img src=""https://www.zhihu.com/equation?tex=%5Cquad+p_k+%3D+1+%5Cquad+%5Ctext%7Bif+the+label+tells+me+that+the+given+picture+is+digit+%7Dk+"" alt=""\quad p_k = 1 \quad \text{if the label tells me that the given picture is digit }k "" eeimg=""1""></p><p><img src=""https://www.zhihu.com/equation?tex=%5Cquad+p_k+%3D+0+%5Cquad+%5Ctext%7Bif+the+label+tells+me+that+the+given+picture+is+a+digit+other+than+%7Dk+"" alt=""\quad p_k = 0 \quad \text{if the label tells me that the given picture is a digit other than }k "" eeimg=""1""></p><p>不好意思用英文，但是 LaTex 里头的中文字体太难看了。。。如果你按照上面的定义，就可以把每一个 <img src=""https://www.zhihu.com/equation?tex=0"" alt=""0"" eeimg=""1""> 到 <img src=""https://www.zhihu.com/equation?tex=9"" alt=""9"" eeimg=""1""> 的 label 转化为其 one-hot 形式，为啥那么简单的事情定义那么繁琐？这就是数学。。。不过，说好不深究数学的。不过有一点要注意，那就是提到的条件概率。包括你模型要预测的也是一个条件概率。概率的东西这儿就不展开，但是如果题主不满足于会使用模型，而是要理解模型为什么可行，概率论和统计是怎么也绕不掉的内容。说概率模型统计模型已经被当代的深度学习淘汰的人就是忽略了一个事实，那怕是神经网络，也是一种概率模型。用到了 one-hot？用到了 cross-entropy? 这就是概率啊！就算一个模型没有显式地使用了概率，要分析其性能，从概率的角度研究也几乎是唯一的途径。</p><p><b>在人类很笨，且对世界本质含混不清的情况下，研究概率是理解现象的唯一途径。—— 我说的</b></p><p>第三，在强调了<b>模型输出是概率，label 也是概率</b>的前提下，我们就可以说到 cross-entropy 了。式子我也不写了，只要记住如下几个事实。</p><ol><li>cross-entropy 的输入是两个概率。</li><li><b>两个概率在相同时，cross-entropy 取得的值最小。而且，离得越“远”，结果就差得越“远”。</b>不信你可以试试， <img src=""https://www.zhihu.com/equation?tex=%5Cmathbb+H%280.2%2C+0.2%29+%3C+%5Cmathbb+H%280.2%2C+0.5%29%3C+%5Cmathbb+H%280.2%2C+0.8%29"" alt=""\mathbb H(0.2, 0.2) &lt; \mathbb H(0.2, 0.5)&lt; \mathbb H(0.2, 0.8)"" eeimg=""1""> ，多维情况也一样， <img src=""https://www.zhihu.com/equation?tex=%5Cmathbb+H%5Cbig%28%280%2C0%2C1%29%2C+%280%2C0%2C1%29%5Cbig%29+%3C+%5Cmathbb+H%5Cbig%28%280%2C0%2C1%29%2C+%280.2%2C0.3%2C0.5%29%5Cbig%29"" alt=""\mathbb H\big((0,0,1), (0,0,1)\big) &lt; \mathbb H\big((0,0,1), (0.2,0.3,0.5)\big)"" eeimg=""1"">。正是这样的性质，使得 cross-entropy 可以作为一种<b>伪距离</b>而存在。</li><li>cross-entropy <b>操作不对称</b>，即，<img src=""https://www.zhihu.com/equation?tex=%5Cmathbb+H+%280.2%2C+0.5%29+%5Cneq+%5Cmathbb+H+%280.5%2C+0.2%29"" alt=""\mathbb H (0.2, 0.5) \neq \mathbb H (0.5, 0.2)"" eeimg=""1""> 。或者说，<b>cross-entropy 不符合距离定义</b>。尤其，别把模型输出和 one-hot label 掉个个儿丢到 cross-entropy 里，不然，<img src=""https://www.zhihu.com/equation?tex=%5Clog%280%29"" alt=""\log(0)"" eeimg=""1""> 看计算机怎么想。</li></ol><p>如果不深究，那么就是一点点思辨的过程了。既然 cross-entropy 作代价函数，它在俩概率相同的时候结果最小。那么我们将 cross-entropy 对模型输出做梯度下降，然后更新模型，那么更新后的模型的输出概率是不是离 one-hot label 更近了呢？因为相同时最小啊！如果是你面对的是一个数据集，且一次丢进模型的是多个数据，那你就求 cross-entropy 之平均，然后再求梯度下降之平均，更新后模型的概率输出在趋势上就离 one-hot label 更近了些。</p><p>最后，谢邀。这回答对 cross-entropy 函数怎么来的并没有涉及太多，这属于信息论内容。但却反复强调了模型输出是概率，one-hot label 是概率。在我看来，的确，如果真正理解了这点，不论用 cross-entropy，还是其他答主提到的 KL-divergence，还是使用可以定义为距离的 JS-divergence，都可以信手拈来。</p><p>如有误，欢迎指正。</p>",153,21,24,xf3227,https://api.zhihu.com/people/99aa85f2b54189288facd3215ee79f14,赞同了回答
1523517023297,利用文本向量如何使用余弦定理计算在局部范围内两个短语之间的相似度？,1.对于短语文本向量化，TF-TDF权重方式处理是否会影响接下相似度计算值的差异过大？ 2.对于一个待匹配短语与符合条件局部范围短语集计算匹配值，既要获得最优相似度又获得相似度靠前的结果。向量文本余弦计算方法是否是可以满足。,1,0,2,https://api.zhihu.com/questions/271762609,爱菠萝的老奶奶,https://api.zhihu.com/people/a4ef13385b5cd714b3df33798fd8275e,1523352379,1523517023,1523517023,<p>这两个问题稍微有些绕。</p><p>第一个问题，我试过的结果，是加入TFIDF计算权重效果会更高。相当于对文本特征做加权。</p><p>第二个问题，表示没看懂。</p>,0,1,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1523144328507,短视频“抖音”是如何火起来的？,,,,,https://api.zhihu.com/articles/26915473,,,,1523144328,,"<p>要说五月份最火的一款APP，我觉得当属抖音APP。</p><p><b>抖音是什么鬼？</b></p><p>最简单来说就是一款类似小咖秀的对嘴型的恶搞APP。</p><p>稍微简单点来说就是一款音乐创意短视频的App，在这个APP上用户可以拍摄15秒的音乐短视频。</p><p>复杂点来说这款APP是今日头条旗下孵化的一款产品，而且还是跟前阵子被央视曝光的涉黄直播平台“火山直播”是同一个阿妈生的，也就是北京微播视界科技有限公司的产品。<br></p><p>小宇哥查了百科先知，抖音上线于去年9月份，在今年年初获得今日头条的数百万元天使投资。在抖音上，用户可以选择歌曲，配以短视频，形成自己的作品。在功能方面，它其实与之前的小咖秀非常相似，不同的是，抖音在技术层面做了一些丰富，用户可以通过视频拍摄快慢、视频编辑、特效(反复、闪一下、慢镜头)等。又是一个不正常的视频APP，抓住了年轻人就等于抓住了money啊！<br></p><figure><noscript><img src=""https://pic2.zhimg.com/v2-daad21703e2fd9ab52614bfe9814ff3f_b.jpg"" data-rawwidth=""1240"" data-rawheight=""364"" class=""origin_image zh-lightbox-thumb"" width=""1240"" data-original=""https://pic2.zhimg.com/v2-daad21703e2fd9ab52614bfe9814ff3f_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1240'%20height='364'&gt;&lt;/svg&gt;"" data-rawwidth=""1240"" data-rawheight=""364"" class=""origin_image zh-lightbox-thumb lazy"" width=""1240"" data-original=""https://pic2.zhimg.com/v2-daad21703e2fd9ab52614bfe9814ff3f_r.jpg"" data-actualsrc=""https://pic2.zhimg.com/v2-daad21703e2fd9ab52614bfe9814ff3f_b.jpg""></figure><br><p>通过上面的百度指数可以看到，在今年3月之前，抖音的搜索指数一直都为0。而直到3月17日就开始出现明显的上升趋势，这个趋势是呈指数级上升的，那这个3月份，抖音到底发生了什么事，可以一夜之间出现我们的眼前。</p><p>这篇文章我们就单从运营的角度来分析抖音为何会火起来！</p><blockquote><p>明星效应始终都是一款产品进行冷启动的关键</p></blockquote><p>刚才我们说到在3月中下旬开始，抖音一下子就走进了用户的视线，很多做运营的人都已经猜到，抖音要不是投了广告宣传，就是做了什么活动或者拉个红人来一起推。是的，在3月13日，岳云鹏转发了一条微博下面是带着抖音APP的logo的，第二天指数就蹿升至2000多，并在此后的几个月内成逐渐上升的趋势。</p><figure><noscript><img src=""https://pic1.zhimg.com/v2-8031c874eb7aa33bac351e1f2be617e0_b.jpg"" data-rawwidth=""600"" data-rawheight=""605"" class=""origin_image zh-lightbox-thumb"" width=""600"" data-original=""https://pic1.zhimg.com/v2-8031c874eb7aa33bac351e1f2be617e0_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='600'%20height='605'&gt;&lt;/svg&gt;"" data-rawwidth=""600"" data-rawheight=""605"" class=""origin_image zh-lightbox-thumb lazy"" width=""600"" data-original=""https://pic1.zhimg.com/v2-8031c874eb7aa33bac351e1f2be617e0_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-8031c874eb7aa33bac351e1f2be617e0_b.jpg""></figure><br><p>在抖音视频本身就是一款充满娱乐性和趣味性以及很符合当下年轻人对鬼畜文化的喜欢，在后面开始有点势头之后，很多明星也开始加入进来体验这款APP。比如胡彦斌就在抖音用新歌《没有选择》做背景音乐，发起了一个音乐视频挑战活动，效果很好，甚至有胡彦斌以前的伴舞也参加了这个挑战，拍摄的视频有超过2W 的点赞。其他明星还有钟丽缇、杜海涛等等，而现在抖音短视频团队也是主推明星效应，现在用户下载APP只要打开APP，就能看到首页给你推荐的一些明星玩抖音的短视频，所以明星效应给了抖音进行冷启动很大的帮助。</p><blockquote><p>内容运营是基础</p></blockquote><p>在内容运营方面，第一点其主要结合了母公司今日头条的算法优势，在内容推荐和分发上，去中心化。也就是说一个没有任何粉丝的普通人，只要拍的好，一样能收割大量关注度。这不是今日头条版的抖音吗？所以才会有越来越多的普通年轻人愿意在这个地方玩，在这里表达自我。</p><figure><noscript><img src=""https://pic3.zhimg.com/v2-d40306df6e48394e473921b84e6a2915_b.jpg"" data-rawwidth=""1293"" data-rawheight=""1080"" class=""origin_image zh-lightbox-thumb"" width=""1293"" data-original=""https://pic3.zhimg.com/v2-d40306df6e48394e473921b84e6a2915_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1293'%20height='1080'&gt;&lt;/svg&gt;"" data-rawwidth=""1293"" data-rawheight=""1080"" class=""origin_image zh-lightbox-thumb lazy"" width=""1293"" data-original=""https://pic3.zhimg.com/v2-d40306df6e48394e473921b84e6a2915_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-d40306df6e48394e473921b84e6a2915_b.jpg""></figure><br><p>第二点就是内容运营团队善于结合当下时事热点，然后再稍微改造成年轻人喜欢的恶搞短视频。比如前一段时间《乡村爱情》谢腾飞的表情包在网上走红。这是很典型的互联网话语，谢腾飞脱离开乡村爱情，被互联网赋予了新的流行符号的意义。然后抖音运营团就更进一步，把谢腾飞乡村爱情里的音乐，和 Thug Life 结合起来，发起了#谢腾飞Thug Life的挑战。而Thug Life 这两年的风潮是源于Thug Life在贴图和短视频恶搞上的走红，其中像素墨镜和大金链是经典元素。最后两种文化符号的混搭，产生了一种”越是本土化越是国际化”的感觉，不到一周就有超过 1 万多用户参与挑战。而我们其实也可以发现，不管是在微博上还是APP上，其发起的一些接力挑战等等一些玩法，看得出，其运营团队在把握这些热点跟短视频相结合的节奏是越来越成熟，这也是一个对运营节奏的把控。</p><blockquote><p>公关传播的恰到好处</p></blockquote><p>其实现在不管是短视频创业还是其他行业的创业，在产品还没有完全向市场铺开时，或者就是用户还不知道有你这个品牌时，很多运营团队就会急急忙忙的开始投各种各样的公关文，进行公关的传播。但其实我们可以看到抖音在没有明星开始玩之前，也是很多用户不知道这款APP之前，他是没有做这方面的努力的。但是在用户慢慢知道这款APP后，其实还是有一点品牌影响力的，但这只停留在圈子里面，也就是玩这个音乐短视频或者本身就喜欢脑洞大开的年轻人才会知道有这个产品。所以我能猜想抖音的运营团队也能意识到离这个APP的爆发只差一步了，就是开始铺广告做公关传播。因此我们在5月份看到，抖音开始集中式地投公关文进行传播，而这些平台也是集中在一些互联网人都关注的垂直科技媒体或者一些大门户网站，为了就是让更多走在互联网前沿的人士知道这个开脑洞的APP，而这群人也是极具传播效应的一群人。</p><p>抖音的爆红可以是顺势而流，但是也离不开运营团队对短视频市场的洞察以及对对运营节奏感的把握良好，所以才会有五月份的爆红不是偶然事件。</p><p>但是我们现在最关注的是抖音会不会火一下就死，因为大家都说其实这款APP其实翻版的小咖秀，因为当时的微博是靠着粗暴式的推广方法以及身傍微博这棵大树，所以让这个纯娱乐工具性的产品一下子就起来了，但是火了一把之后就死了。那抖音会不会像小咖秀一样，火了一把就死呢？我明天会说说我的看法！敬请期待！</p><br><p><b>关注<a href=""https://www.zhihu.com/people/yilinxiaoyu/activities"" class=""internal"">艺林小宇 - 知乎</a> <br></b></p><p><b>艺林小宇，资深运营自媒体人，多个运营专栏作家，如果你也是0-3岁的产品运营，这个公众号适合你：艺林小宇（cs-jy8），新书《全栈运营》即将上市！</b></p><p><b>另外，近期开通了付费圈子，请直接在小密圈搜索：【全栈运营修炼】  即可加入运营大家庭一起交流学习！</b></p>",116,17,,艺林小宇,https://api.zhihu.com/people/1b5dfbf9b8f0d1e30d48547289a0c00b,赞同了文章
1523114614868,抖音,,,,,https://api.zhihu.com/columns/c_164909633,,,,1523114614,,,,,,匿名用户,,关注了专栏
1523114508206,从产品和算法角度来谈谈《抖音》,,,,,https://api.zhihu.com/articles/35393703,,,,1523114508,,"<p>        关于从去年开始爆火的产品《抖音》，比起其他现象级产品《小咖秀》《美拍》，抖音生命周期更长。关于《抖音》，以下几个亮点不得不提到：</p><p><b>1.去中心化</b></p><p>    对比《美拍》《小咖秀》《快手》，抖音去中心化可以说是做的最彻底和最足的。在产品的各处细节和场景中，都在特定避开大V的运营套路，刻意打造一个去中心化的社会网络。 首先是搜索场景，推荐的热门标签是热点事件或场景，往下刷可以看到基于场景聚合的事件。 对于一个视频内容软件，竟然没有排行榜。细想排行榜的逻辑，排行榜在促使优势积累，是形成大V的手段之一。抖音避开排行榜可谓用心良苦。</p><p><b>     抖音的社会化网络是以事件节点的，而非人物。</b>一个用户基于一个场景（比如海草舞）发起来一个挑战，人便基于这个事件而聚合在一起完成一次挑战。</p><figure data-size=""normal""><noscript><img src=""https://pic4.zhimg.com/v2-1d5691278a9d092c91bc47bf4f1f150a_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1658"" data-rawheight=""1498"" class=""origin_image zh-lightbox-thumb"" width=""1658"" data-original=""https://pic4.zhimg.com/v2-1d5691278a9d092c91bc47bf4f1f150a_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1658'%20height='1498'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1658"" data-rawheight=""1498"" class=""origin_image zh-lightbox-thumb lazy"" width=""1658"" data-original=""https://pic4.zhimg.com/v2-1d5691278a9d092c91bc47bf4f1f150a_r.jpg"" data-actualsrc=""https://pic4.zhimg.com/v2-1d5691278a9d092c91bc47bf4f1f150a_b.jpg""></figure><p><b>2.不对用户做任何假设</b></p><p>     关于抖音首页的推荐算法，也可以拿出来一说，从内容逻辑来观察，抖音推荐算法最大程度保留了新鲜度。抖音首页采用的是基于用户行为的推荐。</p><p>      一种推荐方法是基于视频和文本内容提取特征，并与用户画像特征计算相似度，相似度越高，推荐概率越大。这种推荐的方法弊端是推荐内容缺乏新鲜度，用户点击看长腿美女，于是大概率后面的内容也是长腿美女。 另外一种推荐方法是纯粹基于用户行为数据。这种方法不对用户偏好做任何假设，不对内容文本做特征提取。从个人经验来谈，特征工程是双面利器，实际情况是，特征工程常常无法挖掘出内容（视频、音频or 文本）潜在意图和特性。从信息熵增原理来理解，世界的不确定性总是朝增大的方向发展。特征提取会把人为的主观偏见引入模型。<b>最好的假设是不做任何假设。</b></p><p>       那么如何基于用户行为数据，如何建立一个基于事件网络结构的推荐算法模型？<b>给用户的推荐路径可以理解为基于事件网络的随机游走。</b>可以参考pagerank，AP等图模型算法。（这部分可写的东西太多，暂不展开讲)</p><p><b>3.隐秘的上瘾机制</b></p><p>      作为一个爆红的娱乐性产品，上瘾的设计是必不可少的。过年有段时间在家，一有空闲时间便手残般的刷抖音的首屏推荐。</p><p>首先谈谈烟瘾形成的原理，烟中包含的尼古丁可以让大脑分泌一种叫『多巴胺』的物质，让大脑产生愉悦感。但尼古丁来的快去的也快，为了找到消失的快感，大多数人会再点上一支烟，重新获得快感。就在不断的吸烟-&gt;获得愉悦感-&gt;愉悦感消失-&gt;继续吸烟循环中，成瘾性依赖。</p><p>      任何让你上瘾的产品也是一样的原理，在不断的刷中，获得短暂的愉悦感，为了找回愉悦感，继续不断的刷。所有游戏的机制设计，都有奖励成就带来的爽快感和挑战挫折的不适感之间来回转变的机制，在这个过程中成瘾性依赖。与抖音类娱乐产品不同的是，游戏的心理满足感曲线通常是游戏策划通过每个关卡挑战难度、成就和激励的数值调整设计出来的。</p><p>游戏的心流曲线，刻意设计具有规律性。</p><figure data-size=""normal""><noscript><img src=""https://pic3.zhimg.com/v2-a43eafd35826730f8e281e167a93b829_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1410"" data-rawheight=""332"" class=""origin_image zh-lightbox-thumb"" width=""1410"" data-original=""https://pic3.zhimg.com/v2-a43eafd35826730f8e281e167a93b829_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1410'%20height='332'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1410"" data-rawheight=""332"" class=""origin_image zh-lightbox-thumb lazy"" width=""1410"" data-original=""https://pic3.zhimg.com/v2-a43eafd35826730f8e281e167a93b829_r.jpg"" data-actualsrc=""https://pic3.zhimg.com/v2-a43eafd35826730f8e281e167a93b829_b.jpg""></figure><p>抖音的心流曲线:</p><figure data-size=""normal""><noscript><img src=""https://pic1.zhimg.com/v2-d47f72134ed025c59648ae9ea6f3e8a8_b.jpg"" data-caption="""" data-size=""normal"" data-rawwidth=""1420"" data-rawheight=""352"" class=""origin_image zh-lightbox-thumb"" width=""1420"" data-original=""https://pic1.zhimg.com/v2-d47f72134ed025c59648ae9ea6f3e8a8_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1420'%20height='352'&gt;&lt;/svg&gt;"" data-caption="""" data-size=""normal"" data-rawwidth=""1420"" data-rawheight=""352"" class=""origin_image zh-lightbox-thumb lazy"" width=""1420"" data-original=""https://pic1.zhimg.com/v2-d47f72134ed025c59648ae9ea6f3e8a8_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/v2-d47f72134ed025c59648ae9ea6f3e8a8_b.jpg""></figure><p><br></p><p><b>4.把一个点做足</b></p><p>《抖音》有三个主线场景：推荐、搜索、完成一次挑战。在首页的推荐场景中，<b>最重要的一点是快</b>！在用户打开产品刷屏这个动作，网络加载速度一定要快。瞬间到达会愉快的体验快感。</p><p></p>",8,1,,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,发表了文章
1519470013097,自动文摘是当前nlp的热点吗？其难点在哪里？,,,,,https://api.zhihu.com/questions/41465328,,,,1519470013,,,,1,,PaperWeekly,https://api.zhihu.com/people/23d404365eb80c6eb1f9423c61eb508f,关注了问题
1517719878465,如何设计一个算法评分英语阅读理解的难度？,假设已经有所有单词的词频数据。,3,0,7,https://api.zhihu.com/questions/266499014,xirui wang,https://api.zhihu.com/people/7d9cfceb1cb9ae069fc9e0052a4c153e,1517455209,1517719878,1517719878,利用学生的答题数据来评分。现有nlp技术无法理解深层意义和知识逻辑。,0,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1506527150050,,,,,,https://api.zhihu.com/topics/19622354,,,,1506527150,,,,,,,,关注了话题
1497866577782,ResysChina,,,,,https://api.zhihu.com/columns/resyschina,,,,1497866577,,,,,,匿名用户,,关注了专栏
1494588045385,,,,,,https://api.zhihu.com/topics/19571459,,,,1494588045,,,,,,,,关注了话题
1491291775013,你身边的精英都有什么样的特质？,,,,,https://api.zhihu.com/questions/57662817,,,,1491291775,,,,33,,Rachel,https://api.zhihu.com/people/26af8c8f0d0412cae2413448fd11bbbd,关注了问题
1491188076920,如何理解概率图模型中的i-map？,,,,,https://api.zhihu.com/questions/48815001,,,,1491188076,,,,0,,贝尔曼福特,https://api.zhihu.com/people/a152ffabfacc20e84c88e1c2ea771f2e,关注了问题
1491187639850,,,,,,https://api.zhihu.com/topics/19819560,,,,1491187639,,,,,,,,关注了话题
1491187555925,,,,,,https://api.zhihu.com/topics/19684579,,,,1491187555,,,,,,,,关注了话题
1491187033319,Occam's Razor,,,,,https://api.zhihu.com/columns/occamrazor,,,,1491187033,,,,,,匿名用户,,关注了专栏
1491141352998,分布式机器学习的故事：Rephil和MapReduce,,,,,https://api.zhihu.com/articles/19901994,,,,1491141352,,"<h3>描述长尾数据的数学模型</h3><p>Google Rephil是Google AdSense背后广告相关性计算的头号秘密武器。但是这个系统没有发表过论文。只是其作者（博士Uri Lerner和工程师Mike Yar）在2002年在湾区举办的几次小规模交流中简要介绍过。所以Kevin Murphy把这些内容写进了他的书《Machine Learning: a Probabilitic Perspecitve》里。在吴军博士的《数学之美》里也提到了Rephil。</p><p>Rephil的模型是一个全新的模型，更像一个神经元网络。这个网络的学习过程从Web scale的文本数据中归纳海量的<strong>语义</strong>——比如“apple”这个词有多个意思：一个公司的名字、一种水果、以及其他。当一个网页里包含""apple"", “stock”, “ipad""等词汇的时候，Rephil可以告诉我们这个网页是关于apple这个公司的，而不是水果。</p><p>这个功能按说pLSA和LDA也都能实现。为什么需要一个全新的模型呢？</p><p>从2007年至今，国内外很多团队都尝试过并行化pLSA和LDA。心灵手巧的工程师们，成功的开发出能学习数万甚至上十万语义（latent topics）的训练系统。但是不管大家用什么训练数据，都会发现，得到的大部分语义（相关的词的聚类）都是非常类似，或者说“重复”的。如果做一个“去重”处理，几万甚至十万的语义，就只剩下几百几千了。</p><p>这是怎么回事？</p><p>如果大家尝试着把训练语料中的低频词去掉，会发现训练得到的语义和用全量数据训练得到的差不多。换句话说，pLSA和LDA模型的训练算法<em>没有在意低频数据</em>。</p><p>为什么会这样呢？因为pLSA和LDA这类概率模型的主要构造单元都是<em>指数族分布</em>（exponential family）。比如pLSA假设一个文档中的语义的分布是multinomial的，每个语义中的词的分布也是multinomial的。因为multinomial是一种典型的指数族分布，这样整个模型描述的海量数据的分布，不管哪个维度上的marginalization，都是指数族分布。在LDA中也类似——因为LDA假设各个文档中的语义分布的multinomial distributions的参数是符合Dirichlet分布的，并且各个语义中的词的分布的multinomial distributions的参数也是符合Dirichlet分布的，这样整个模型是假设数据是指数族分布的。</p><p>可是Internet上的实际数据基本都不是指数族分布的——而是长尾分布的。至于为什么是这样？可以参见2006年纽约时报排名畅销书The Long Tail: Why the Future of Business is Selling Less of More。或者看看其作者Chris Anderson的博客<a href=""http://link.zhihu.com/?target=http%3A//www.thelongtail.com/"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">The Long Tail</a>。</p><p>长尾分布的形状大致如下图所示：</p><p>其中x轴表示数据的类型，y轴是各种类型的频率，少数类型的频率很高（称为<em>大头</em>，图中红色部分），大部分很低，但是大于0（称为<em>长尾</em>，图中黄色部分）。一个典型的例子是文章中词的分布，有个具体的名字Zipf’s law，就是典型的长尾分布。而指数族分布基本就只有大头部分——换句话说，如果我们假设长尾数据是指数族分布的，我们实际上就把尾巴给割掉了。</p><p>割掉数据的尾巴——这就是pLSA和LDA这样的模型做的——那条长尾巴覆盖的多种多样的数据类型，就是Internet上的人生百态。理解这样的百态是很重要的。比如百度和Google为什么能如此赚钱？因为互联网广告收益。传统广告行业，只有有钱的大企业才有财力联系广告代理公司，一帮西装革履的高富帅聚在一起讨论，竞争电视或者纸媒体上的广告机会。互联网广告里，任何人都可以登录到一个网站上去投放广告，即使每日广告预算只有几十块人民币。这样一来，刘备这样织席贩屡的小业主，也能推销自己做的席子和鞋子。而搜索引擎用户的兴趣也是百花齐放的——从人人爱戴的陈老师苍老师到各种小众需求包括“红酒木瓜汤”（一种丰胸秘方，应该出丰胸广告）或者“苹果大尺度”（在搜索范冰冰主演的《苹果》电影呢）。把各种需求和各种广告通过智能技术匹配起来，就酝酿了互联网广告的革命性力量。这其中，理解各种小众需求、长尾意图就非常重要了。</p><p>实际上，Rephil就是这样一个能理解百态的模型。因为它把Google AdSense的盈利能力大幅提升，最终达到Google收入的一半。两位作者荣获Google的多次大奖，包括Founders' Award。</p><p>而切掉长尾是一个很糟糕的做法。大家还记得小说《1984》里有这样一个情节吗？老大哥要求发布“新话”——一种新的语言，删掉自然英语中大部分词汇，只留下那些主流的词汇。看看小说里的人们生活的世界，让人浑身发毛，咱们就能体会“割尾巴”的恶果了。没有看过《1984》的朋友可以想象一下<a href=""http://link.zhihu.com/?target=http%3A//www.newsmth.net/"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">水木</a>首页上只有“全站十大”，连“分类十大”都删掉之后的样子。</p><p>既然如此，为什么这类模型还要假设数据是指数族分布的呢？——实在是不得已。指数族分布是一种数值计算上非常方便的数学元素。拿LDA来说，它利用了Dirichlet和multinomial两种分布的共轭性，使得其计算过程中，模型的参数都被积分给积掉了（integrated out）。这是AD-LDA这样的ad hoc并行算法——在其他模型上都不好使的做法——在LDA上好用的原因之一。换句话说，这是<em>为了计算方便，掩耳盗铃地假设数据是指数族分布的</em>。</p><p>实际上，这种掩耳盗铃在机器学习领域很普遍。比如有个兄弟听了上面的故事后说：“那我们就别用概率模型做语义分析了，咱们还用矩阵分解吧？SVD分解怎么样？” 很不好意思的，当我们把SVD分解用在语义分析（称为LSA，latent semantic analysis）上的时候，我们还是引入了指数族分布假设——Gaussian assumption或者叫normality assumption。这怎么可能呢？SVD不就是个矩阵分解方法吗？确实传统SVD没有对数据分布的假设，但是当我们用EM之类的算法解决存在<strong>missing data</strong>的问题——比如LSA，还有推荐系统里的协同过滤（collaborative filtering）——这时不仅引入了Gaussian assumption，而且引入了linearity assumption。当我们用其他很多矩阵分解方法做，都存在同样的 问题。</p><p>掩耳盗铃的做法怎么能存在得如此自然呢？这是因为指数族分布假设（尤其是Gaussian assumption）有过很多成功的应用，包括通信、数据压缩、制导系统等。这些应用里，我们关注的就是数据中的低频部分；而高频部分（或者说距离mean比较远的数据）即使丢掉了，电话里的声音也能听懂，压缩还原的图像也看得明白，导弹也还是能沿着“最可能”靠谱的路线飞行。我们当然会假设数据是指数族分布的，这样不仅省计算开销，而且自然的忽略高频数据，我们还鄙夷地称之为outlier或者noise。</p><p>可是在互联网的世界里，正是这些五花八门的outliers和noise，蕴含了世间百态，让数据不可压缩，从而产生了“大数据”这么个概念。处理好大数据的公司，赚得盆满钵满，塑造了一个个传奇。这里有一个听起来比较极端的说法<strong>大数据里无噪声</strong>——很多一开始频率很低，相当长尾，会被词过滤系统认为是拼写错误的queries，都能后来居上成为主流。比如“神马”，“酱紫”。</p><p>Rephil系统实现的模型是一个神经元网络模型（neural network）。它的设计的主要考虑，就是要能尽量好的描述长尾分布的文本数据和其中蕴含的语义。Rephil模型的具体技术细节因为没有在论文中发表过，所以不便在这里透露。但是Rephil模型描述长尾数据的能力，是下文将要介绍的Peacock系统的原动力，虽然两者在模型上完全不同。</p><p>Rephil系统是基于Google MapReduce构建的。如上节所述，MapReduce在用来实现迭代算法的时候，效率是比较低的。这也是Peacock要设计全新框架的原动力——使其比MapReduce高效，但同时像MapReduce一样支持fault recovery。</p>",76,7,,王益,https://api.zhihu.com/people/364805142e029ab96179081a15cdbb55,赞同了文章
1490847081972,HMM和贝叶斯网络的关系是什么？,,6,0,71,https://api.zhihu.com/questions/37391215,Dead Pan,https://api.zhihu.com/people/30615b7158501db02e6854fe314a4021,1447231855,1490847081,1490847081,HMM是一种特殊的贝叶斯网络。<br><br>HMM有一个条件就是观测变量yt只取决于隐含变量xt。而贝叶斯网络结构则可以非常灵活。,0,0,1,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1490540343735,算法是不是产品经理应该考虑的问题？为什么？,经常被研发同事问倒，这个功能应该怎么做呢？算法问题应该是实现层面的问题了，不知道各位在实际过程中的经验是怎么样的。其实职能本来没有一个特别清晰的界定，但是一定有最效率的方式。 这么说太虚了，举个例子，比如，产品需要根据用户添加“喜欢”的内容向用户推送用户喜欢的内容，就是一个“猜”的功能，那么这个猜出来的内容是怎么算出来的，这个算法。,45,4,1315,https://api.zhihu.com/questions/19905199,毕真,https://api.zhihu.com/people/a75c4afadcc03498849a08a8183c4376,1320512805,1490540343,1490540343,两年前做产品经理的时候被问到过同样的问题。<br>其实大部分所谓的“产品经理”在公司实际只是产品设计师，负责需求分析、交互设计、部分项目推进的工作。<br>当技术问你这个算法怎么做的时候，他在对你提出了更高的要求，也就是把你当成解决问题的人。<b>产品经理的核心职能就是解决问题，产品里所有的问题，即使你解决不了，也需要查资料动用资源找到解决问题的人。</b><br>很多小公司一般是没有算法工程师、数据科学家这种职位。,1,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1490517887377,有40万资金，如何做到年收益10%以上？,,,,,https://api.zhihu.com/questions/56320620,,,,1490517887,,,,18,,嘟嘟,https://api.zhihu.com/people/2c1b04718bb1ede027f852537e7a4d5d,关注了问题
1490186424236,,,,,,https://api.zhihu.com/topics/19670916,,,,1490186424,,,,,,,,关注了话题
1489984031208,,,,,,https://api.zhihu.com/topics/19628071,,,,1489984031,,,,,,,,关注了话题
1489715258795,,,,,,https://api.zhihu.com/topics/19613318,,,,1489715258,,,,,,,,关注了话题
1489549001679,各种机器学习算法的应用场景分别是什么（比如朴素贝叶斯、决策树、K 近邻、SVM、逻辑回归最大熵模型）？,k近邻、贝叶斯、决策树、svm、逻辑斯蒂回归和最大熵模型、隐马尔科夫、条件随机场、adaboost、em 这些在一般工作中分别用到的频率多大？一般用途是什么？需要注意什么？,27,0,9778,https://api.zhihu.com/questions/26726794,匿名用户,,1416623643,1489549001,1489549000,算法是解决方法的数学抽象，一个算法诞生于某个应用场景下，但也可以用在其他应用场景。按场景来分不太合理。<br><br><br>比如pagerank是用来做网页排序的，有人把它用在文本处理上，发现效果奇好，于是发明了textrank。再比如word2vec是自然语言处理的方法，但有人用它来处理交互数据给微博用户做推荐。,4,0,2,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1489548540779,各种机器学习算法的应用场景分别是什么（比如朴素贝叶斯、决策树、K 近邻、SVM、逻辑回归最大熵模型）？,,,,,https://api.zhihu.com/questions/26726794,,,,1489548540,,,,0,,匿名用户,,关注了问题
1489547594104,如何正确地撸《算法导论》？,"大四，通信工程，做Linux c软件开发，尚未去公司，（水平只有写写链表，进程线程控制的地步。）想在毕业前多学点东西，每天花费2小时看《算法导论》目前看了三天，到第四章最大子数组这里。我有点担忧，因为能大致看懂数学推导和渐进记号,但是自我推导和证明很吃力。想只看伪代码和设计思路，不知道只看伪代码和设计思路会不会影响后面的内容看不看的懂？求撸完的大佬们分享一下学习方法呗！（为什么我感觉网易公开课上的课没有看书好？不看书完全听不懂在讲啥而且板书惨不忍睹...)",24,2,1675,https://api.zhihu.com/questions/56904981,扶醉入香闺,https://api.zhihu.com/people/edd59e50f7c6422fae0fd3be6f4bb98e,1489135088,1489547594,1489547721,<b>基于一个有价值的应用场景深入的去想解决方法。</b>比如如何用算法设计公交车线路，如何设计知乎回答排名算法，如何预测房价……<br>以算法书为参考书来寻求解决方案，理解算法本质原理。不要硬啃，记住答案没用。<br>如何我告诉你，这个问题解决方案值一百万，你什么算法都学得会。,6,0,2,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1489477029583,,,,,,https://api.zhihu.com/topics/19558740,,,,1489477029,,,,,,,,关注了话题
1489471498305,如何通俗的理解项目反应理论？,项目反应理论比如数学考试，意思是应用题更加能说明应试者的能力吗？,9,0,98,https://api.zhihu.com/questions/24671541,张雲,https://api.zhihu.com/people/aea9b9b4e7c13e554d9f705a7562d3b4,1406705433,1489471498,1489471498,<p>传统的测试理论：<b>正确率就是知识点掌握程度。</b></p><p><b>IRT</b>（项目反应理论）:<b>通过正确率来推测知识点掌握程度。</b></p><p>正确率是知识点掌握程度有什么问题？假如一个老师出了一套卷子很难，小明学习很好，只考了60分。老师又出了一套卷子很简单，小红学习很差，考了90分。这个90分和60分是无法比较的。</p><p>测评的本质是什么？<b>是把不同人在同一个度量空间里区分开来。</b>运用IRT自适应的算法，每个人虽然做的题目不一样（由前一道题的答题情况决定下一道题测什么），但是测出来的知识点掌握程度是可以比较的。</p><p>IRT模型起源于上世纪60年代，是机器学习算法的一种。大部分机器学习算法，都是<b>通过现象来推测本质规律。IRT是通过学生答题情况来推测潜在特质（也就是知识点掌握程度），通过建立数学模型，根据答题情况的目标函数来求出知识点掌握程度（或者叫能力值）。</b></p><p>仔细看IRT数学模型，实际是变形了的逻辑斯蒂克回归模型。求解可用极大似然估计和牛顿迭代法来求（具体看书，不累述了）</p>,27,5,7,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1489114757637,为什么索尼 Xperia Z 系列在国内销量这么差？,是什么原因呢？,150,7,659,https://api.zhihu.com/questions/28238571,沧澜,https://api.zhihu.com/people/e69b6a65a8ffdc32af232bd79fe35153,1424176226,1489114757,1489135781,索尼移动工作过的人出来说一下。产品烂是有原因的：<br><br>1.不重视产品的工作。很多需求来自运营商。我在索尼工作的两年多，从没见过product manager这个角色，倒是见过一堆的project manager，那些人产品技术都不懂，只负责催进度排项目，还拿年度star。<br><br>2.索尼有全世界最复杂的流程管理系统。当时为了改一个应用图标和名称，我走流程，跟瑞典负责审核的老外沟通了一个月，他们总是很忙的。可想而知，这种办事效率肯定跟不上市场变化。<br><br>3.索尼移动中国最高层基本都是欧洲人。欧洲人普遍清闲惯了，过惯了贵族生活，喝个下午茶什么的，也不懂中国市场中国人。怎么跟资源稀缺惯了的中国人拼。,3,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1486528655724,,,,,,https://api.zhihu.com/topics/19672772,,,,1486528655,,,,,,,,关注了话题
1486283997086,,,,,,https://api.zhihu.com/topics/19570973,,,,1486283997,,,,,,,,关注了话题
1485687604225,,,,,,https://api.zhihu.com/topics/19607065,,,,1485687604,,,,,,,,关注了话题
1484300691387,超智能体,,,,,https://api.zhihu.com/columns/YJango,,,,1484300691,,,,,,匿名用户,,关注了专栏
1484197710890,,,,,,https://api.zhihu.com/topics/20043586,,,,1484197710,,,,,,,,关注了话题
1483971051551,在线教育会成为下一个爆发点吗？,在线教育的创业和投资风潮正在到来。 在线教育会成为下一个爆发点吗？,224,11,8618,https://api.zhihu.com/questions/24683830,戴新和,https://api.zhihu.com/people/87016c5d8e9f0673d7b9c5335b3e3e22,1406791577,1483971051,1483611457,"看到这个问题，一时没控制住，也过来聊两句。<br><br>我曾被央视报道，号称网红物理教师。<br><br>为了收入和热度，我在2016年直播了近1000个小时的高考课程。现在累得一身病，精神也常年萎靡。<br><br>这个话题关于在线教育，大家对这个行业也一直在意淫，这些年从电视大学到精品视频课又到慕课，在线教育一直新瓶装老酒，恰逢2015技术端的革新浪潮又赶上了疯狂直播热。所以当前的在线教育方向仿佛就是直播课，直播课好像也成了在线教育变现的唯一手段。<br><br>真的是这样吗。我2016年直播了近1000小时的课程，加上我几乎每课必拖堂负责的教学态度，我想我应该算有资格来跟大家谈一谈，在线教育是不是真的就只剩直播课了。<br><br>我觉得不是。<br><br>直播课能这么火主要是拜业内领头羊——猿辅导所赐。后有""作业帮""紧随其后，其他传统老牌网校如腾讯课堂，沪江网也开始了直播课的业务。但是，大部分的从业者都是跟风，很少有人真正看透直播课的未来和瓶颈，以我所从事的高考教培行业来说，直播课走不远。<br><br>我相信会有人不认同，但我实在没时间把论据一一摆出来。<br><br>只能跟大家先分享两个点，以后有机会再补。<br><br>先问大家一个问题，直播课，重点应该在直播还是在课。<br><br>应该有不少人认为重心在于课，我和你们一样，也曾这样天真的以为过。<br><br>但我现在的结论——重心在于直播本身，而直播课当下形式更要顺应直播的热潮，否则反其道而行，事倍功半。<br><br>打开任何直播平台，你都可以不需要缴纳任何费用进入一个直播间，或者看游戏，或者看唱歌跳舞欧巴桑。那些网红的直播间在线人数少则上完多则上百万，比如2016大火的LOL主播Pdd，通常深夜时分还有500万人看他打游戏吹牛逼，而这些一线的主播，他们的签约费最高可达4000万一年。<br><br>反观直播课，进入教室首先需要缴学费，而报名之后离正式开课还有一段时间。这一下游戏规则就全变了，学生当即的报名冲动及学习的动力无法得到反馈，那种当时报名时恨不能头悬梁锥刺股的拼劲一下被卸了，等到实际开课时学生往往想不起来为什么要报这节课。这种契约式的直播模式已经失去直播的最核心竞争力——及时性及被动接受性。而真正开课时到课率只有10%（高三）的魔咒，像我这样的一线名师也难逃其中。<br><br>我们的直播其实已经不是直播，跟视频通话差不多，只不过是一人对多人而已。而且还得提前约好才行。<br><br>第二点，我们在反其道而行。高三的学生，尤其现在这个阶段，根本没有时间，他们要的是提分，不是什么这种课那种课，我们看上去提供了教学服务，可实际这种服务压根不是服务，反倒是负担。而且学习本就是反人性的，如果真有一种办法可以像打针一样注入人体技能和知识，我想没有人会愿意享受这种教学服务的。这种服务跟足疗保健不一样，一个是让人身心放松，一个是让人神情紧张。<br><br>尤其那些提倡利用碎片时间学习的，更是一派胡言。学习是一种独立长久的研磨过程，需要时间的消耗，那种碎片化学习的方式适合伟人，不适合老百姓。老百姓更喜欢在上厕所、等公交、坐地铁时翻翻朋友圈，看点娱乐花边什么的，至于拿出手机能被两个单词的，我想这种人将来也会必成大事。<br><br>所以，直播课在高三这个用户群体中无法跑通，反观我2016这1000小时的直播课，能被称为精品的可能也就一半左右，剩下一半也会因为精力不足，体力不支而只能尽力无法尽美。<br><br>直播当然有很多优点，互动增强，贴合热潮，但当大浪退去，留给用户的是当时的无聊时间的打发还是务实的成绩提高，我想从业者有各中滋味。但如果是后者，那何必非要以直播呈现，大可精美录制课程后配合书籍、习题、图文课来切实有效帮助用户掌握技能提高成绩。其实我的学生都知道，我的大招笔记比我的课更受欢迎，你们说呢？",283,52,67,高考物理王羽,https://api.zhihu.com/people/b97b04fa72d3766b76c4c5bc85d6f284,赞同了回答
1483970574540,LDA 在文本分类中，如何提高分类器的精确度？,,,,,https://api.zhihu.com/questions/28679943,,,,1483970574,,,,0,,Charles,https://api.zhihu.com/people/9b48baec5db625bf21d8a12f7522d446,关注了问题
1483335450887,word2vec有什么应用？,,,,,https://api.zhihu.com/questions/25269336,,,,1483335450,,,,0,,bean mr,https://api.zhihu.com/people/807114adabb9060957fef1f06de47c2c,关注了问题
1483327374635,如何用 word2vec 计算两个句子之间的相似度？,,,,,https://api.zhihu.com/questions/29978268,,,,1483327374,,,,2,,匿名用户,,关注了问题
1483327363731,,,,,,https://api.zhihu.com/topics/19886836,,,,1483327363,,,,,,,,关注了话题
1482066072049,如何评价 2015 版的 Magi 搜索引擎？,,,,,https://api.zhihu.com/questions/28627372,,,,1482066072,,,,12,,季逸超,https://api.zhihu.com/people/7b426b1afe45c18850b2342854e09745,关注了问题
1478050607365,,,,,,https://api.zhihu.com/roundtables/biz-school,,,,1478050607,,,,166,,,,关注了圆桌
1476888765662,,,,,,https://api.zhihu.com/topics/19560026,,,,1476888765,,,,,,,,关注了话题
1476888762177,,,,,,https://api.zhihu.com/topics/19838204,,,,1476888762,,,,,,,,关注了话题
1476884848452,数据科学家 (Data Scientist) 的核心技能是什么？,,74,4,4732,https://api.zhihu.com/questions/27604790,匿名用户,,1421307849,1476884848,1476884848,思想，思考问题的方式。<br><br>其实任何行业想成为大师级别，核心技能都一样。比如程序员，北大青鸟和985名校硕士的区别在于：前者教的是工具技能，后者教的是思想,0,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1475756286250,,,,,,https://api.zhihu.com/topics/19634400,,,,1475756286,,,,,,,,关注了话题
1475756277001,,,,,,https://api.zhihu.com/topics/20013785,,,,1475756277,,,,,,,,关注了话题
1475302716929,三节课,,,,,https://api.zhihu.com/columns/sanjieke,,,,1475302716,,,,,,匿名用户,,关注了专栏
1474878233396,BMAN,,,,,https://api.zhihu.com/columns/worldcitizen,,,,1474878233,,,,,,匿名用户,,关注了专栏
1474878228910,李叫兽,,,,,https://api.zhihu.com/columns/lijiaoshou,,,,1474878228,,,,,,匿名用户,,关注了专栏
1468242875874,神经科学,,,,,https://api.zhihu.com/columns/neuroscience,,,,1468242875,,,,,,匿名用户,,关注了专栏
1468242633596,原来大脑是这么工作的：神经科学学术笔记,,,,,https://api.zhihu.com/columns/learningneuroscience,,,,1468242633,,,,,,匿名用户,,关注了专栏
1467812808002,,,,,,https://api.zhihu.com/topics/19860606,,,,1467812808,,,,,,,,关注了话题
1467775158326,,,,,,https://api.zhihu.com/topics/19619550,,,,1467775158,,,,,,,,关注了话题
1464616721399,,,,,,https://api.zhihu.com/topics/19562101,,,,1464616721,,,,,,,,关注了话题
1464616707577,,,,,,https://api.zhihu.com/topics/19559450,,,,1464616707,,,,,,,,关注了话题
1464616688242,,,,,,https://api.zhihu.com/topics/19559424,,,,1464616688,,,,,,,,关注了话题
1464600819123,猿题库的数据分析、算法、个性化评估及指导有多精确或可靠？（针对K12）,这两天看见新闻，说“猿题库正式进入了这个领域最核心的战场“K-12”（基础教育），于今日发布了高考题库产品，涉足中小学教育领域。”“它提供了专项智能练习、组卷模考和真题模考等多种练习方式，每次练习后系统生成智能评估报告，能直观呈现学生在相关知识点上的掌握情况，同时根据能力情况智能推荐对应难度的题目供学生练习。” 想知识这个猿题库对K12的应试教育研究有多深？多专业？另外，近来火热的“大数据”分析玄乎其玄，也有说大数据、云计算没有想象中精确的文章，所以想知道，猿题库在K12应试教育方面的数据分析、个性化服务等，到底个性化、精确、靠谱到什么地步。。。？ 关于猿题库的两篇报告，在技术上有多少实在的干货？号称“云计算”和“大数据公司”的猿题库，在如何改造传统考试题库产品 |PingWest 猿题库终于对高考下手了：真题、数据与K12 |PingWest,7,0,93,https://api.zhihu.com/questions/21709871,Jane Air,https://api.zhihu.com/people/8cee40020843f3a78708e3871cebda69,1380184434,1464600819,1464601125,先科普下:<br>机器学习在教育测评领域的运用有两个体系:IRT和BKT。IRT主要用在测量学生能力和知识点掌握程度。BKT主要关注学习过程中学生掌握情况的变化。两者应用场景不一样。延伸出来的算法也很多。比如HIRT，TIRT，DKT。<br>说说我的看法:<br>猿题库用的核心算法应该是IRT，看应用场景，产品里会打个分数也就是能力值。BKT主要用在mooc系统。<br>使用后发现猿题库测评结果准确率不高，其实IRT是一套很成熟的理论体系，准确率不高并不是算法问题而是数据问题。猿题库从课后场景切入，学生答题数据的信度效度有问题。,5,2,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1464509673888,我关注的几个 data science 资源,,,,,https://api.zhihu.com/articles/20564819,,,,1464509673,,"最近半年比较少看论文，倒是周末不时会看看 Twitter 的讨论、一些做 data science 团队的技术博客/视频... 感觉挺不错，趁着新年分享给大家。<br><h2>一. TechTalk 视频</h2><p>主要是 <a href=""http://link.zhihu.com/?target=http%3A//mlconf.com"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">The Machine Learning Conference</a> 办的 tech talk，他们的 <a href=""http://link.zhihu.com/?target=https%3A//www.youtube.com/channel/UCjeM1xxYb_37bZfyparLS3Q"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">YouTube Page</a></p><p>每一期都会有来自学术、工业界的十多位嘉宾；有大家可能比较熟悉的 Xavier Amatriain 讲 Quora 的机器学习系统演进和实操经验，也有貌似最近比较活跃的公司 Ayasdi 讲 topology for data science；</p><p>从演讲内容来看，一类是比较资深的团队 leader 讲方法论、系统；不会在方法/技巧上讲很细，倒是侧重于把系统的设计和理念，透过他们的业务场景向听众娓娓道来；从中可以体会到数据科学团队对公司业务的推动，比如 Airbnb 的一篇文章中提到产品初创时（即便当时 DS 团队只有很少几个人）便非常注重 data driven 的 dicision making，感觉他们比较强调 DS 团队和公司 vision/mission 的结合</p><br><p>另一类是在某个领域有许多经验的研究员，会讲比较细一些；可以看到一些比较新的技术在业界的应用。总之各有特点。</p><p>还有一个特点是这类内容不会十分烧脑，哈哈比较适合周末和休闲。但多看看这些内容可以帮助自己建立比较好的产品感觉、或是形成对系统的概念和理解。<br></p><br><h2>二. Tech Blog</h2><p>我一般通过 feedly 来订阅这些内容。</p><p>1. <a href=""http://link.zhihu.com/?target=http%3A//nerds.airbnb.com"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Airbnb Engineering</a> 有 high-level 的哲学类讨论，也有很细致的对模型的讨论。叙事手段也是比较丰富的，可以透过这些文字看到数据团队的氛围和自信；</p><p>2. <a href=""http://link.zhihu.com/?target=http%3A//multithreaded.stitchfix.com/"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Stitch Fix Technology | Multithreaded</a> 是一家为客户做穿衣搭配推荐的公司，比较多 vision 的应用</p><p>3. <a href=""http://link.zhihu.com/?target=http%3A//tech.opentable.com"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Ingredients - Stories from the team behind OpenTable</a> 是一家做餐厅推荐的公司，比较多 NLP 的内容；比如讲他们如何做餐厅的评论挖掘和推荐</p><p>4. <a href=""http://link.zhihu.com/?target=https%3A//data.quora.com/"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Data @Quora</a> 比较喜欢用 story telling 的方式，用数据分析来揭示 Quora 上有趣的现象</p><p>当然，如果你顺藤摸瓜还可以找到他们的 GitHub，看到他们开源的代码。总之蛮不错的。</p><br><h2>三. 学习资源</h2><p>前面两种方式大概比较粗矿，适合培养系统层面的感觉。不过若是要实际应用这些，还有一些可以参考的视频课程。最近比较新的课程莫过于 Stanford 提供的 DL for NLP &amp; CV 了，分别是 cs224d &amp; cs231n，还有 <a href=""http://link.zhihu.com/?target=https%3A//www.youtube.com/channel/UCEjvv31JNlj4rNf1su0-Txg"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">Reinforcement Learning | YouTube</a>；都有不时跟一下，看看视频写写码。 </p><br><h2>最后</h2><p>干活大半年了，有幸参与到团队里建设基础设施的工作中去，体验了一把和工程团队合作的乐趣，也趁此机会了解到系统的前前后后，虽然还没法了解各个细节，但也总算体会到啥是系统了，也感觉到负责设计一套系统的不易；当然，这对我来说都是趣味。并且在使用这套工具的时候，能逐渐感觉到它的优势和不足，有办法促成它的升级进化，老板和我都蛮开心。</p><p>和人聊天时大家都会问我们是什么团队，其实我不知道怎么回答才会让讨论的你我都比较满意，至少是有深度的讨论嘛。所以我的回答是，我们不像搜索或是阿里妈妈那样是做算法的核心部门，没待过也不清楚会有怎样的体验，前者肯定有更深的技术积累和更完善的基础设施；在实际做事上来说，我们这边的事情是多样化，跟运营同学结合，意味着某个同学需要(可以)了解(学习)蛮多，还要锻炼自己和非算法同学讨论、讲演的能力。同时团队结构是比较多元的，有 ETL、工程、算法.. 合作也是跨团队的，因此对新人来说是有蛮多可能性的。</p><p>去年放假前看到福贝/赵海平的演讲，讲他对中台战略的理解。他提到，Facebook的内部协作平台就像员工的记事本和日历，这些点滴都会留在硬盘上，成为公司的资产，日后新人可以透过它看到产品发展的全部，看到对当时技术方案的讨论和代码；他还说，其实目前公司对工程师的定级虽然写的很抽象，但实际上已经命中了他对工程师水平的理解，专家写的码被多个团队使用，等等... 通过鼓励大家写可维护的码，可复用的码，中台自然建成。</p><p>虽然这些日子我没做很多东西，自以为能力也没有实现飞越；但很欣慰自己在看到这些东西时仍能有在学校时那样的激情，也很开心能在公司里向各个方向的同事学习。总之，这是很有意义的一年，希望新一年能继续实施自己的计划，给团队带更多的沉淀。最后祝大家新年快乐~</p>",929,21,,郑梓豪,https://api.zhihu.com/people/12135f10b08a64c54e8bfd537dd7bee7,赞同了文章
1464320558234,产品经理，如何转行到人工智能/机器人领域？,,,,,https://api.zhihu.com/questions/46753823,,,,1464320558,,,,1,,hanniman,https://api.zhihu.com/people/e035c12c7c0382a132dfc8abdd9bd759,关注了问题
1464173326051,为什么说好的产品经理一将难求？,,,,,https://api.zhihu.com/questions/41267806,,,,1464173326,,,,9,,匿名用户,,关注了问题
1462110802885,女神进化论,,,,,https://api.zhihu.com/columns/hibetterme,,,,1462110802,,,,,,匿名用户,,关注了专栏
1455499573908,词向量（ Distributed Representation）工作原理是什么？,能否举个通俗的例子说明一下？,10,0,716,https://api.zhihu.com/questions/21714667,匿名用户,,1380260100,1455499573,1381912169,"        要将自然语言交给机器学习中的算法来处理，通常需要首先将语言数学化，词向量就是用来将语言中的词进行数学化的一种方式。<br><br>        一种最简单的词向量方式是  one-hot representation，就是用一个很长的向量来表示一个词，向量的长度为词典的大小，向量的分量只有一个 1，其他全为 0， 1 的位置对应该词在词典中的位置。但这种词表示有两个缺点：（1）容易受维数灾难的困扰，尤其是将其用于 Deep Learning 的一些算法时；（2）不能很好地刻画词与词之间的相似性（术语好像叫做“词汇鸿沟”）。<br><br>        另一种就是你提到 Distributed Representation 这种表示，它最早是 Hinton 于 1986 年提出的，可以克服 one-hot representation 的缺点。其基本想法是：<br>        通过训练将某种语言中的每一个词映射成一个固定长度的短向量（当然这里的“短”是相对于 one-hot representation 的“长”而言的），将所有这些向量放在一起形成一个词向量空间，而每一向量则为该空间中的一个点，在这个空间上引入“距离”，则可以根据词之间的距离来判断它们之间的（词法、语义上的）相似性了。<br><br>        为更好地理解上述思想，我们来举一个通俗的例子：假设在二维平面上分布有 N 个不同的点，给定其中的某个点，现在想在平面上找到与这个点最相近的一个点，我们是怎么做的呢？首先，建立一个直角坐标系，基于该坐标系，其上的每个点就唯一地对应一个坐标 （x,y）；接着引入欧氏距离；最后分别计算这个词与其他 N-1 个词之间的距离，对应最小距离值的那个词便是我们要找的词了。<br><br>        上面的例子中，坐标（x,y） 的地位相当于词向量，它用来将平面上一个点的位置在数学上作量化。坐标系建立好以后，要得到某个点的坐标是很容易的，然而，在 NLP 任务中，要得到词向量就复杂得多了，而且词向量并不唯一，其质量也依赖于训练语料、训练算法和词向量长度等因素。<br><br>        一种生成词向量的途径是利用神经网络算法，当然，词向量通常和语言模型捆绑在一起，即训练完后两者同时得到。用神经网络来训练语言模型的思想最早由百度 IDL （深度学习研究院）的徐伟提出。 这方面最经典的文章要数 Bengio 于 2003 年发表在 JMLR 上的 A Neural Probabilistic Language Model，其后有一系列相关的研究工作，其中包括谷歌 Tomas Mikolov 团队的 word2vec （<a href=""http://link.zhihu.com/?target=https%3A//code.google.com/p/word2vec/"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">word2vec -
 
 
 Tool for computing continuous distributed representations of words.</a>）。<br><br>        最近了解到词向量在机器翻译领域的一个应用，报道（<a href=""http://link.zhihu.com/?target=http%3A//www.looooker.com/archives/5621"" class="" wrap external"" target=""_blank"" rel=""nofollow noreferrer"">机器翻译领域的新突破</a>）是这样的：<br><blockquote><i>谷歌的 Tomas Mikolov 团队开发了一种词典和术语表的自动生成技术，能够把一种语言转变成另一种语言。该技术利用数据挖掘来构建两种语言的结构模型，然后加以对比。每种语言词语之间的关系集合即“语言空间”，可以被表征为数学意义上的向量集合。在向量空间内，不同的语言享有许多共性，只要实现一个向量空间向另一个向量空间的映射和转换，语言翻译即可实现。该技术效果非常不错，对英语和西语间的翻译准确率高达 90%</i>。</blockquote>        我读了一下那篇文章（<a href=""http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1309.4168.pdf"" class="" external"" target=""_blank"" rel=""nofollow noreferrer""><span class=""invisible"">http://</span><span class=""visible"">arxiv.org/pdf/1309.4168</span><span class=""invisible"">.pdf</span><span class=""ellipsis""></span></a>），引言中介绍算法工作原理的时候举了一个例子，我觉得它可以帮助我们更好地理解词向量的工作原理，特介绍如下：<br>        考虑英语和西班牙语两种语言，通过训练分别得到它们对应的词向量空间 E 和 S。从英语中取出五个词 one，two，three，four，five，设其在 E 中对应的词向量分别为 v1，v2，v3，v4，v5，为方便作图，利用主成分分析（PCA）降维，得到相应的二维向量 u1，u2，u3，u4，u5，在二维平面上将这五个点描出来，如下图左图所示。类似地，在西班牙语中取出（与 one，two，three，four，five 对应的） uno，dos，tres，cuatro，cinco，设其在 S 中对应的词向量分别为 s1，s2，s3，s4，s5，用 PCA 降维后的二维向量分别为 t1，t2，t3，t4，t5，将它们在二维平面上描出来（可能还需作适当的旋转），如下图右图所示：<br><figure><noscript><img src=""https://pic1.zhimg.com/469f845025ef071bba1a578565d8b261_b.jpg"" data-rawwidth=""1211"" data-rawheight=""445"" class=""origin_image zh-lightbox-thumb"" width=""1211"" data-original=""https://pic1.zhimg.com/469f845025ef071bba1a578565d8b261_r.jpg""></noscript><img src=""data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1211'%20height='445'&gt;&lt;/svg&gt;"" data-rawwidth=""1211"" data-rawheight=""445"" class=""origin_image zh-lightbox-thumb lazy"" width=""1211"" data-original=""https://pic1.zhimg.com/469f845025ef071bba1a578565d8b261_r.jpg"" data-actualsrc=""https://pic1.zhimg.com/469f845025ef071bba1a578565d8b261_b.jpg""></figure>观察左、右两幅图，容易发现：五个词在两个向量空间中的相对位置差不多，这说明两种不同语言对应向量空间的结构之间具有相似性，从而进一步说明了在词向量空间中利用距离刻画词之间相似性的合理性。",281,13,49,皮果提,https://api.zhihu.com/people/e7c8be0d61d7f9c86990e6f58a1c357d,赞同了回答
1455499558739,词向量（ Distributed Representation）工作原理是什么？,,,,,https://api.zhihu.com/questions/21714667,,,,1455499558,,,,0,,匿名用户,,关注了问题
1454381974400,中国的在线教育为什么没有knewton knowre这样的公司？,,2,0,21,https://api.zhihu.com/questions/20929313,张琨,https://api.zhihu.com/people/2c37adb1bd350a054c7cc04c20d33727,1365427936,1454381974,1454382041,中国在线教育处于初期探索阶段。而自适应学习系统有两大核心：知识图谱和学生答题数据。<br>中国的教研体系很复杂，知识图谱的构建是很庞大的工程。<br>学生答题数据收集是个难事，现在学生做作业和考试都是纸质试卷，录入计算机很麻烦。而国内基本很难买到完整、准确的学生答题数据。,3,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1454381707490,有谁知道美国knewton针对K12教育的核心功能是什么？数据来源是怎么来的?还有商业模式是怎样的?,特别是他的个性化教育，是个怎么个性化法，怎样在学生群里产生共鸣，并且依靠用户传播，商业模式什么样的,4,0,43,https://api.zhihu.com/questions/24443894,壹课,https://api.zhihu.com/people/9b14b7189163d37fa3d820ed44ff8e50,1404999342,1454381707,1454381737,Knewton是个性化教学服务提供商。知识图谱、学生的答题数据、推荐系统和诊断系统构成构成闭环。为教育内容提供商、渠道商、硬件设备制造商提供核心技术。<br>自适应学习实际上就是量体裁衣、因材施教。,0,0,0,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1449109370912,跟谁学平台会成功吗？,,54,2,458,https://api.zhihu.com/questions/31133946,匿名用户,,1433862736,1449109370,1449109370,谁规定了互联网做产品就得单点切入，谁规定了淘宝模式就一定不能成功。事在人为，行业缺的不是看到问题的人，而是知道怎样能行的人。,1,0,2,章鱼小丸子,https://api.zhihu.com/people/a13060756ec6c8f3dbc6319e482631c0,回答了问题
1449109248921,跟谁学平台会成功吗？,,,,,https://api.zhihu.com/questions/31133946,,,,1449109248,,,,2,,匿名用户,,关注了问题
1449054888626,在线教育会成为下一个爆发点吗？,,,,,https://api.zhihu.com/questions/24683830,,,,1449054888,,,,11,,戴新和,https://api.zhihu.com/people/87016c5d8e9f0673d7b9c5335b3e3e22,关注了问题
1449029074844,有哪些事情是你入行时不以为然甚至嗤之以鼻，入行后却整个颠覆了之前的认知并奉为至理的？,,,,,https://api.zhihu.com/questions/38080498,,,,1449029074,,,,9,,唐中山,https://api.zhihu.com/people/e4379ffcaf81e7fc374807c23010be6d,关注了问题
1448976433753,,,,,,https://api.zhihu.com/topics/19555457,,,,1448976433,,,,,,,,关注了话题
1448976427168,,,,,,https://api.zhihu.com/topics/19553370,,,,1448976427,,,,,,,,关注了话题
1448976047396,猿题库未来能否成为移动端在线教育巨头？,,,,,https://api.zhihu.com/questions/22356126,,,,1448976047,,,,0,,匿名用户,,关注了问题
1448963445631,如何在当今在线教育行业杀出重围？是不是目前的商业模式过于单一？,,,,,https://api.zhihu.com/questions/37940571,,,,1448963445,,,,1,,云凝华,https://api.zhihu.com/people/2ef613e472425dcdd67bd2fa3852e4a2,关注了问题
1448963354942,互联网教育机构这么多，你最看好哪一家？理由是什么？,,,,,https://api.zhihu.com/questions/37663732,,,,1448963354,,,,7,,冯兄,https://api.zhihu.com/people/9110107093995097f75780ee61e54a55,关注了问题
1447385672437,中国的在线教育为什么没有knewton knowre这样的公司？,,,,,https://api.zhihu.com/questions/20929313,,,,1447385672,,,,0,,张琨,https://api.zhihu.com/people/2c37adb1bd350a054c7cc04c20d33727,关注了问题
1447379662307,有谁知道美国knewton针对K12教育的核心功能是什么？数据来源是怎么来的?还有商业模式是怎样的?,,,,,https://api.zhihu.com/questions/24443894,,,,1447379662,,,,0,,壹课,https://api.zhihu.com/people/9b14b7189163d37fa3d820ed44ff8e50,关注了问题
1445909890571,,,,,,https://api.zhihu.com/topics/19555682,,,,1445909890,,,,,,,,关注了话题
1443332897884,,,,,,https://api.zhihu.com/topics/19896040,,,,1443332897,,,,,,,,关注了话题
1436070756571,,,,,,https://api.zhihu.com/topics/19556975,,,,1436070756,,,,,,,,关注了话题
1433928298610,,,,,,https://api.zhihu.com/topics/19562940,,,,1433928298,,,,,,,,关注了话题
1433733253586,,,,,,https://api.zhihu.com/topics/19660843,,,,1433733253,,,,,,,,关注了话题
1431222877160,如何长时间高效学习？,,,,,https://api.zhihu.com/questions/28358499,,,,1431222877,,,,150,,匿名用户,,关注了问题
